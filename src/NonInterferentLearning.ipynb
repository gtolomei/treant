{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Documentation\n",
    "\n",
    " - http://lightgbm.readthedocs.io/en/latest/\n",
    " - http://lightgbm.readthedocs.io/en/latest/Python-Intro.html\n",
    " - https://github.com/Microsoft/LightGBM/tree/master/examples/python-guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm\n",
    "import functools\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "# Adding the following line, allows Jupyter Notebook to visualize plots\n",
    "# produced by matplotlib directly below the code cell which generated those.\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(73)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load 6 different datasets:\n",
    "-  Training set (original)\n",
    "-  Training set (_attacked_)\n",
    "-  Validation set (original)\n",
    "-  Validation set (_attacked_)\n",
    "-  Test set (original)\n",
    "-  Test set (_attacked_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS_PATH = \"../data/census\"\n",
    "MODELS_PATH = \"../out/models\"\n",
    "ATTACKER = \"strong\" # weak\n",
    "TRAINING_SET=\"train_ori.csv.bz2\" # original training set\n",
    "TRAINING_SET_ATT=\"train_\"+ATTACKER+\"_att.csv.bz2\" # perturbed training set\n",
    "VALIDATION_SET=\"valid_ori.csv.bz2\" # original validation set\n",
    "VALIDATION_SET_ATT=\"valid_\"+ATTACKER+\"_att.csv.bz2\" # perturbed validation set\n",
    "TEST_SET=\"test_ori.csv.bz2\" # original test set\n",
    "TEST_SET_ATT=\"test_\"+ATTACKER+\"_att.csv.bz2\" # perturbed test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path, dataset_filename, sep=\",\"):\n",
    "    return pd.read_csv(path+\"/\"+dataset_filename, sep=sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_categorical_features(dataset):\n",
    "    categorical_features = []\n",
    "    for column in dataset.columns:\n",
    "        if dataset[column].dtype == 'object':\n",
    "            categorical_features.append(column)\n",
    "    return categorical_features\n",
    "            \n",
    "def label_encode(dataset, categorical_features):\n",
    "    dataset_le = dataset.copy()\n",
    "    for column in dataset_le.columns:\n",
    "        if column in categorical_features:\n",
    "            dataset_le[column] = dataset_le[column].astype('category')\n",
    "            dataset_le[column] = dataset_le[column].cat.codes\n",
    "    return dataset_le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(dataset, label):\n",
    "    dataset_oh = pd.get_dummies(dataset)\n",
    "    columns = dataset_oh.columns.tolist()\n",
    "    columns.insert(len(columns), columns.pop(columns.index(label)))\n",
    "    dataset_oh = dataset_oh.loc[:,columns]\n",
    "    dataset_oh.columns = columns\n",
    "    \n",
    "    return dataset_oh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = load_dataset(DATASETS_PATH, TRAINING_SET)\n",
    "TRAIN_ATT = load_dataset(DATASETS_PATH, TRAINING_SET_ATT)\n",
    "\n",
    "VALID = load_dataset(DATASETS_PATH, VALIDATION_SET)\n",
    "VALID_ATT = load_dataset(DATASETS_PATH, VALIDATION_SET_ATT)\n",
    "\n",
    "TEST = load_dataset(DATASETS_PATH, TEST_SET)\n",
    "TEST_ATT = load_dataset(DATASETS_PATH, TEST_SET_ATT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute group lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_ATT_OFFSETS = TRAIN_ATT['instance_id'].value_counts().sort_index().values\n",
    "VALID_ATT_OFFSETS = VALID_ATT['instance_id'].value_counts().sort_index().values\n",
    "TEST_ATT_OFFSETS = TEST_ATT['instance_id'].value_counts().sort_index().values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infer _categorical_ features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CATEGORICAL_FEATURES = infer_categorical_features(TRAIN)\n",
    "print(\"List of categorical features: [{}]\"\n",
    "      .format(\", \".join([cf for cf in CATEGORICAL_FEATURES])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform _categorical_ features to _numeric_ (label encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = label_encode(TRAIN, set(CATEGORICAL_FEATURES))\n",
    "TRAIN_ATT = label_encode(TRAIN_ATT.iloc[:,1:], set(CATEGORICAL_FEATURES))\n",
    "\n",
    "VALID = label_encode(VALID, set(CATEGORICAL_FEATURES))\n",
    "VALID_ATT = label_encode(VALID_ATT.iloc[:,1:], set(CATEGORICAL_FEATURES))\n",
    "\n",
    "TEST = label_encode(TEST, set(CATEGORICAL_FEATURES))\n",
    "TEST_ATT = label_encode(TEST_ATT.iloc[:,1:], set(CATEGORICAL_FEATURES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform _categorical_ features to _numeric_ (one-hot encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN = one_hot_encode(TRAIN, \"income_greater_than_50k\")\n",
    "# TRAIN_ATT = one_hot_encode(TRAIN_ATT.iloc[:,1:], \"income_greater_than_50k\")\n",
    "\n",
    "# VALID = one_hot_encode(VALID, \"income_greater_than_50k\")\n",
    "# VALID_ATT = one_hot_encode(VALID_ATT.iloc[:,1:], \"income_greater_than_50k\")\n",
    "\n",
    "# TEST = one_hot_encode(TEST, \"income_greater_than_50k\")\n",
    "# TEST_ATT = one_hot_encode(TEST_ATT.iloc[:,1:], \"income_greater_than_50k\")\n",
    "\n",
    "# common_columns = TRAIN.columns & VALID.columns & TEST.columns\n",
    "\n",
    "# TRAIN = TRAIN[common_columns]\n",
    "# TRAIN_ATT = TRAIN_ATT[common_columns]\n",
    "# VALID = VALID[common_columns]\n",
    "# VALID_ATT = VALID_ATT[common_columns]\n",
    "# TEST = TEST[common_columns]\n",
    "# TEST_ATT = TEST_ATT[common_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(TRAIN.shape)\n",
    "print(TRAIN_ATT.shape)\n",
    "print(VALID.shape)\n",
    "print(VALID_ATT.shape)\n",
    "print(TEST.shape)\n",
    "print(TEST_ATT.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_VALID = pd.concat([TRAIN, VALID], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(TRAIN_VALID.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters used for _standard_ and _baseline_ learning\n",
    "\n",
    "-  Training is done by optimizing (i.e., minimizing) standard **(binary) log loss** (<code>fobj=optimize_log_loss</code>)\n",
    "-  Evaluation is measured using standard **(binary) log loss** (<code>feval=avg_log_loss</code>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please, refer to https://slundberg.github.io/shap/notebooks/Census%20income%20classification%20with%20LightGBM.html for any further detail\n",
    "# or\n",
    "# https://medium.com/@pushkarmandot/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc\n",
    "STD_PARAMS = {\n",
    "    \"max_bin\": 511,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"boosting_type\": \"gbdt\",#\"rf\"\n",
    "    \"objective\": \"regression_l2\", #\"binary\",\n",
    "    \"metric\": [\"None\"], # We use our own implementation of binary log loss (i.e., optimize_log_loss) \n",
    "                        # instead of the default one (i.e., \"binary_logloss\"), which may be in fact cross-entropy\n",
    "    \"num_leaves\": 15,\n",
    "    \"verbose\": 1,\n",
    "    \"min_data_in_leaf\": 20,\n",
    "    \"bagging_freq\": 1,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"boost_from_average\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters used for _non-interferent_ learning\n",
    "\n",
    "-  Training is done by optimizing (i.e., minimizing) one of our custom _objective functions_:\n",
    "    -  **(binary) log loss under max attack** (<code>fobj=optimize_log_loss_uma</code>);\n",
    "    -  **weighted sum of (binary) log loss and (binary) log loss under max attack** (<code>fobj=optimize_weighted_sum_log_loss_log_loss_uma</code>);\n",
    "    \n",
    "-  Evaluation is measured using one of our custom _evaluation functions_: \n",
    "    -  **(binary) log loss under max attack** (<code>feval=avg_log_loss_uma</code>);\n",
    "    -  **weighted sum of (binary) log loss and (binary) log loss under max attack** (<code>feval=avg_weighted_sum_log_loss_log_loss_uma</code>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please, refer to https://slundberg.github.io/shap/notebooks/Census%20income%20classification%20with%20LightGBM.html for any further detail\n",
    "# or\n",
    "# https://medium.com/@pushkarmandot/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc\n",
    "NON_INTERFERENT_PARAMS = {\n",
    "    \"max_bin\": 511,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"boosting_type\": \"gbdt\",#\"rf\",\n",
    "    \"objective\": \"regression_l2\",\n",
    "    \"metric\": [\"None\"], # We will specify our own custom objective function (i.e., optimize_binary_logloss_under_max_attack)\n",
    "    \"num_leaves\": 15,\n",
    "    \"verbose\": 1,\n",
    "    \"min_data_in_leaf\": 20,\n",
    "    \"bagging_freq\": 1,\n",
    "    \"bagging_fraction\": 0.8,\n",
    "    \"feature_fraction\": 0.8,\n",
    "    \"boost_from_average\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_BOOST_ROUNDS = 200\n",
    "MIN_BOOST_ROUNDS = 200\n",
    "STEP_BOOST_ROUNDS = 50\n",
    "BOOST_ROUNDS = [br for br in range(MIN_BOOST_ROUNDS, MAX_BOOST_ROUNDS+1, STEP_BOOST_ROUNDS)]\n",
    "STD_ALPHA = 0.0 # alpha weight for standard learning (i.e., the loss coincides with the binary log loss)\n",
    "NON_INTERFERENT_ALPHA = 1.0 # alpha weight for non-interferent learning (i.e., the loss coincides with the binary log loss under max attack)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard objective function\n",
    "\n",
    "The following function, called <code>optimize_log_loss</code>, is the one that should be optimized (i.e., minimized) for learning _standard_ and _baseline_ approaches. More specifically, this is the standard binary log loss which is used to train any _standard_ or _baseline_ model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $L$ = <code>optimize_log_loss</code>\n",
    "\n",
    "$$\n",
    "L = \\frac{1}{|\\mathcal{D}|} \\cdot \\sum_{(\\mathbf{x},y) \\in \\mathcal{D}}\\ell(h(\\mathbf{x}), y)\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\n",
    "\\ell(h(\\mathbf{x}), y) = log(1+e^{(-yh(\\mathbf{x}))})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self-defined objective function\n",
    "# f(preds: array, train_data: Dataset) -> grad: array, hess: array\n",
    "\n",
    "# To be used with a regression task\n",
    "def optimize_log_loss(preds, train_data):\n",
    "    labels = train_data.get_label()\n",
    "    exp_pl = np.exp(preds * labels)\n",
    "    # http://www.wolframalpha.com/input/?i=differentiate+log(1+%2B+exp(-kx)+)\n",
    "    grads = -labels / (1.0 +  exp_pl)  \n",
    "    # http://www.wolframalpha.com/input/?i=d%5E2%2Fdx%5E2+log(1+%2B+exp(-kx)+)\n",
    "    hess = labels**2 * exp_pl / (1.0 + exp_pl)**2 \n",
    "\n",
    "    # this is to optimize average logloss\n",
    "    norm = 1.0/len(preds)\n",
    "    grads *= norm\n",
    "    hess *= norm\n",
    "    \n",
    "    return grads, hess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom objective function\n",
    "\n",
    "In addition to the standard binary log loss used to train a model, we introduce our custom <code>optimize_non_interferent_log_loss</code>, which is computed as the weighted combination of two objective functions, as follows:\n",
    "\n",
    "-  $L$ = <code>optimize_log_loss</code> (standard, already seen above);\n",
    "-  $L^A$ = <code>optimize_log_loss_uma</code> (custom, defined below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $L^A$ = <code>optimize_log_loss_uma</code>\n",
    "\n",
    "This function is used to train a **full** _non-interferent_ model; in other words, full non-interferent models are learned by optimizing (i.e., minimizing) the function which measures the binary log loss **under the maximal attack** possible.\n",
    "\n",
    "$$\n",
    "L^A = \\frac{1}{|\\mathcal{D}|} \\cdot \\sum_{(\\mathbf{x},y) \\in \\mathcal{D}} \\log  \\left( \\sum_{\\mathbf{x}' \\in \\mathit{MaxAtk}({\\mathbf{x}},{A})} e^{\\ell(h(\\mathbf{x}'), y)} \\right).\n",
    "$$\n",
    "\n",
    "where still:\n",
    "\n",
    "$$\n",
    "\\ell(h(\\mathbf{x}), y) = log(1+e^{(-yh(\\mathbf{x}))})\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self-defined objective function\n",
    "# f(preds: array, train_data: Dataset) -> grad: array, hess: array\n",
    "\n",
    "def optimize_log_loss_uma(preds, train_data):\n",
    "    labels = train_data.get_label()\n",
    "    attack_lens = train_data.get_group()\n",
    "    \n",
    "    grads = np.zeros_like(labels, dtype=np.float64)\n",
    "    hess = np.zeros_like(grads)\n",
    "    \n",
    "    if attack_lens is not None:\n",
    "\n",
    "        norm = 1.0 / float(len(attack_lens))\n",
    "\n",
    "        offset = 0\n",
    "        for atk in attack_lens:\n",
    "            exp_pl = np.exp(- preds[offset:offset+atk] * labels[offset:offset+atk])\n",
    "\n",
    "            inv_sum = 1.0 / np.sum(1.0 + exp_pl)\n",
    "\n",
    "            x_grad = inv_sum * exp_pl\n",
    "\n",
    "            grads[offset:offset+atk] = norm * x_grad * (- labels[offset:offset+atk])\n",
    "            hess[offset:offset+atk]  = norm * x_grad * (1.0 - x_grad)\n",
    "\n",
    "            offset += atk    \n",
    "    \n",
    "    return grads, hess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code>optimize_non_interferent_log_loss</code>\n",
    "\n",
    "$$\n",
    "\\alpha\\cdot L^A + (1-\\alpha)\\cdot L\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\alpha \\cdot \\underbrace{\\Bigg[\\frac{1}{|\\mathcal{D}|} \\cdot \\sum_{(\\mathbf{x},y) \\in \\mathcal{D}} \\log  \\left( \\sum_{\\mathbf{x}' \\in \\mathit{MaxAtk}({\\mathbf{x}},{A})} e^{\\ell(h(\\mathbf{x}'), y)} \\right)\\Bigg]}_{L^A} + (1-\\alpha) \\cdot \\underbrace{\\Bigg[\\frac{1}{|\\mathcal{D}|} \\cdot \\sum_{(\\mathbf{x},y) \\in \\mathcal{D}} \\ell(h(\\mathbf{x}, y))\\Bigg]}_{L}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self-defined objective function\n",
    "# f(preds: array, train_data: Dataset) -> grad: array, hess: array\n",
    "\n",
    "def optimize_non_interferent_log_loss(preds, train_data, alpha=1.0):\n",
    "    # binary logloss under maximal attack\n",
    "    grads_uma, hess_uma = optimize_log_loss_uma(preds, train_data)\n",
    "    \n",
    "    # binary logloss (plain)\n",
    "    grads_plain, hess_plain = optimize_log_loss(preds, train_data)\n",
    "    \n",
    "    # combine the above two losses together\n",
    "    grads = alpha*grads_uma + (1.0-alpha)*grads_plain\n",
    "    hess  = alpha*hess_uma  + (1.0-alpha)*hess_plain\n",
    "    \n",
    "    return grads, hess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using one objective function for both _standard_ and _non-interferent_ learning\n",
    "\n",
    "The advantage of the <code>optimize_non_interferent_log_loss</code> function defined above is that we can wrap it so that we can use it as the only objective function (<code>fobj</code>) passed in to LightGBM. \n",
    "\n",
    "In other words, if we call <code>fobj=optimize_non_interferent_log_loss</code> with <code>alpha=0.0</code>, this will end up optimizing (i.e., minimizing) the standard objective function (i.e., the standard binary log loss, defined by the function <code>optimize_log_loss</code> above).\n",
    "\n",
    "Conversely, calling <code>fobj=optimize_non_interferent_log_loss</code> with <code>alpha=1.0</code> turns into optimizing (i.e., minimizing) the full non-interferent objective function (i.e., the custom binary log loss under max attack, defined by the function <code>optimize_log_loss_uma</code> above).\n",
    "\n",
    "Anything that sits in between (i.e., <code>0 < alpha < 1</code>) optimizes an objective function that trades off between the standard and the full non-interferent term."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standard evaluation metric\n",
    "\n",
    "The following function is the one used for evaluating the quality of the learned model (either _standard_, _baseline_, or _non-interferent_). This is the standard <code>avg_log_loss</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic(x):\n",
    "    return 1.0/(1.0 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit(p):\n",
    "    return np.log(p/(1-p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code>avg_log_loss</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self-defined eval metric\n",
    "# f(preds: array, train_data: Dataset) -> name: string, value: array, is_higher_better: bool\n",
    "def avg_log_loss(preds, train_data):\n",
    "    \n",
    "    labels = train_data.get_label()\n",
    "    losses = np.log(1.0 + np.exp(-preds*labels))\n",
    "    avg_loss = np.mean(losses)\n",
    "    \n",
    "    return 'avg_binary_log_loss', avg_loss, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_log_loss(model, boost_round, test, test_groups=None):\n",
    "    \n",
    "    lgbm_test = lightgbm.Dataset(data=test.iloc[:,:-1].values, \n",
    "                                 label=test.iloc[:,-1].values,\n",
    "                                 free_raw_data=False)\n",
    "    \n",
    "    if boost_round < 0: # no trees have been generated (used for evaluating other non-tree-based models like SVM)\n",
    "        # use the logit function (i.e., the inverse of the logistic function) to map probabilities output\n",
    "        # by sklearn's predict_proba in the range [0,1] to a real number in the range [-inf, +inf]\n",
    "        return avg_log_loss(logit(model.predict_proba(test.iloc[:,:-1].values)[:,1]), lgbm_test)[1]\n",
    "    \n",
    "    return avg_log_loss(model.predict(test.iloc[:,:-1].values, num_iteration=boost_round), lgbm_test)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom evaluation metric\n",
    "\n",
    "Similarly to what we have done for <code>fobj</code>, <code>feval</code> can be computed from a weighted combination of two evaluation metrics:\n",
    "\n",
    "-  <code>avg_log_loss</code> (standard, defined above);\n",
    "-  <code>avg_log_loss_uma</code> (custom, defined below)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code>avg_log_loss_uma</code>\n",
    "\n",
    "This is the binary log loss yet modified to operate on groups of perturbed instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our custom metric\n",
    "\n",
    "def binary_log_loss(pred, true_label):\n",
    "\n",
    "    return np.log(1.0 + np.exp(-pred * true_label))\n",
    "\n",
    "# self-defined eval metric\n",
    "# f(preds: array, train_data: Dataset) -> name: string, value: array, is_higher_better: bool\n",
    "def avg_log_loss_uma(preds, train_data):\n",
    "    labels = train_data.get_label()\n",
    "    attack_lens = train_data.get_group()\n",
    "    \n",
    "    offset = 0\n",
    "    max_logloss = []\n",
    "    avg_max_logloss = 0.0\n",
    "    \n",
    "    if attack_lens is not None:\n",
    "    \n",
    "        for atk in attack_lens:\n",
    "            losses = [binary_log_loss(h,t) for h,t in zip(preds[offset:offset+atk], labels[offset:offset+atk])]\n",
    "            max_logloss.append(max(losses))\n",
    "\n",
    "            offset += atk\n",
    "        \n",
    "        avg_max_logloss = np.mean(max_logloss)  \n",
    "\n",
    "    return 'avg_binary_log_loss_under_max_attack', avg_max_logloss, False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_log_loss_uma(model, boost_round, test, test_groups=None):\n",
    "    \n",
    "    lgbm_test = lightgbm.Dataset(data=test.iloc[:,:-1].values, \n",
    "                                 label=test.iloc[:,-1].values,\n",
    "                                 group=test_groups,\n",
    "                                 free_raw_data=False)\n",
    "    \n",
    "    if boost_round < 0: # no trees have been generated (used for evaluating other non-tree-based models like SVM)\n",
    "        # use the logit function (i.e., the inverse of the logistic function) to map probabilities output\n",
    "        # by sklearn's predict_proba in the range [0,1] to a real number in the range [-inf, +inf]\n",
    "        return avg_log_loss_uma(logit(model.predict_proba(test.iloc[:,:-1].values)[:,1]), \n",
    "                                               lgbm_test)[1]\n",
    "    \n",
    "    return avg_log_loss_uma(model.predict(test.iloc[:,:-1].values, num_iteration=boost_round), \n",
    "                                               lgbm_test)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code>feval=avg_non_interferent_log_loss</code>\n",
    "\n",
    "Used for measuring the validity of any model (either _standard_, _baseline_, or _non-interferent_). More precisely, <code>avg_non_interferent_log_loss</code> is the weighted sum of the binary log loss and the binary log loss under maximal attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM takes lambda x,y: avg_weighted_sum_log_loss_log_loss_uma(preds, train_data, alpha=0.5)\n",
    "\n",
    "def avg_non_interferent_log_loss(preds, train_data, alpha=1.0):\n",
    "    \n",
    "    # binary logloss under maximal attack\n",
    "    _, loss_uma, _    = avg_log_loss_uma(preds, train_data)\n",
    "    \n",
    "    # binary logloss (plain)\n",
    "    _, loss_plain, _  = avg_log_loss(preds, train_data)\n",
    "    \n",
    "    # combine the above two losses together\n",
    "    weighted_loss = alpha*loss_uma + (1.0-alpha)*loss_plain\n",
    "\n",
    "    return 'avg_non_interferent_log_loss [alpha={}]'.format(alpha), weighted_loss, False\n",
    "\n",
    "\n",
    "def eval_non_interferent_log_loss(model, boost_round, test, test_groups=None, alpha=1.0):\n",
    "    \n",
    "    lgbm_test = lightgbm.Dataset(data=test.iloc[:,:-1].values, \n",
    "                                 label=test.iloc[:,-1].values,\n",
    "                                 group=test_groups,\n",
    "                                 free_raw_data=False)\n",
    "    \n",
    "    if boost_round < 0: # no trees have been generated (used for evaluating other non-tree-based models like SVM)\n",
    "        # use the logit function (i.e., the inverse of the logistic function) to map probabilities output\n",
    "        # by sklearn's predict_proba in the range [0,1] to a real number in the range [-inf, +inf]\n",
    "        return avg_non_interferent_log_loss(logit(model.predict_proba(test.iloc[:,:-1].values)[:,1]), \n",
    "                                                  lgbm_test,\n",
    "                                                  alpha=alpha\n",
    "                                                 )[1]\n",
    "    \n",
    "    return avg_non_interferent_log_loss(model.predict(test.iloc[:,:-1].values, num_iteration=boost_round), \n",
    "                                                  lgbm_test,\n",
    "                                                  alpha=alpha\n",
    "                                                 )[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional validity measures\n",
    "\n",
    "In addition to the evaluation metrics defined above (used for training), we also consider the following **4** measures of validity to compare the performance of each learned model:\n",
    "\n",
    "-  <code>eval_binary_err_rate</code>: This is the traditional binary error rate (1-accuracy);\n",
    "-  <code>eval_binary_err_rate_uma</code>: This is the binary error rate modified to operate on groups of perturbed instances under maximal attack.\n",
    "-  <code>eval_roc_auc</code>: This is the classical ROC AUC score;\n",
    "-  <code>eval_roc_auc_uma</code>: This is the ROC AUC score modified to operate on groups of perturbed instances under maximal attack.\n",
    "\n",
    "Again, note that those are **not** metrics used at training time (i.e., they do not define any <code>feval</code>), rather they are used to assess the (offline) quality of each learned model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code>eval_binary_err_rate</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def eval_binary_err_rate(model, boost_round, test_set, test_groups=None):\n",
    "    X = test_set.iloc[:,:-1].values\n",
    "    labels = test_set.iloc[:,-1].values\n",
    "    \n",
    "    model_predictions = []\n",
    "    if boost_round < 0: # no trees have been generated (used for evaluating other non-tree-based models like SVM)\n",
    "        # use the logit function (i.e., the inverse of the logistic function) to map probabilities output\n",
    "        # by sklearn's predict_proba in the range [0,1] to a real number in the range [-inf, +inf]\n",
    "        model_predictions = logit(model.predict_proba(X)[:,1])\n",
    "    else:\n",
    "        model_predictions = model.predict(X, num_iteration=boost_round)\n",
    "        \n",
    "    predictions = [1 if p > 0 else -1 for p in model_predictions]\n",
    "    \n",
    "    errs = 0\n",
    "    for p,l in zip(predictions,labels):\n",
    "        if p != l:\n",
    "            errs += 1\n",
    "    return errs/len(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code>eval_binary_err_rate_uma</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_binary_err_rate_uma(model, boost_round, test_set, test_groups=None):\n",
    "    X = test_set.iloc[:,:-1].values\n",
    "    labels = test_set.iloc[:,-1].values\n",
    "    \n",
    "    model_predictions = []\n",
    "    if boost_round < 0: # no trees have been generated (used for evaluating other non-tree-based models like SVM)\n",
    "        # use the logit function (i.e., the inverse of the logistic function) to map probabilities output\n",
    "        # by sklearn's predict_proba in the range [0,1] to a real number in the range [-inf, +inf]\n",
    "        model_predictions = logit(model.predict_proba(X)[:,1])\n",
    "    else:\n",
    "        model_predictions = model.predict(X, num_iteration=boost_round)\n",
    "        \n",
    "    predictions = [1 if p > 0 else -1 for p in model_predictions]\n",
    "    \n",
    "    offset = 0\n",
    "    errs = 0\n",
    "\n",
    "    for g in test_groups:\n",
    "        predictions_att = predictions[offset:offset+g]\n",
    "        true_label = labels[offset]\n",
    "        if np.any([p != true_label for p in predictions_att]):\n",
    "            errs += 1\n",
    "        offset += g\n",
    "\n",
    "    return errs/len(test_groups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code>eval_roc_auc</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def eval_roc_auc(model, boost_round, test_set, test_groups=None):\n",
    "    X = test_set.iloc[:,:-1].values\n",
    "    labels = test_set.iloc[:,-1].values\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    if boost_round < 0: # no trees have been generated (used for evaluating other non-tree-based models like SVM)\n",
    "        # use the logit function (i.e., the inverse of the logistic function) to map probabilities output\n",
    "        # by sklearn's predict_proba in the range [0,1] to a real number in the range [-inf, +inf]\n",
    "        predictions = logit(model.predict_proba(X)[:,1])\n",
    "    else:\n",
    "        predictions = model.predict(X, num_iteration=boost_round)\n",
    "        \n",
    "    \n",
    "    return roc_auc_score(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code>eval_roc_auc_uma</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def eval_roc_auc_uma(model, boost_round, test_set, test_groups=None):\n",
    "    X = test_set.iloc[:,:-1].values\n",
    "    labels = test_set.iloc[:,-1].values\n",
    "    \n",
    "    predictions = []\n",
    "    \n",
    "    if boost_round < 0: # no trees have been generated (used for evaluating other non-tree-based models like SVM)\n",
    "        # use the logit function (i.e., the inverse of the logistic function) to map probabilities output\n",
    "        # by sklearn's predict_proba in the range [0,1] to a real number in the range [-inf, +inf]\n",
    "        predictions = logit(model.predict_proba(X)[:,1])\n",
    "    else:\n",
    "        predictions = model.predict(X, num_iteration=boost_round)\n",
    "    \n",
    "    \n",
    "    offset = 0\n",
    "    true_labels = []\n",
    "    worst_predictions = []\n",
    "    \n",
    "    for g in test_groups:\n",
    "        true_label = labels[offset]\n",
    "        true_labels.append(true_label)\n",
    "        predictions_att = predictions[offset:offset+g]\n",
    "        prediction_distances = np.abs(predictions_att - true_label)\n",
    "        worst_predictions.append(predictions_att[np.argmax(prediction_distances)])\n",
    "    \n",
    "        offset += g\n",
    "        \n",
    "    return roc_auc_score(true_labels, worst_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code>eval_f1</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def eval_f1(model, boost_round, test_set, test_groups=None):\n",
    "    X = test_set.iloc[:,:-1].values\n",
    "    labels = test_set.iloc[:,-1].values\n",
    "    \n",
    "    model_predictions = []\n",
    "    if boost_round < 0: # no trees have been generated (used for evaluating other non-tree-based models like SVM)\n",
    "        # use the logit function (i.e., the inverse of the logistic function) to map probabilities output\n",
    "        # by sklearn's predict_proba in the range [0,1] to a real number in the range [-inf, +inf]\n",
    "        model_predictions = logit(model.predict_proba(X)[:,1])\n",
    "    else:\n",
    "        model_predictions = model.predict(X, num_iteration=boost_round)\n",
    "        \n",
    "    predictions = [1 if p > 0 else -1 for p in model_predictions]\n",
    "    \n",
    "    return f1_score(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <code>eval_f1_uma</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def eval_f1_uma(model, boost_round, test_set, test_groups=None):\n",
    "    X = test_set.iloc[:,:-1].values\n",
    "    labels = test_set.iloc[:,-1].values\n",
    "    \n",
    "    model_predictions = []\n",
    "    if boost_round < 0: # no trees have been generated (used for evaluating other non-tree-based models like SVM)\n",
    "        # use the logit function (i.e., the inverse of the logistic function) to map probabilities output\n",
    "        # by sklearn's predict_proba in the range [0,1] to a real number in the range [-inf, +inf]\n",
    "        model_predictions = logit(model.predict_proba(X)[:,1])\n",
    "    else:\n",
    "        model_predictions = model.predict(X, num_iteration=boost_round)\n",
    "        \n",
    "    predictions = [1 if p > 0 else -1 for p in model_predictions]\n",
    "    \n",
    "    offset = 0\n",
    "    true_labels = []\n",
    "    worst_predictions = []\n",
    "    \n",
    "    for g in test_groups:\n",
    "        true_label = labels[offset]\n",
    "        true_labels.append(true_label)\n",
    "        predictions_att = predictions[offset:offset+g]\n",
    "        prediction_distances = np.abs(predictions_att - true_label)\n",
    "        worst_predictions.append(predictions_att[np.argmax(prediction_distances)])\n",
    "    \n",
    "        offset += g\n",
    "        \n",
    "    return f1_score(true_labels, worst_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save/Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model_filename, model):\n",
    "    with open(model_filename, 'wb') as fout:\n",
    "        pickle.dump(model, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_filename):\n",
    "    with open(model_filename, 'rb') as fin:\n",
    "        return pickle.load(fin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EVAL_METRICS = [eval_log_loss, \n",
    "                eval_binary_err_rate, \n",
    "                eval_roc_auc\n",
    "               ]\n",
    "\n",
    "EVAL_METRICS_UNDER_MAX_ATTACK = [eval_log_loss_uma,\n",
    "                                 eval_binary_err_rate_uma, \n",
    "                                 eval_roc_auc_uma\n",
    "                                ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate each model w.r.t. _all_ evaluation metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_learned_model(model, boost_round, eval_metric, test, test_groups=None):\n",
    "    return eval_metric(model, boost_round, test, test_groups=test_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_learned_models(model, model_type, boost_round, test, test_groups=None):\n",
    "\n",
    "    eval_metrics = EVAL_METRICS\n",
    "    d_test = \"D_test\"\n",
    "    if test_groups is not None:\n",
    "        eval_metrics = EVAL_METRICS_UNDER_MAX_ATTACK\n",
    "        d_test = \"D_test_att\"\n",
    "    \n",
    "    header = ['Model','N. of Trees'] + [m.__name__.replace('eval_','').replace('_',' ').title() for m in eval_metrics]\n",
    "    df = pd.DataFrame(columns=header)\n",
    "    first_row = [model_type, boost_round] + [None for m in eval_metrics]\n",
    "    df.loc[0] = first_row\n",
    "\n",
    "    for eval_metric in eval_metrics:\n",
    "        res = eval_learned_model(model, boost_round, eval_metric, test, test_groups=test_groups)\n",
    "        print(\"{} learning - {} on {} [boost rounds={}] = {:.5f}\"\n",
    "                  .format(model_type, eval_metric.__name__, d_test, boost_round, res))\n",
    "        df[eval_metric.__name__.replace('eval_','').replace('_',' ').title()] = res\n",
    "    print(\"******************************************************************************************************\")\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom non-interferent objective function\n",
    "LOSS_OBJ_FUNC = optimize_non_interferent_log_loss\n",
    "# custom non-interferent evaluation function\n",
    "LOSS_EVAL_FUNC = avg_non_interferent_log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_select_instances(groups, p_attacked_inst, n_attacks_per_inst, seed=73):\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    i = 0\n",
    "    selected_instances = []\n",
    "    for g in groups:\n",
    "        selected_instances.append(i)\n",
    "        if n_attacks_per_inst > 0:\n",
    "            if g > n_attacks_per_inst:\n",
    "                if np.random.random_sample() <= p_attacked_inst: # the instance is going to be attacked\n",
    "                    selected = np.random.choice(g-1, n_attacks_per_inst, replace=False) + i + 1\n",
    "                    selected_instances.extend(sorted(selected))\n",
    "            else:\n",
    "                selected_instances.extend([x for x in range(i+1,i+g)])\n",
    "        i += g\n",
    "    \n",
    "    return selected_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(train, \n",
    "          valid, \n",
    "          params, \n",
    "          fobj,\n",
    "          feval,\n",
    "          num_boost_round,\n",
    "          train_group,\n",
    "          valid_group,\n",
    "          p_attacked_inst, \n",
    "          n_attacks_per_inst\n",
    "         ):\n",
    "    \n",
    "    lgbm_info = {}\n",
    "    \n",
    "    lgbm_train = lightgbm.Dataset(data=train.iloc[:,:-1].values, \n",
    "                                  label=train.iloc[:,-1].values\n",
    "                                 )\n",
    "    \n",
    "    if train_group is not None:\n",
    "        if n_attacks_per_inst > 0:\n",
    "            selected_instances = random_select_instances(train_group, p_attacked_inst, n_attacks_per_inst)\n",
    "            train = train.loc[selected_instances]\n",
    "        \n",
    "            lgbm_train = lightgbm.Dataset(data=train.iloc[:,:-1].values, \n",
    "                                          label=train.iloc[:,-1].values\n",
    "                                         )\n",
    "        else:\n",
    "\n",
    "            lgbm_train = lightgbm.Dataset(data=train.iloc[:,:-1].values, \n",
    "                                          label=train.iloc[:,-1].values, \n",
    "                                          group=train_group\n",
    "                                         )\n",
    "        \n",
    "    \n",
    "    \n",
    "    lgbm_valid = lightgbm.Dataset(data=valid.iloc[:,:-1].values, \n",
    "                                  label=valid.iloc[:,-1].values, \n",
    "                                  reference=lgbm_train, \n",
    "                                  free_raw_data=False\n",
    "                                 )\n",
    "    \n",
    "    if valid_group is not None:\n",
    "        lgbm_valid = lightgbm.Dataset(data=valid.iloc[:,:-1].values, \n",
    "                                      label=valid.iloc[:,-1].values, \n",
    "                                      group=valid_group,\n",
    "                                      reference=lgbm_train, \n",
    "                                      free_raw_data=False\n",
    "                                     )\n",
    "    \n",
    "    lgbm_model = lightgbm.train(params=params, \n",
    "                                train_set=lgbm_train, \n",
    "                                num_boost_round=num_boost_round, \n",
    "                                valid_sets = [lgbm_valid],\n",
    "                                valid_names  = [\"validation\"], \n",
    "                                evals_result = lgbm_info,\n",
    "                                fobj = fobj,\n",
    "                                feval = feval,\n",
    "                                early_stopping_rounds=50,\n",
    "                                verbose_eval=20\n",
    "                               )\n",
    "    \n",
    "    return lgbm_model, lgbm_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_runs(train, \n",
    "                  valid, \n",
    "                  params,\n",
    "                  fobj,\n",
    "                  feval,\n",
    "                  num_boost_round,\n",
    "                  train_group=None,\n",
    "                  valid_group=None,\n",
    "                  is_partial=False,\n",
    "                  p_attacked_inst=1.0,\n",
    "                  n_attacks_per_inst=0,\n",
    "                  run_type=\"Standard\"\n",
    "                 ):\n",
    "    \n",
    "    learning_runs = {}\n",
    "    learning_runs['type'] = run_type\n",
    "    learning_runs['run'] = {}\n",
    "    \n",
    "    fobj_name = \"\" \n",
    "    if not is_partial:\n",
    "        fobj_name = fobj.__name__\n",
    "    else:\n",
    "        fobj_name = fobj.func.__name__\n",
    "    \n",
    "    dataset_name = \"D_train\"\n",
    "    if train_group is not None:\n",
    "        dataset_name += \"_att\"\n",
    "    \n",
    "    for br in num_boost_round:\n",
    "        print(\"***** {} learning - Optimizing `{}` on {} [boost rounds={}; p_attacked_inst={:.2f}; n_attacks_per_inst={}] *****\"\n",
    "              .format(learning_runs['type'], fobj_name, dataset_name, br, p_attacked_inst, n_attacks_per_inst))\n",
    "        model, res = learn(train, \n",
    "                           valid, \n",
    "                           params,\n",
    "                           fobj,\n",
    "                           feval,\n",
    "                           br,\n",
    "                           train_group,\n",
    "                           valid_group,\n",
    "                           p_attacked_inst, \n",
    "                           n_attacks_per_inst\n",
    "                          )\n",
    "        learning_runs['run'][br] = {}\n",
    "        learning_runs['run'][br]['model'] = model\n",
    "        learning_runs['run'][br]['results'] = res\n",
    "\n",
    "    return learning_runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. _Standard_ Learning: Models are trained on the original dataset $\\mathcal{D}_{train}$ using _standard_ binary log loss\n",
    "\n",
    "-  This model is trained on the original training set by minimizing standard **binary log loss** (i.e., <code>fobj=optimize_non_interferent_log_loss</code> with <code>alpha=0.0</code>)\n",
    "\n",
    "-  Its performance is assessed by means of <code>feval=avg_non_interferent_log_loss</code>, still with <code>alpha=0.0</code>, which results into the standard <code>avg_log_loss</code> (i.e., the metric optimized during training) both on training and validation set.\n",
    "\n",
    "-  The model leading to the lowest **binary log loss** on the validation test is the one returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "std_runs = learning_runs(TRAIN, \n",
    "                          VALID,\n",
    "                          STD_PARAMS, \n",
    "                          functools.partial(LOSS_OBJ_FUNC, alpha=STD_ALPHA),\n",
    "                          functools.partial(LOSS_EVAL_FUNC, alpha=STD_ALPHA),\n",
    "                          BOOST_ROUNDS,\n",
    "                          is_partial=True,\n",
    "                          run_type=\"Standard\"\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STD_MODEL_FILENAME = MODELS_PATH+\"/std_\"+ATTACKER+\"_\"+str(MAX_BOOST_ROUNDS)+\".pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persist _standard_ model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(STD_MODEL_FILENAME, std_runs['run'][MAX_BOOST_ROUNDS]['model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. _Baseline_: Learning models trained on the attacked $\\mathcal{D}_{train\\_att}$ using _standard_ binary log loss\n",
    "\n",
    "-  This model is trained on the attacked training set by minimizing standard **binary log loss** (i.e., <code>fobj=optimize_non_interferent_log_loss</code> with <code>alpha=0.0</code>)\n",
    "\n",
    "-  Its performance is assessed by means of <code>feval=avg_non_interferent_log_loss</code>, still with <code>alpha=0.0</code>, which results into the standard <code>avg_log_loss</code> (i.e., the metric optimized during training) both on training and validation set.\n",
    "\n",
    "-  The model leading to the lowest **binary log loss** on the validation test is the one returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_ATTACKED_INSTANCE = [0.25, 0.5, 0.75, 1.0]\n",
    "N_ATTACKS_PER_INSTANCE = [1, 1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Persist _baseline_ model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for pa in P_ATTACKED_INSTANCE:\n",
    "    for na in N_ATTACKS_PER_INSTANCE:\n",
    "        baseline_runs = learning_runs(TRAIN_ATT, \n",
    "                                       VALID_ATT, \n",
    "                                       STD_PARAMS,\n",
    "                                       functools.partial(LOSS_OBJ_FUNC, alpha=STD_ALPHA),\n",
    "                                       functools.partial(LOSS_EVAL_FUNC, alpha=STD_ALPHA),\n",
    "                                       BOOST_ROUNDS,\n",
    "                                       train_group=TRAIN_ATT_OFFSETS,\n",
    "                                       is_partial=True,\n",
    "                                       p_attacked_inst=pa, \n",
    "                                       n_attacks_per_inst=na,\n",
    "                                       run_type=\"Baseline\"\n",
    "                                    )\n",
    "\n",
    "        BASELINE_MODEL_FILENAME = MODELS_PATH+\"/baseline_\"+ATTACKER+\"_\"+str(MAX_BOOST_ROUNDS)+\"_p-{}_n-{}.pkl\".format(int(pa*100), na)\n",
    "        save_model(BASELINE_MODEL_FILENAME, baseline_runs['run'][MAX_BOOST_ROUNDS]['model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. _Full-Non-Interferent_: Learn full _non-interferent_ models trained on the attacked $\\mathcal{D}_{train\\_att}$ using _only_ our custom objective function (binary log loss under max attack)\n",
    "\n",
    "-  This model is trained on the original training set by minimizing our custom objective function, i.e., **binary log loss under max attack** (i.e., <code>fobj=optimize_non_interferent_log_loss</code> with <code>alpha=1.0</code>).\n",
    "\n",
    "-  Its performance is assessed by means of <code>feval=avg_non_interferent_log_loss</code>, still with <code>alpha=1.0</code>, which results into the custom <code>avg_log_loss_uma</code>, both on training and validation set.\n",
    "\n",
    "-  The model leading to the lowest <code>avg_log_loss_uma</code> on the validation test is the one returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "full_non_interferent_runs = learning_runs(TRAIN_ATT, \n",
    "                                     VALID_ATT,\n",
    "                                     NON_INTERFERENT_PARAMS,\n",
    "                                     functools.partial(LOSS_OBJ_FUNC, alpha=NON_INTERFERENT_ALPHA),\n",
    "                                     functools.partial(LOSS_EVAL_FUNC, alpha=NON_INTERFERENT_ALPHA),\n",
    "                                     BOOST_ROUNDS,\n",
    "                                     train_group=TRAIN_ATT_OFFSETS,\n",
    "                                     valid_group=VALID_ATT_OFFSETS,\n",
    "                                     is_partial=True,\n",
    "                                     run_type=\"Full-Non-Interferent\"\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL_NON_INTERFERENT_MODEL_FILENAME = MODELS_PATH+\"/full-non-interferent_\"+ATTACKER+\"_\"+str(MAX_BOOST_ROUNDS)+\".pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persist _full-non-interferent_ model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(FULL_NON_INTERFERENT_MODEL_FILENAME, full_non_interferent_runs['run'][MAX_BOOST_ROUNDS]['model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. _Non-Interferent_ weighted: Learn _non-interferent_ models trained on the attacked $\\mathcal{D}_{train\\_att}$ using custom cost function (weighted sum of log loss and log loss under max attack)\n",
    "\n",
    "-  This model is trained on the original training set by minimizing our custom objective function, i.e., the weighted sum of the standard **binary log loss** and **binary log loss under max attack** (i.e., <code>fobj=optimize_non_interferent_log_loss</code> with <code>0 < alpha < 1</code>).\n",
    "\n",
    "-  Its performance is assessed by means of <code>feval=avg_non_interferent_log_loss</code>, still with the same value of <code>alpha</code> used during training, which results into the custom <code>avg_non_interferent_log_loss</code>, both on training and validation set.\n",
    "\n",
    "-  The model leading to the lowest <code>avg_non_interferent_log_loss</code> on the validation test is the one returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHAS = [0.10, 0.25, 0.50, 0.75, 0.90]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Persist _non-interferent_ weighted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "for alpha in ALPHAS:\n",
    "    weighted_non_interferent_runs = learning_runs(TRAIN_ATT, \n",
    "                                              VALID_ATT, \n",
    "                                              NON_INTERFERENT_PARAMS,\n",
    "                                              functools.partial(LOSS_OBJ_FUNC, alpha=alpha),\n",
    "                                              functools.partial(LOSS_EVAL_FUNC, alpha=alpha),\n",
    "                                              BOOST_ROUNDS,\n",
    "                                              train_group=TRAIN_ATT_OFFSETS,\n",
    "                                              valid_group=VALID_ATT_OFFSETS,\n",
    "                                              is_partial=True,\n",
    "                                              run_type=\"Weighted-Non-Interferent\"\n",
    "                                             )\n",
    "    WEIGHTED_NON_INTERFERENT_MODEL_FILENAME = MODELS_PATH+\"/weighted-non-interferent_\"+ATTACKER+\"_\"+str(MAX_BOOST_ROUNDS)+\"_alpha-{}.pkl\".format(int(alpha*100))\n",
    "    save_model(WEIGHTED_NON_INTERFERENT_MODEL_FILENAME, weighted_non_interferent_runs['run'][MAX_BOOST_ROUNDS]['model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. SVM: Learn SVM classifier trained on the original $\\mathcal{D}_{\\text{train}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_HYPERPARAMS = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10],\n",
    "    #'gamma': [0.001, 0.01, 0.1, 1]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svm(X, y, hyperparams, nfolds=5, scoring='neg_log_loss'):\n",
    "    grid_search = GridSearchCV(SVC(kernel='rbf', probability=True), \n",
    "                               hyperparams, \n",
    "                               cv=nfolds, \n",
    "                               scoring=scoring, \n",
    "                               n_jobs=-1, \n",
    "                               verbose=4)\n",
    "    grid_search.fit(X, y)\n",
    "    \n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = TRAIN_VALID.iloc[:,:-1].values\n",
    "y = TRAIN_VALID.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y[y == -1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_grid = train_svm(X, y, SVM_HYPERPARAMS) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Persist SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_C = svm_grid.best_params_['C']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_MODEL_FILENAME = MODELS_PATH+\"/svm_\"+ATTACKER+\"_C-{}.pkl\".format(str(best_C).replace('.',''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(SVM_MODEL_FILENAME, svm_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_EVAL_TREES = 200\n",
    "MIN_EVAL_TREES = 10\n",
    "STEP_EVAL_TREES = 10\n",
    "EVAL_TREES = sorted(list(set([t for t in range(MIN_EVAL_TREES, MAX_EVAL_TREES, STEP_EVAL_TREES)] + [MAX_EVAL_TREES])))\n",
    "# The following adds the \"best_iteration\" learned on the validation set\n",
    "EVAL_TREES = [0] + EVAL_TREES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_runs(model, model_type, test, eval_trees=EVAL_TREES, test_groups=None):\n",
    "    eval_results = []\n",
    "    for t in eval_trees:\n",
    "        eval_results.append(eval_learned_models(model, model_type, t, test, test_groups=test_groups))\n",
    "        \n",
    "    eval_df = pd.concat(eval_results, axis=0)\n",
    "    eval_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    return eval_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve all model files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_filenames():\n",
    "    return sorted([f for f in listdir(MODELS_PATH) if f != '.gitignore' and isfile(join(MODELS_PATH, f))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_model_filenames = get_model_filenames()\n",
    "print(\"\\n\".join([mf for mf in all_model_filenames]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load _standard_ models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STD_MODEL_FILENAME = MODELS_PATH+\"/std_\"+ATTACKER+\"_\"+str(MAX_BOOST_ROUNDS)+\".pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_model = load_model(STD_MODEL_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate _standard_ models on $D_{test}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "std_df = eval_runs(std_model, \"Standard\", TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate _standard_ models on $D_{test\\_att}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_att_df = eval_runs(std_model, \"Standard\", TEST_ATT, test_groups=TEST_ATT_OFFSETS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge both _standard_ evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_std_df = pd.merge(std_df, std_att_df, on=['Model', 'N. of Trees'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_std_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load _baseline_ models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASELINE_MODEL_100_1_FILENAME = MODELS_PATH+\"/baseline_\"+ATTACKER+\"_\"+str(MAX_BOOST_ROUNDS)+\"_p-100_n-1.pkl\"\n",
    "#BASELINE_MODEL_100_4_FILENAME = MODELS_PATH+\"/baseline_\"+ATTACKER+\"_\"+str(MAX_BOOST_ROUNDS)+\"_p-100_n-4.pkl\"\n",
    "BASELINE_MODEL_100_MAX_FILENAME = MODELS_PATH+\"/baseline_\"+ATTACKER+\"_\"+str(MAX_BOOST_ROUNDS)+\"_p-100_n-1000.pkl\"\n",
    "#BASELINE_MODEL_50_1_FILENAME = MODELS_PATH+\"/baseline_\"+ATTACKER+\"+\"_\"+str(MAX_BOOST_ROUNDS)+\"_p-50_n-1.pkl\"\n",
    "#BASELINE_MODEL_50_4_FILENAME = MODELS_PATH+\"/baseline_\"+ATTACKER+\"_\"+str(MAX_BOOST_ROUNDS)+\"_p-50_n-4.pkl\"\n",
    "#BASELINE_MODEL_50_MAX_FILENAME = MODELS_PATH+\"/baseline_\"+ATTACKER+\"_\"+str(MAX_BOOST_ROUNDS)+\"_p-50_n-1000.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model_100_1 = load_model(BASELINE_MODEL_100_1_FILENAME)\n",
    "#baseline_model_100_4 = load_model(BASELINE_MODEL_100_4_FILENAME)\n",
    "baseline_model_100_MAX = load_model(BASELINE_MODEL_100_MAX_FILENAME)\n",
    "# baseline_model_50_1 = load_model(BASELINE_MODEL_50_1_FILENAME)\n",
    "# baseline_model_50_4 = load_model(BASELINE_MODEL_50_4_FILENAME)\n",
    "# baseline_model_50_MAX = load_model(BASELINE_MODEL_50_MAX_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate _baseline_ models on $D_{test}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_100_1_df = eval_runs(baseline_model_100_1, \"Baseline [p=1.0; n=1]\", TEST)\n",
    "# baseline_100_4_df = eval_runs(baseline_model_100_4, \"Baseline [p=1.0; n=4]\", TEST)\n",
    "baseline_100_max_df = eval_runs(baseline_model_100_MAX, \"Baseline [p=1.0; n=max]\", TEST)\n",
    "# baseline_50_1_df = eval_runs(baseline_model_50_1, \"Baseline [p=0.5; n=1]\", TEST)\n",
    "# baseline_50_4_df = eval_runs(baseline_model_50_4, \"Baseline [p=0.5; n=4]\", TEST)\n",
    "# baseline_50_max_df = eval_runs(baseline_model_50_MAX, \"Baseline [p=0.5; n=max]\", TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_df = pd.concat(#[baseline_100_1_df, baseline_100_4_df, baseline_100_max_df, baseline_50_1_df, baseline_50_4_df, baseline_50_max_df], \n",
    "                        [baseline_100_1_df, baseline_100_max_df], \n",
    "                        axis=0)\n",
    "baseline_df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate _baseline_ model on $D_{test\\_att}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "baseline_att_100_1_df = eval_runs(baseline_model_100_1, \"Baseline [p=1.0; n=1]\", TEST_ATT, test_groups=TEST_ATT_OFFSETS)\n",
    "#baseline_att_100_4_df = eval_runs(baseline_model_100_4, \"Baseline [p=1.0; n=4]\", TEST_ATT, test_groups=TEST_ATT_OFFSETS)\n",
    "baseline_att_100_max_df = eval_runs(baseline_model_100_MAX, \"Baseline [p=1.0; n=max]\", TEST_ATT, test_groups=TEST_ATT_OFFSETS)\n",
    "#baseline_att_50_1_df = eval_runs(baseline_model_50_1, \"Baseline [p=0.5; n=1]\", TEST_ATT, test_groups=TEST_ATT_OFFSETS)\n",
    "#baseline_att_50_4_df = eval_runs(baseline_model_50_4, \"Baseline [p=0.5; n=4]\", TEST_ATT, test_groups=TEST_ATT_OFFSETS)\n",
    "#baseline_att_50_max_df = eval_runs(baseline_model_50_MAX, \"Baseline [p=0.5; n=max]\", TEST_ATT, test_groups=TEST_ATT_OFFSETS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_att_df = pd.concat(# [baseline_att_100_1_df, baseline_att_100_4_df, baseline_att_100_max_df, baseline_att_50_1_df, baseline_att_50_4_df, baseline_att_50_max_df]\n",
    "    [baseline_att_100_1_df, baseline_att_100_max_df], axis=0)\n",
    "baseline_att_df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_att_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge both _baseline_ evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_baseline_df = pd.merge(baseline_df, baseline_att_df, on=[\"Model\", \"N. of Trees\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_baseline_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load _full-non-interferent_ model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL_NON_INTERFERENT_MODEL_FILENAME = MODELS_PATH+\"/full-non-interferent_\"+ATTACKER+\"_\"+str(MAX_BOOST_ROUNDS)+\".pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_non_interferent_model = load_model(FULL_NON_INTERFERENT_MODEL_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate _full-non-interferent_ model on $D_{test}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_non_interferent_df = eval_runs(full_non_interferent_model, \"Full-Non-Interferent\", TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate _full-non-interferent_ model on $D_{test\\_att}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "full_non_interferent_att_df = eval_runs(full_non_interferent_model, \"Full-Non-Interferent\", TEST_ATT, test_groups=TEST_ATT_OFFSETS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge both _full-non-interferent_ evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_full_non_interferent_df = pd.merge(full_non_interferent_df, full_non_interferent_att_df, on=['Model', 'N. of Trees'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load _weighted-non-interferent_ models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WEIGHTED_NON_INTERFERENT_MODEL_10_FILENAME = MODELS_PATH+\"/weighted-non-interferent_\"+ATTACKER+\"_\"+str(MAX_BOOST_ROUNDS)+\"_alpha-10.pkl\"\n",
    "WEIGHTED_NON_INTERFERENT_MODEL_25_FILENAME = MODELS_PATH+\"/weighted-non-interferent_\"+ATTACKER+\"_\"+str(MAX_BOOST_ROUNDS)+\"_alpha-25.pkl\"\n",
    "WEIGHTED_NON_INTERFERENT_MODEL_50_FILENAME = MODELS_PATH+\"/weighted-non-interferent_\"+ATTACKER+\"_\"+str(MAX_BOOST_ROUNDS)+\"_alpha-50.pkl\"\n",
    "WEIGHTED_NON_INTERFERENT_MODEL_75_FILENAME = MODELS_PATH+\"/weighted-non-interferent_\"+ATTACKER+\"_\"+str(MAX_BOOST_ROUNDS)+\"_alpha-75.pkl\"\n",
    "WEIGHTED_NON_INTERFERENT_MODEL_90_FILENAME = MODELS_PATH+\"/weighted-non-interferent_\"+ATTACKER+\"_\"+str(MAX_BOOST_ROUNDS)+\"_alpha-90.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_non_interferent_model_10 = load_model(WEIGHTED_NON_INTERFERENT_MODEL_10_FILENAME)\n",
    "weighted_non_interferent_model_25 = load_model(WEIGHTED_NON_INTERFERENT_MODEL_25_FILENAME)\n",
    "weighted_non_interferent_model_50 = load_model(WEIGHTED_NON_INTERFERENT_MODEL_50_FILENAME)\n",
    "weighted_non_interferent_model_75 = load_model(WEIGHTED_NON_INTERFERENT_MODEL_75_FILENAME)\n",
    "weighted_non_interferent_model_90 = load_model(WEIGHTED_NON_INTERFERENT_MODEL_90_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate _weighted-non-interferent_ model on $D_{test}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_non_interferent_10_df = eval_runs(weighted_non_interferent_model_10, \"Weighted-Non-Interferent [alpha=0.10]\", TEST)\n",
    "weighted_non_interferent_25_df = eval_runs(weighted_non_interferent_model_25, \"Weighted-Non-Interferent [alpha=0.25]\", TEST)\n",
    "weighted_non_interferent_50_df = eval_runs(weighted_non_interferent_model_50, \"Weighted-Non-Interferent [alpha=0.50]\", TEST)\n",
    "weighted_non_interferent_75_df = eval_runs(weighted_non_interferent_model_75, \"Weighted-Non-Interferent [alpha=0.75]\", TEST)\n",
    "weighted_non_interferent_90_df = eval_runs(weighted_non_interferent_model_90, \"Weighted-Non-Interferent [alpha=0.90]\", TEST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_non_interferent_df = pd.concat([weighted_non_interferent_10_df, \n",
    "                                             weighted_non_interferent_25_df,\n",
    "                                             weighted_non_interferent_50_df,\n",
    "                                             weighted_non_interferent_75_df,\n",
    "                                             weighted_non_interferent_90_df], axis=0)\n",
    "\n",
    "weighted_non_interferent_df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate _weighted-non-interferent_ model on $D_{test\\_att}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "weighted_non_interferent_att_10_df = eval_runs(weighted_non_interferent_model_10, \"Weighted-Non-Interferent [alpha=0.10]\", TEST_ATT, test_groups=TEST_ATT_OFFSETS)\n",
    "weighted_non_interferent_att_25_df = eval_runs(weighted_non_interferent_model_25, \"Weighted-Non-Interferent [alpha=0.25]\", TEST_ATT, test_groups=TEST_ATT_OFFSETS)\n",
    "weighted_non_interferent_att_50_df = eval_runs(weighted_non_interferent_model_50, \"Weighted-Non-Interferent [alpha=0.50]\", TEST_ATT, test_groups=TEST_ATT_OFFSETS)\n",
    "weighted_non_interferent_att_75_df = eval_runs(weighted_non_interferent_model_75, \"Weighted-Non-Interferent [alpha=0.75]\", TEST_ATT, test_groups=TEST_ATT_OFFSETS)\n",
    "weighted_non_interferent_att_90_df = eval_runs(weighted_non_interferent_model_90, \"Weighted-Non-Interferent [alpha=0.90]\", TEST_ATT, test_groups=TEST_ATT_OFFSETS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_non_interferent_att_df = pd.concat([weighted_non_interferent_att_10_df, \n",
    "                                             weighted_non_interferent_att_25_df,\n",
    "                                             weighted_non_interferent_att_50_df,\n",
    "                                             weighted_non_interferent_att_75_df,\n",
    "                                             weighted_non_interferent_att_90_df], axis=0)\n",
    "\n",
    "weighted_non_interferent_att_df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge both _weighted-non-interferent_ evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_weighted_non_interferent_df = pd.merge(weighted_non_interferent_df, weighted_non_interferent_att_df, on=['Model', 'N. of Trees'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load SVM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVM_MODEL_FILENAME = MODELS_PATH+\"/svm_\"+ATTACKER+\"_C-10.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model = load_model(SVM_MODEL_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate SVM model on $D_{test}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "svm_df = eval_runs(svm_model, \"SVM [C=10]\", TEST, eval_trees=[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate SVM model on $D_{test\\_att}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "svm_att_df = eval_runs(svm_model, \"SVM [C=10]\", TEST_ATT, test_groups=TEST_ATT_OFFSETS, eval_trees=[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merge both SVM evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_svm_df = pd.merge(svm_df, svm_att_df, on=['Model', 'N. of Trees'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stack _all_ evaluations one on top of each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_df = pd.concat([overall_full_non_interferent_df, \n",
    "                        overall_weighted_non_interferent_df,\n",
    "                        overall_svm_df,\n",
    "                        overall_baseline_df, \n",
    "                        overall_std_df], axis=0)\n",
    "\n",
    "overall_df.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the DataFrame containing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_df.to_csv(\"../plots/\"+ATTACKER+\".csv\", sep=\",\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
