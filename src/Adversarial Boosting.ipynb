{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- True implementation\n",
    "  - LightGBM (scikit learn)\n",
    "  - Loop: attack & train\n",
    "    - annotate full-attacks dataset with costs\n",
    "    - generate attacked datasets\n",
    "    - continue training from previous forest with new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "import lightgbm\n",
    "#import functools\n",
    "#from os import listdir\n",
    "#from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS_PATH = \"../data/census\"\n",
    "MODELS_PATH = \"../out/models\"\n",
    "ATTACKER = \"strong\" # weak\n",
    "TRAINING_SET=\"train_ori.csv.bz2\" # original training set\n",
    "TRAINING_SET_ATT=\"train_\"+ATTACKER+\"_att.csv.bz2\" # perturbed training set\n",
    "VALIDATION_SET=\"valid_ori.csv.bz2\" # original validation set\n",
    "VALIDATION_SET_ATT=\"valid_\"+ATTACKER+\"_att.csv.bz2\" # perturbed validation set\n",
    "TEST_SET=\"test_ori.csv.bz2\" # original test set\n",
    "TEST_SET_ATT=\"test_\"+ATTACKER+\"_att.csv.bz2\" # perturbed test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path, dataset_filename, sep=\",\"):\n",
    "    return pd.read_csv(path+\"/\"+dataset_filename, sep=sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_categorical_features(dataset):\n",
    "    categorical_features = []\n",
    "    for column in dataset.columns:\n",
    "        if dataset[column].dtype == 'object':\n",
    "            categorical_features.append(column)\n",
    "    return categorical_features\n",
    "            \n",
    "def label_encode(dataset, categorical_features):\n",
    "    dataset_le = dataset.copy()\n",
    "    for column in dataset_le.columns:\n",
    "        if column in categorical_features:\n",
    "            dataset_le[column] = dataset_le[column].astype('category')\n",
    "            dataset_le[column] = dataset_le[column].cat.codes\n",
    "    return dataset_le"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = load_dataset(DATASETS_PATH, TRAINING_SET)\n",
    "TRAIN_ATT = load_dataset(DATASETS_PATH, TRAINING_SET_ATT)\n",
    "\n",
    "VALID = load_dataset(DATASETS_PATH, VALIDATION_SET)\n",
    "VALID_ATT = load_dataset(DATASETS_PATH, VALIDATION_SET_ATT)\n",
    "\n",
    "TEST = load_dataset(DATASETS_PATH, TEST_SET)\n",
    "TEST_ATT = load_dataset(DATASETS_PATH, TEST_SET_ATT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_ATT_OFFSETS = TRAIN_ATT['instance_id'].value_counts().sort_index().values\n",
    "VALID_ATT_OFFSETS = VALID_ATT['instance_id'].value_counts().sort_index().values\n",
    "TEST_ATT_OFFSETS = TEST_ATT['instance_id'].value_counts().sort_index().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of categorical features: [workclass, education, marital_status, occupation, relationship, race, sex, native_country]\n",
      "List of categorical features: [workclass, education, marital_status, occupation, relationship, race, sex, native_country]\n",
      "List of categorical features: [workclass, education, marital_status, occupation, relationship, race, sex, native_country]\n",
      "List of categorical features: [workclass, education, marital_status, occupation, relationship, race, sex, native_country]\n",
      "List of categorical features: [workclass, education, marital_status, occupation, relationship, race, sex, native_country]\n",
      "List of categorical features: [workclass, education, marital_status, occupation, relationship, race, sex, native_country]\n"
     ]
    }
   ],
   "source": [
    "def process_categorical_features(dataset):\n",
    "    fx = infer_categorical_features(dataset)\n",
    "    print(\"List of categorical features: [{}]\"\n",
    "          .format(\", \".join([cf for cf in fx])))\n",
    "    return label_encode(dataset, set(fx))\n",
    "\n",
    "TRAIN = process_categorical_features(TRAIN)\n",
    "TRAIN_ATT = process_categorical_features(TRAIN_ATT.iloc[:,1:])\n",
    "\n",
    "VALID = process_categorical_features(VALID)\n",
    "VALID_ATT = process_categorical_features(VALID_ATT.iloc[:,1:])\n",
    "\n",
    "TEST = process_categorical_features(TEST)\n",
    "TEST_ATT = process_categorical_features(TEST_ATT.iloc[:,1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load some model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model_filename, model):\n",
    "    with open(model_filename, 'wb') as fout:\n",
    "        pickle.dump(model, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_filename):\n",
    "    with open(model_filename, 'rb') as fin:\n",
    "        return pickle.load(fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_model = load_model(\"../out/models/std_strong_200.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AdvBoosting_gen_data(model, data, groups):\n",
    "    ''' \n",
    "    model  : is the LightGBM Model\n",
    "    data   : data matrix with all valid attacks (last column is label)\n",
    "    groups : grouping of same attacked instance \n",
    "    returns the new data matrix and new groups\n",
    "    \n",
    "    WARNING: currently works only for binary classification\n",
    "    '''\n",
    "    # score the datataset\n",
    "    labels = data.iloc[:,-1]\n",
    "    \n",
    "    predictions = model.predict(data.iloc[:,:-1]) # exclude labels\n",
    "    # binarize\n",
    "    predictions = (predictions>0).astype(np.float)\n",
    "    predictions = 2*predictions - 1\n",
    "    \n",
    "    # check mispredictions\n",
    "    matchings = labels * predictions\n",
    "    \n",
    "    # select original data + attacked instances\n",
    "    new_selected = [] # id of selected instances\n",
    "    new_groups   = []\n",
    "    \n",
    "    offset = 0\n",
    "    for g in groups:\n",
    "        if g==1:\n",
    "            # there are not attacks, just add original\n",
    "            new_selected += [offset]\n",
    "            new_groups   += [1]\n",
    "        else:\n",
    "            # get a slice of the matching scores\n",
    "            g_matchings = matchings[offset:offset+g].values\n",
    "\n",
    "            # most misclassified (smallest margin)\n",
    "            # skip original\n",
    "            adv_instance = np.argmin(g_matchings[1:])+1\n",
    "\n",
    "            # add original and adversarial\n",
    "            new_selected += [offset, adv_instance]\n",
    "            new_groups   += [2]\n",
    "        \n",
    "        offset += g\n",
    "    \n",
    "    new_dataset = data.iloc[new_selected,:]\n",
    "    \n",
    "    return new_dataset, new_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AdvBoosting_extend_model(model, data):\n",
    "    ''' \n",
    "    model  : is the LightGBM Model\n",
    "    data   : data matrix with all valid attacks (last column is label)\n",
    "    returns the new model (is model modified inplace?)\n",
    "    '''\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "adv_data, _ = AdvBoosting_gen_data(std_model, TRAIN_ATT, TRAIN_ATT_OFFSETS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_model = AdvBoosting_extend_model(std_model, adv_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
