{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def infer_categorical_features(dataset):\n",
    "#     categorical_features = set([])\n",
    "#     for column in dataset.columns:\n",
    "#         if dataset[column].dtype == 'object':\n",
    "#             categorical_features.add(column)\n",
    "#     return categorical_features\n",
    "            \n",
    "def label_encode(dataset, categorical_features):\n",
    "    dataset_le = dataset.copy()\n",
    "    for column in dataset_le.columns:\n",
    "        if column in categorical_features:\n",
    "            dataset_le[column] = dataset_le[column].astype('category')\n",
    "            dataset_le[column] = dataset_le[column].cat.codes.astype(np.int32)\n",
    "    return dataset_le\n",
    "\n",
    "def load_atk_train_valid_test( atk_train_file, atk_valid_file, atk_test_file, \n",
    "                               train_split=0.6, valid_split=0.2,\n",
    "                               force=False):\n",
    "    \n",
    "    \n",
    "    if  ( force or \n",
    "          not os.path.exists(atk_train_file+\".cat.bz2\") or\n",
    "          not os.path.exists(atk_valid_file+\".cat.bz2\") or\n",
    "          not os.path.exists(atk_test_file+\".cat.bz2\") or \n",
    "          not os.path.exists(atk_train_file+\".cat.json\") ):\n",
    "    \n",
    "        print (\"Pre-processing original files...\")\n",
    "\n",
    "        print (\"Loading:\", atk_train_file)\n",
    "        print (\"Loading:\", atk_valid_file)\n",
    "        print (\"Loading:\", atk_test_file)\n",
    "\n",
    "        train = pd.read_csv(atk_train_file)\n",
    "        valid = pd.read_csv(atk_valid_file)\n",
    "        test  = pd.read_csv(atk_test_file)\n",
    "        \n",
    "        print (\"Train/Valid/Test sizes:\", train.shape, valid.shape, test.shape)\n",
    "        print (\"Train/Valid/Test split: {:.2f} {:.2f} {:.2f}\"\n",
    "                   .format( train.shape[0]/(train.shape[0]+valid.shape[0]+test.shape[0]),\n",
    "                            valid.shape[0]/(train.shape[0]+valid.shape[0]+test.shape[0]),\n",
    "                            test.shape[0] /(train.shape[0]+valid.shape[0]+test.shape[0]) ) )\n",
    "\n",
    "        # concat to process correctly label encoding\n",
    "        full = pd.concat( [train, valid, test] )\n",
    "\n",
    "        # get index of categorical features (-1 because of instance_id)\n",
    "        cat_fx = full.columns.values[ np.where(full.dtypes=='object')[0] ]\n",
    "        cat_fx = list(cat_fx)    \n",
    "        full = label_encode(full, cat_fx)\n",
    "        with open(atk_train_file+\".cat.json\", 'w') as fp:\n",
    "            json.dump(cat_fx, fp)\n",
    "        print (\"CatFX:\", cat_fx)\n",
    "\n",
    "        # split-back into train valid test\n",
    "        train_size = int( full.shape[0]*train_split )\n",
    "        valid_size = int( full.shape[0]*valid_split )\n",
    "        train_cat = full.iloc[0:train_size,:]\n",
    "        valid_cat = full.iloc[train_size:train_size+valid_size,:]\n",
    "        test_cat  = full.iloc[train_size+valid_size:,:]    \n",
    "\n",
    "        print (\"Train/Valid/Test sizes:\", train_cat.shape, valid_cat.shape, test_cat.shape)\n",
    "        print (\"Train/Valid/Test split: {:.2f} {:.2f} {:.2f}\"\n",
    "                   .format( train_cat.shape[0]/(train_cat.shape[0]+valid_cat.shape[0]+test_cat.shape[0]),\n",
    "                            valid_cat.shape[0]/(train_cat.shape[0]+valid_cat.shape[0]+test_cat.shape[0]),\n",
    "                            test_cat.shape[0] /(train_cat.shape[0]+valid_cat.shape[0]+test_cat.shape[0]) ) )\n",
    "\n",
    "        # save to file\n",
    "        print (\"Saving processed files *.cat.bz2\")\n",
    "        train_cat.to_csv(atk_train_file+\".cat.bz2\", compression=\"bz2\", index=False)\n",
    "        valid_cat.to_csv(atk_valid_file+\".cat.bz2\", compression=\"bz2\", index=False)\n",
    "        test_cat.to_csv (atk_test_file+\".cat.bz2\",  compression=\"bz2\", index=False)\n",
    "        \n",
    "    else:\n",
    "        print (\"Loading pre-processed files...\")\n",
    "\n",
    "        train_cat = pd.read_csv(atk_train_file+\".cat.bz2\")\n",
    "        valid_cat = pd.read_csv(atk_valid_file+\".cat.bz2\")\n",
    "        test_cat  = pd.read_csv(atk_test_file+\".cat.bz2\")\n",
    "        \n",
    "        with open(atk_train_file+\".cat.json\", 'r') as fp:\n",
    "            cat_fx = json.load(fp)\n",
    "    \n",
    "    # return data\n",
    "    return train_cat, valid_cat, test_cat, cat_fx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AdvBoosting_gen_data(model, data, groups):\n",
    "    ''' \n",
    "    model  : is the LightGBM Model\n",
    "    data   : data matrix with all valid attacks (last column is label)\n",
    "    groups : grouping of same attacked instance \n",
    "    returns the new data matrix and new groups\n",
    "    \n",
    "    WARNING: currently works only for binary classification\n",
    "    '''\n",
    "    # score the datataset\n",
    "    labels = data[:,-1]\n",
    "    \n",
    "    predictions = model.predict(data[:,:-1]) # exclude labels\n",
    "    # binarize\n",
    "    predictions = (predictions>0).astype(np.float)\n",
    "    predictions = 2*predictions - 1\n",
    "    \n",
    "    # check mispredictions\n",
    "    matchings = labels * predictions\n",
    "    \n",
    "    # select original data + attacked instances\n",
    "    new_selected = [] # id of selected instances\n",
    "    new_groups   = []\n",
    "    \n",
    "    offset = 0\n",
    "    for g in groups:\n",
    "        if g==0:\n",
    "            print (\"Error !!!!\")\n",
    "        elif g==1:\n",
    "            # there are no attacks, just add original\n",
    "            new_selected += [offset]\n",
    "            new_groups   += [1]\n",
    "        else:\n",
    "            # get a slice of the matching scores\n",
    "            g_matchings = matchings[offset:offset+g]\n",
    "\n",
    "            # most misclassified (smallest margin)\n",
    "            # skip original\n",
    "            adv_instance = np.argmin(g_matchings[1:])+1\n",
    "\n",
    "            # add original and adversarial\n",
    "            new_selected += [offset, adv_instance]\n",
    "            new_groups   += [2]\n",
    "        \n",
    "        offset += g\n",
    "    \n",
    "    new_dataset = data[new_selected,:]\n",
    "    \n",
    "    return new_dataset, new_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our custom metrics\n",
    "def binary_log_loss(pred, true_label):\n",
    "\n",
    "    return np.log(1.0 + np.exp(-pred * true_label))\n",
    "\n",
    "# self-defined eval metric\n",
    "# f(preds: array, train_data: Dataset) -> name: string, value: array, is_higher_better: bool\n",
    "def avg_log_loss(preds, train_data):\n",
    "    \n",
    "    labels = train_data.get_label()\n",
    "    losses = np.log(1.0 + np.exp(-preds*labels))\n",
    "    avg_loss = np.mean(losses)\n",
    "    \n",
    "    return 'avg_binary_log_loss', avg_loss, False\n",
    "\n",
    "\n",
    "def optimize_log_loss(preds, train_data):\n",
    "    labels = train_data.get_label()\n",
    "    exp_pl = np.exp(preds * labels)\n",
    "    # http://www.wolframalpha.com/input/?i=differentiate+log(1+%2B+exp(-kx)+)\n",
    "    grads = -labels / (1.0 +  exp_pl)  \n",
    "    # http://www.wolframalpha.com/input/?i=d%5E2%2Fdx%5E2+log(1+%2B+exp(-kx)+)\n",
    "    hess = labels**2 * exp_pl / (1.0 + exp_pl)**2 \n",
    "\n",
    "    # this is to optimize average logloss\n",
    "    norm = 1.0/len(preds)\n",
    "    grads *= norm\n",
    "    hess *= norm\n",
    "    \n",
    "    return grads, hess\n",
    "\n",
    "\n",
    "# self-defined eval metric\n",
    "# f(preds: array, train_data: Dataset) -> name: string, value: array, is_higher_better: bool\n",
    "def avg_log_loss_uma(preds, train_data):\n",
    "    labels = train_data.get_label()\n",
    "    attack_lens = train_data.get_group()\n",
    "    \n",
    "    offset = 0\n",
    "    max_logloss = []\n",
    "    avg_max_logloss = 0.0\n",
    "    \n",
    "    if attack_lens is not None:\n",
    "    \n",
    "        for atk in attack_lens:\n",
    "            losses = [binary_log_loss(h,t) for h,t in zip(preds[offset:offset+atk], labels[offset:offset+atk])]\n",
    "            max_logloss.append(max(losses))\n",
    "\n",
    "            offset += atk\n",
    "        \n",
    "        avg_max_logloss = np.mean(max_logloss)  \n",
    "\n",
    "    return 'avg_binary_log_loss_under_max_attack', avg_max_logloss, False\n",
    "\n",
    "def avg_non_interferent_log_loss(preds, train_data, alpha=1.0):\n",
    "    \n",
    "    # binary logloss under maximal attack\n",
    "    _, loss_uma, _    = avg_log_loss_uma(preds, train_data)\n",
    "    \n",
    "    # binary logloss (plain)\n",
    "    _, loss_plain, _  = avg_log_loss(preds, train_data)\n",
    "    \n",
    "    # combine the above two losses together\n",
    "    weighted_loss = alpha*loss_uma + (1.0-alpha)*loss_plain\n",
    "\n",
    "    return 'avg_non_interferent_log_loss [alpha={}]'.format(alpha), weighted_loss, False\n",
    "\n",
    "def optimize_log_loss_uma(preds, train_data):\n",
    "    labels = train_data.get_label()\n",
    "    attack_lens = train_data.get_group()\n",
    "    \n",
    "    grads = np.zeros_like(labels, dtype=np.float64)\n",
    "    hess = np.zeros_like(grads)\n",
    "    \n",
    "    if attack_lens is not None:\n",
    "\n",
    "        norm = 1.0 / float(len(attack_lens))\n",
    "\n",
    "        offset = 0\n",
    "        for atk in attack_lens:\n",
    "            exp_pl = np.exp(- preds[offset:offset+atk] * labels[offset:offset+atk])\n",
    "\n",
    "            inv_sum = 1.0 / np.sum(1.0 + exp_pl)\n",
    "\n",
    "            x_grad = inv_sum * exp_pl\n",
    "\n",
    "            grads[offset:offset+atk] = norm * x_grad * (- labels[offset:offset+atk])\n",
    "            hess[offset:offset+atk]  = norm * x_grad * (1.0 - x_grad)\n",
    "\n",
    "            offset += atk    \n",
    "    \n",
    "    return grads, hess\n",
    "\n",
    "def optimize_non_interferent_log_loss(preds, train_data, alpha=1.0):\n",
    "    # binary logloss under maximal attack\n",
    "    grads_uma, hess_uma = optimize_log_loss_uma(preds, train_data)\n",
    "    \n",
    "    # binary logloss (plain)\n",
    "    grads_plain, hess_plain = optimize_log_loss(preds, train_data)\n",
    "    \n",
    "    # combine the above two losses together\n",
    "    grads = alpha*grads_uma + (1.0-alpha)*grads_plain\n",
    "    hess  = alpha*hess_uma  + (1.0-alpha)*hess_plain\n",
    "    \n",
    "    return grads, hess\n",
    "\n",
    "\n",
    "\n",
    "def AdvBoosting_extend_model(data, cat_fx, input_model=None, num_trees=1, params=None):\n",
    "    ''' \n",
    "    model  : is the LightGBM Model\n",
    "    data   : data matrix with all valid attacks (last column is label)\n",
    "    returns the new model (is model modified inplace?)\n",
    "    '''\n",
    "    \n",
    "    if cat_fx is None or len(cat_fx)==0:\n",
    "        cat_fx = \"auto\"\n",
    "\n",
    "    lgbm_train = lightgbm.Dataset(data=data[:,:-1], \n",
    "                                  label=data[:,-1],\n",
    "                                  categorical_feature = cat_fx)\n",
    "    \n",
    "    lgbm_info = {}\n",
    "    lgbm_model = lightgbm.train(params, lgbm_train, \n",
    "                                num_boost_round = num_trees, \n",
    "                                init_model = input_model,\n",
    "                                fobj = optimize_log_loss, \n",
    "                                feval = avg_log_loss,\n",
    "                                evals_result = lgbm_info,\n",
    "                                valid_sets   = [lgbm_train], \n",
    "                                valid_names  = ['adv-train'],\n",
    "                                verbose_eval=10)\n",
    "\n",
    "    return lgbm_model, lgbm_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AdvBoosting( atk_train, trees, \n",
    "                 cat_fx,\n",
    "                 params,\n",
    "                 output_model_file,\n",
    "                 partial_save=10, \n",
    "                 adv_rounds=1):\n",
    "    ''' \n",
    "    atk_data: full dataset including all valid attacks\n",
    "    atk_groups: lenght of each attack set\n",
    "    trees: total number of trees to be produced\n",
    "    adv_rounds: adversarial instance injecting frequency\n",
    "    '''\n",
    "    # temp lgbm file\n",
    "    temp = output_model_file+\".tmp\"\n",
    "    \n",
    "    # get groups and remove instance ids\n",
    "    atk_groups = atk_train['instance_id'].value_counts().sort_index().values\n",
    "    atk_train.drop('instance_id', axis=1, inplace=True)\n",
    "    \n",
    "    # get index of categorical features \n",
    "    cat_fx = np.where(atk_train.columns.isin(cat_fx) )[0]\n",
    "    cat_fx = list([int(x) for x in cat_fx])  \n",
    "    # print (\"CatFX:\", atk_train.columns.values[cat_fx])\n",
    "\n",
    "    # prepare data (avoiding pandas)\n",
    "    atk_data   = atk_train.values\n",
    "\n",
    "    # train first trees\n",
    "    original_ids = np.cumsum(atk_groups[:-1])\n",
    "    original_ids = np.insert(original_ids, 0, 0)\n",
    "    \n",
    "    model, model_info = AdvBoosting_extend_model( atk_data[original_ids, :], \n",
    "                                                  cat_fx=cat_fx,\n",
    "                                                  input_model=None, \n",
    "                                                  num_trees=adv_rounds, \n",
    "                                                  params=params )\n",
    "    \n",
    "    \n",
    "    # train remaining trees\n",
    "    for t in range(adv_rounds+1, trees+1, adv_rounds):\n",
    "        # attack dataset\n",
    "        adv_data, adv_offsets = AdvBoosting_gen_data( model, atk_data, atk_groups )\n",
    "        \n",
    "        # train additional trees\n",
    "        model.save_model(temp)\n",
    "        model, model_info = AdvBoosting_extend_model( adv_data, \n",
    "                                                      cat_fx=cat_fx,\n",
    "                                                      input_model=temp, \n",
    "                                                      num_trees=adv_rounds, \n",
    "                                                      params=params)\n",
    "        # save partial model\n",
    "        if t%partial_save==0 and t!=trees:\n",
    "            partial_filename = \"{}.T{:03d}.lgbm\".format(output_model_file, t)\n",
    "            model.save_model( filename=partial_filename )\n",
    "            \n",
    "    model.save_model(filename=output_model_file)\n",
    "    \n",
    "    return model, model_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scripting wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training( atk_train_file, atk_valid_file, atk_test_file,\n",
    "                  output_model_file,\n",
    "                  num_trees,\n",
    "                  learning_rate,\n",
    "                  num_leaves,\n",
    "                  partial_save,\n",
    "                  adversarial_rounds):\n",
    "\n",
    "    # load train/valid/test\n",
    "    train, valid, test, cat_fx = load_atk_train_valid_test(atk_train_file, atk_valid_file, atk_test_file)\n",
    "    \n",
    "    # train params\n",
    "    lgbm_params = { 'learning_rate': float(learning_rate), \n",
    "                    'num_leaves': int(num_leaves)} \n",
    "\n",
    "    # start training\n",
    "    return AdvBoosting(  train, \n",
    "                         trees=num_trees, \n",
    "                         cat_fx = cat_fx,\n",
    "                         output_model_file=output_model_file, \n",
    "                         adv_rounds=adversarial_rounds, \n",
    "                         partial_save=partial_save, \n",
    "                         params=lgbm_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Baseline\n",
    "\n",
    "Some results:\n",
    "\n",
    "       learning rate  num leaves  num trees  avg_binary_log_loss\n",
    "    0            0.1        16.0      196.0             0.297940\n",
    "    1            0.1        32.0      197.0             0.301215\n",
    "    2            0.5        16.0       81.0             0.297984\n",
    "    3            0.5        32.0       22.0             0.305828\n",
    "    \n",
    "I chose the following setting:\n",
    "\n",
    "        learning rate  num leaves  num trees  avg_binary_log_loss\n",
    "    2            0.5        16.0       81.0             0.297984\n",
    "\n",
    "The final model is saved here: `../out/models/lgbm_census_T81_L16_S050.model`\n",
    "\n",
    "**No need to run this again**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gradient_boosting_baseline( train_file, valid_file, test_file,\n",
    "                                      output_model_file):\n",
    "    \n",
    "    exp = pd.DataFrame(columns=['learning rate', 'num leaves', 'num trees', 'avg_binary_log_loss'])\n",
    "    \n",
    "    # load train/valid/test\n",
    "    train, valid, test, cat_fx = load_atk_train_valid_test(train_file, valid_file, test_file)\n",
    "\n",
    "    # get index of categorical features \n",
    "    cat_fx = np.where(train.columns.isin(cat_fx) )[0]\n",
    "    cat_fx = list([int(x) for x in cat_fx])  \n",
    "    print (\"CatFX:\", train.columns.values[cat_fx])\n",
    "    \n",
    "\n",
    "    num_trees     = 200\n",
    "    for learning_rate in [0.1, 0.5]:\n",
    "        for num_leaves in [16, 32]:\n",
    "    \n",
    "            # datasets\n",
    "            lgbm_train = lightgbm.Dataset(data=train.values[:,:-1], \n",
    "                                          label=train.values[:,-1],\n",
    "                                          categorical_feature = cat_fx)\n",
    "\n",
    "            lgbm_valid = lightgbm.Dataset(data=valid.values[:,:-1], \n",
    "                                          label=valid.values[:,-1],\n",
    "                                          categorical_feature = cat_fx)\n",
    "\n",
    "            # run train\n",
    "            lgbm_params = { 'learning_rate': learning_rate, \n",
    "                            'num_leaves': num_leaves} \n",
    "            lgbm_info = {}\n",
    "            lgbm_model = lightgbm.train(lgbm_params, lgbm_train, \n",
    "                                        num_boost_round = num_trees,\n",
    "                                        fobj            = optimize_log_loss, \n",
    "                                        feval           = avg_log_loss,\n",
    "                                        evals_result    = lgbm_info,\n",
    "                                        valid_sets      = [lgbm_train, lgbm_valid], \n",
    "                                        valid_names     = ['train', 'valid'],\n",
    "                                        verbose_eval    = 10)\n",
    "\n",
    "            # save file\n",
    "            best_valid_iter = np.argmin(lgbm_info['valid']['avg_binary_log_loss'] )\n",
    "\n",
    "            model_file_name = \"{:s}_T{:d}_L{:d}_S{:03d}.model\".format(output_model_file,\n",
    "                                                               best_valid_iter+1,\n",
    "                                                               num_leaves,\n",
    "                                                               int(learning_rate*100) )\n",
    "            lgbm_model.save_model(model_file_name, num_iteration=best_valid_iter+1)\n",
    "            print (\"Model saved to\", model_file_name)\n",
    "\n",
    "            # update experimental results\n",
    "            exp = exp.append( {  'learning rate':learning_rate, \n",
    "                                 'num leaves':num_leaves, \n",
    "                                 'num trees':best_valid_iter+1, \n",
    "                                 'avg_binary_log_loss':lgbm_info['valid']['avg_binary_log_loss'][best_valid_iter] },\n",
    "                             ignore_index=True)\n",
    "    \n",
    "    return exp\n",
    "\n",
    "# enable/disable LGBM Baseline\n",
    "if False:\n",
    "    experiments = train_gradient_boosting_baseline ( \"../data/census/train_ori.csv.bz2\",\n",
    "                                                     \"../data/census/valid_ori.csv.bz2\",\n",
    "                                                     \"../data/census/test_ori.csv.bz2\",\n",
    "                                                     \"../out/models/lgbm_census\")  \n",
    "\n",
    "    experiments.to_csv('LGBM_Census_Baseline.csv')\n",
    "\n",
    "    print (experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversatial Boosting Training\n",
    "\n",
    "**Nota**: Adversarial Boosing mi pare a volte pi√π veloce della baseline. E' corretto ?!?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_adversarial_boosting( train_file, valid_file, test_file,\n",
    "                                output_model_file,\n",
    "                                learning_rate=0.5,\n",
    "                                num_leaves=16):\n",
    "    \n",
    "    exp = pd.DataFrame(columns=['learning rate', 'num leaves', 'num trees', 'avg_binary_log_loss'])\n",
    "    \n",
    "    # load train/valid/test\n",
    "    train, valid, test, cat_fx = load_atk_train_valid_test(train_file, valid_file, test_file)\n",
    "\n",
    "    # get index of categorical features \n",
    "    cat_fx = np.where(train.columns.isin(cat_fx) )[0]\n",
    "    cat_fx = list([int(x) for x in cat_fx])  \n",
    "    print (\"CatFX:\", train.columns.values[cat_fx])\n",
    "    \n",
    "\n",
    "    for num_trees in [500]:\n",
    "        # datasets\n",
    "        lgbm_train = lightgbm.Dataset(data=train.values[:,:-1], \n",
    "                                      label=train.values[:,-1],\n",
    "                                      categorical_feature = cat_fx)\n",
    "\n",
    "        lgbm_valid = lightgbm.Dataset(data=valid.values[:,:-1], \n",
    "                                      label=valid.values[:,-1],\n",
    "                                      categorical_feature = cat_fx)\n",
    "\n",
    "        # run train\n",
    "        lgbm_params = { 'learning_rate': learning_rate, \n",
    "                        'num_leaves': num_leaves} \n",
    "        lgbm_info = {}\n",
    "        lgbm_model = lightgbm.train(lgbm_params, lgbm_train, \n",
    "                                    num_boost_round = num_trees,\n",
    "                                    fobj            = optimize_log_loss, \n",
    "                                    feval           = avg_log_loss,\n",
    "                                    evals_result    = lgbm_info,\n",
    "                                    valid_sets      = [lgbm_train, lgbm_valid], \n",
    "                                    valid_names     = ['train', 'valid'],\n",
    "                                    verbose_eval    = 10)\n",
    "        \n",
    "        # save file\n",
    "        best_valid_iter = np.argmin(lgbm_info['valid']['avg_binary_log_loss'] )\n",
    "\n",
    "        model_file_name = \"{:s}_T{:d}_L{:d}_S{:03d}.model\".format(output_model_file,\n",
    "                                                           best_valid_iter+1,\n",
    "                                                           num_leaves,\n",
    "                                                           int(learning_rate*100) )\n",
    "        lgbm_model.save_model(model_file_name, num_iteration=best_valid_iter+1)\n",
    "        print (\"Model saved to\", model_file_name)\n",
    "\n",
    "        # update experimental results\n",
    "        exp = exp.append( {  'learning rate':learning_rate, \n",
    "                             'num leaves':num_leaves, \n",
    "                             'num trees':best_valid_iter+1, \n",
    "                             'avg_binary_log_loss':lgbm_info['valid']['avg_binary_log_loss'][best_valid_iter] },\n",
    "                         ignore_index=True)\n",
    "    \n",
    "    return exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable/disable\n",
    "if False:\n",
    "    experiments = train_adversarial_boosting ( \"../data/census/train_ori.csv.bz2\",\n",
    "                                               \"../data/census/valid_ori.csv.bz2\",\n",
    "                                               \"../data/census/test_ori.csv.bz2\",\n",
    "                                               \"../out/models/AdvBoost_census\")  \n",
    "\n",
    "    experiments.to_csv('AdvBoosting_Census_Baseline.csv')\n",
    "\n",
    "    print (experiments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Under Attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-processed files...\n",
      "CatFX: ['workclass' 'marital_status' 'occupation' 'relationship' 'race' 'sex'\n",
      " 'native_country']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucchese/.local/lib/python3.6/site-packages/lightgbm/basic.py:1036: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "/home/lucchese/.local/lib/python3.6/site-packages/lightgbm/basic.py:681: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\ttrain's avg_binary_log_loss: 0.0881606\tvalid's avg_binary_log_loss: 0.088576\n",
      "[20]\ttrain's avg_binary_log_loss: 0.0845659\tvalid's avg_binary_log_loss: 0.0859332\n",
      "[30]\ttrain's avg_binary_log_loss: 0.0763656\tvalid's avg_binary_log_loss: 0.0786738\n",
      "[40]\ttrain's avg_binary_log_loss: 0.0753332\tvalid's avg_binary_log_loss: 0.078013\n",
      "[50]\ttrain's avg_binary_log_loss: 0.0746124\tvalid's avg_binary_log_loss: 0.0777822\n",
      "[60]\ttrain's avg_binary_log_loss: 0.0685185\tvalid's avg_binary_log_loss: 0.0721167\n",
      "[70]\ttrain's avg_binary_log_loss: 0.0679041\tvalid's avg_binary_log_loss: 0.0720017\n",
      "[80]\ttrain's avg_binary_log_loss: 0.0673566\tvalid's avg_binary_log_loss: 0.0719959\n",
      "[90]\ttrain's avg_binary_log_loss: 0.0668565\tvalid's avg_binary_log_loss: 0.0719737\n",
      "[100]\ttrain's avg_binary_log_loss: 0.0663494\tvalid's avg_binary_log_loss: 0.0719345\n",
      "[110]\ttrain's avg_binary_log_loss: 0.063515\tvalid's avg_binary_log_loss: 0.0692359\n",
      "[120]\ttrain's avg_binary_log_loss: 0.0631388\tvalid's avg_binary_log_loss: 0.0691593\n",
      "[130]\ttrain's avg_binary_log_loss: 0.0627718\tvalid's avg_binary_log_loss: 0.0691056\n",
      "[140]\ttrain's avg_binary_log_loss: 0.0604481\tvalid's avg_binary_log_loss: 0.0670232\n",
      "[150]\ttrain's avg_binary_log_loss: 0.060021\tvalid's avg_binary_log_loss: 0.0669903\n",
      "[160]\ttrain's avg_binary_log_loss: 0.0597015\tvalid's avg_binary_log_loss: 0.0669692\n",
      "[170]\ttrain's avg_binary_log_loss: 0.059411\tvalid's avg_binary_log_loss: 0.0668654\n",
      "[180]\ttrain's avg_binary_log_loss: 0.0591395\tvalid's avg_binary_log_loss: 0.066937\n",
      "[190]\ttrain's avg_binary_log_loss: 0.0587708\tvalid's avg_binary_log_loss: 0.0668978\n",
      "[200]\ttrain's avg_binary_log_loss: 0.0585194\tvalid's avg_binary_log_loss: 0.0668603\n",
      "[210]\ttrain's avg_binary_log_loss: 0.0582744\tvalid's avg_binary_log_loss: 0.0668523\n",
      "[220]\ttrain's avg_binary_log_loss: 0.0580182\tvalid's avg_binary_log_loss: 0.0668846\n",
      "[230]\ttrain's avg_binary_log_loss: 0.0577458\tvalid's avg_binary_log_loss: 0.0669527\n",
      "[240]\ttrain's avg_binary_log_loss: 0.0575189\tvalid's avg_binary_log_loss: 0.0670136\n",
      "[250]\ttrain's avg_binary_log_loss: 0.0572653\tvalid's avg_binary_log_loss: 0.0670034\n",
      "[260]\ttrain's avg_binary_log_loss: 0.0559672\tvalid's avg_binary_log_loss: 0.0657246\n",
      "[270]\ttrain's avg_binary_log_loss: 0.0557573\tvalid's avg_binary_log_loss: 0.0657387\n",
      "[280]\ttrain's avg_binary_log_loss: 0.0555357\tvalid's avg_binary_log_loss: 0.0658293\n",
      "[290]\ttrain's avg_binary_log_loss: 0.0543662\tvalid's avg_binary_log_loss: 0.0648241\n",
      "[300]\ttrain's avg_binary_log_loss: 0.0541506\tvalid's avg_binary_log_loss: 0.0649063\n",
      "[310]\ttrain's avg_binary_log_loss: 0.0539545\tvalid's avg_binary_log_loss: 0.0649324\n",
      "[320]\ttrain's avg_binary_log_loss: 0.0537533\tvalid's avg_binary_log_loss: 0.0650398\n",
      "[330]\ttrain's avg_binary_log_loss: 0.0535406\tvalid's avg_binary_log_loss: 0.0650155\n",
      "[340]\ttrain's avg_binary_log_loss: 0.0533918\tvalid's avg_binary_log_loss: 0.0649955\n",
      "[350]\ttrain's avg_binary_log_loss: 0.0531947\tvalid's avg_binary_log_loss: 0.0650415\n",
      "[360]\ttrain's avg_binary_log_loss: 0.0529921\tvalid's avg_binary_log_loss: 0.0650476\n",
      "[370]\ttrain's avg_binary_log_loss: 0.0523381\tvalid's avg_binary_log_loss: 0.0646298\n",
      "[380]\ttrain's avg_binary_log_loss: 0.0521705\tvalid's avg_binary_log_loss: 0.0646814\n",
      "[390]\ttrain's avg_binary_log_loss: 0.0519976\tvalid's avg_binary_log_loss: 0.0647798\n",
      "[400]\ttrain's avg_binary_log_loss: 0.0518361\tvalid's avg_binary_log_loss: 0.0649013\n",
      "[410]\ttrain's avg_binary_log_loss: 0.0516936\tvalid's avg_binary_log_loss: 0.0649276\n",
      "[420]\ttrain's avg_binary_log_loss: 0.0515471\tvalid's avg_binary_log_loss: 0.0649378\n",
      "[430]\ttrain's avg_binary_log_loss: 0.0514203\tvalid's avg_binary_log_loss: 0.0650031\n",
      "[440]\ttrain's avg_binary_log_loss: 0.0512774\tvalid's avg_binary_log_loss: 0.0650404\n",
      "[450]\ttrain's avg_binary_log_loss: 0.051136\tvalid's avg_binary_log_loss: 0.0650327\n",
      "[460]\ttrain's avg_binary_log_loss: 0.0509741\tvalid's avg_binary_log_loss: 0.0650516\n",
      "[470]\ttrain's avg_binary_log_loss: 0.0508241\tvalid's avg_binary_log_loss: 0.0650478\n",
      "[480]\ttrain's avg_binary_log_loss: 0.0507048\tvalid's avg_binary_log_loss: 0.0651385\n",
      "[490]\ttrain's avg_binary_log_loss: 0.0505732\tvalid's avg_binary_log_loss: 0.0652007\n",
      "[500]\ttrain's avg_binary_log_loss: 0.0504393\tvalid's avg_binary_log_loss: 0.0652961\n",
      "Model saved to ../out/models/AdvBoost_census_B5_T365_L16_S050.model\n",
      "   learning rate  num leaves  num trees  avg_binary_log_loss\n",
      "0            0.5        16.0      365.0             0.064527\n",
      "Pre-processing original files...\n",
      "Loading: ../data/census/train_B15.csv.bz2\n",
      "Loading: ../data/census/valid_B15.csv.bz2\n",
      "Loading: ../data/census/test_B15.csv.bz2\n",
      "Train/Valid/Test sizes: (418102, 15) (45598, 15) (232444, 15)\n",
      "Train/Valid/Test split: 0.60 0.07 0.33\n",
      "CatFX: ['workclass', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'native_country']\n",
      "Train/Valid/Test sizes: (417686, 15) (139228, 15) (139230, 15)\n",
      "Train/Valid/Test split: 0.60 0.20 0.20\n",
      "Saving processed files *.cat.bz2\n",
      "CatFX: ['workclass' 'marital_status' 'occupation' 'relationship' 'race' 'sex'\n",
      " 'native_country']\n",
      "[10]\ttrain's avg_binary_log_loss: 0.0524283\tvalid's avg_binary_log_loss: 0.0528968\n",
      "[20]\ttrain's avg_binary_log_loss: 0.0482649\tvalid's avg_binary_log_loss: 0.0488407\n",
      "[30]\ttrain's avg_binary_log_loss: 0.0473341\tvalid's avg_binary_log_loss: 0.0480381\n",
      "[40]\ttrain's avg_binary_log_loss: 0.0467708\tvalid's avg_binary_log_loss: 0.0475028\n",
      "[50]\ttrain's avg_binary_log_loss: 0.0463078\tvalid's avg_binary_log_loss: 0.0471539\n",
      "[60]\ttrain's avg_binary_log_loss: 0.0460151\tvalid's avg_binary_log_loss: 0.0469851\n",
      "[70]\ttrain's avg_binary_log_loss: 0.0457606\tvalid's avg_binary_log_loss: 0.0468059\n",
      "[80]\ttrain's avg_binary_log_loss: 0.0455297\tvalid's avg_binary_log_loss: 0.0466903\n",
      "[90]\ttrain's avg_binary_log_loss: 0.0409954\tvalid's avg_binary_log_loss: 0.0423635\n",
      "[100]\ttrain's avg_binary_log_loss: 0.0379966\tvalid's avg_binary_log_loss: 0.0393485\n",
      "[110]\ttrain's avg_binary_log_loss: 0.0377064\tvalid's avg_binary_log_loss: 0.0391339\n",
      "[120]\ttrain's avg_binary_log_loss: 0.0375809\tvalid's avg_binary_log_loss: 0.0390455\n",
      "[130]\ttrain's avg_binary_log_loss: 0.0374594\tvalid's avg_binary_log_loss: 0.0389926\n",
      "[140]\ttrain's avg_binary_log_loss: 0.0373584\tvalid's avg_binary_log_loss: 0.0389757\n",
      "[150]\ttrain's avg_binary_log_loss: 0.037262\tvalid's avg_binary_log_loss: 0.0389617\n",
      "[160]\ttrain's avg_binary_log_loss: 0.0371828\tvalid's avg_binary_log_loss: 0.0389748\n",
      "[170]\ttrain's avg_binary_log_loss: 0.0370751\tvalid's avg_binary_log_loss: 0.0389709\n",
      "[180]\ttrain's avg_binary_log_loss: 0.0369829\tvalid's avg_binary_log_loss: 0.0389382\n",
      "[190]\ttrain's avg_binary_log_loss: 0.0354288\tvalid's avg_binary_log_loss: 0.0374375\n",
      "[200]\ttrain's avg_binary_log_loss: 0.0353588\tvalid's avg_binary_log_loss: 0.0374278\n",
      "[210]\ttrain's avg_binary_log_loss: 0.0342573\tvalid's avg_binary_log_loss: 0.0363528\n",
      "[220]\ttrain's avg_binary_log_loss: 0.0341573\tvalid's avg_binary_log_loss: 0.0363261\n",
      "[230]\ttrain's avg_binary_log_loss: 0.034085\tvalid's avg_binary_log_loss: 0.0363075\n",
      "[240]\ttrain's avg_binary_log_loss: 0.0340083\tvalid's avg_binary_log_loss: 0.0362753\n",
      "[250]\ttrain's avg_binary_log_loss: 0.0339502\tvalid's avg_binary_log_loss: 0.0362631\n",
      "[260]\ttrain's avg_binary_log_loss: 0.0338965\tvalid's avg_binary_log_loss: 0.036269\n",
      "[270]\ttrain's avg_binary_log_loss: 0.03385\tvalid's avg_binary_log_loss: 0.0362795\n",
      "[280]\ttrain's avg_binary_log_loss: 0.0337816\tvalid's avg_binary_log_loss: 0.0363022\n",
      "[290]\ttrain's avg_binary_log_loss: 0.033725\tvalid's avg_binary_log_loss: 0.0362833\n",
      "[300]\ttrain's avg_binary_log_loss: 0.0336621\tvalid's avg_binary_log_loss: 0.0362617\n",
      "[310]\ttrain's avg_binary_log_loss: 0.0336038\tvalid's avg_binary_log_loss: 0.0362813\n",
      "[320]\ttrain's avg_binary_log_loss: 0.0335364\tvalid's avg_binary_log_loss: 0.0362879\n",
      "[330]\ttrain's avg_binary_log_loss: 0.0334787\tvalid's avg_binary_log_loss: 0.0363018\n",
      "[340]\ttrain's avg_binary_log_loss: 0.0334276\tvalid's avg_binary_log_loss: 0.0362964\n",
      "[350]\ttrain's avg_binary_log_loss: 0.0333921\tvalid's avg_binary_log_loss: 0.0362753\n",
      "[360]\ttrain's avg_binary_log_loss: 0.0333507\tvalid's avg_binary_log_loss: 0.0362643\n",
      "[370]\ttrain's avg_binary_log_loss: 0.0332983\tvalid's avg_binary_log_loss: 0.036284\n",
      "[380]\ttrain's avg_binary_log_loss: 0.0332493\tvalid's avg_binary_log_loss: 0.0362791\n",
      "[390]\ttrain's avg_binary_log_loss: 0.0331992\tvalid's avg_binary_log_loss: 0.0362988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[400]\ttrain's avg_binary_log_loss: 0.0331553\tvalid's avg_binary_log_loss: 0.0362885\n",
      "[410]\ttrain's avg_binary_log_loss: 0.0331064\tvalid's avg_binary_log_loss: 0.0363127\n",
      "[420]\ttrain's avg_binary_log_loss: 0.0330635\tvalid's avg_binary_log_loss: 0.0363284\n",
      "[430]\ttrain's avg_binary_log_loss: 0.0330199\tvalid's avg_binary_log_loss: 0.0363315\n",
      "[440]\ttrain's avg_binary_log_loss: 0.0329757\tvalid's avg_binary_log_loss: 0.0363178\n",
      "[450]\ttrain's avg_binary_log_loss: 0.0329426\tvalid's avg_binary_log_loss: 0.0363124\n",
      "[460]\ttrain's avg_binary_log_loss: 0.0324043\tvalid's avg_binary_log_loss: 0.0357645\n",
      "[470]\ttrain's avg_binary_log_loss: 0.0323734\tvalid's avg_binary_log_loss: 0.035769\n",
      "[480]\ttrain's avg_binary_log_loss: 0.0317113\tvalid's avg_binary_log_loss: 0.0350373\n",
      "[490]\ttrain's avg_binary_log_loss: 0.0316763\tvalid's avg_binary_log_loss: 0.0350377\n",
      "[500]\ttrain's avg_binary_log_loss: 0.0312486\tvalid's avg_binary_log_loss: 0.034587\n",
      "Model saved to ../out/models/AdvBoost_census_B15_T500_L16_S050.model\n",
      "   learning rate  num leaves  num trees  avg_binary_log_loss\n",
      "0            0.5        16.0      500.0             0.034587\n",
      "Pre-processing original files...\n",
      "Loading: ../data/census/train_B150.csv.bz2\n",
      "Loading: ../data/census/valid_B150.csv.bz2\n",
      "Loading: ../data/census/test_B150.csv.bz2\n",
      "Train/Valid/Test sizes: (1261180, 15) (137558, 15) (701292, 15)\n",
      "Train/Valid/Test split: 0.60 0.07 0.33\n",
      "CatFX: ['workclass', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'native_country']\n",
      "Train/Valid/Test sizes: (1260018, 15) (420006, 15) (420006, 15)\n",
      "Train/Valid/Test split: 0.60 0.20 0.20\n",
      "Saving processed files *.cat.bz2\n",
      "CatFX: ['workclass' 'marital_status' 'occupation' 'relationship' 'race' 'sex'\n",
      " 'native_country']\n",
      "[10]\ttrain's avg_binary_log_loss: 0.0237729\tvalid's avg_binary_log_loss: 0.0238513\n",
      "[20]\ttrain's avg_binary_log_loss: 0.0212787\tvalid's avg_binary_log_loss: 0.0213197\n",
      "[30]\ttrain's avg_binary_log_loss: 0.0208079\tvalid's avg_binary_log_loss: 0.0208054\n",
      "[40]\ttrain's avg_binary_log_loss: 0.0205758\tvalid's avg_binary_log_loss: 0.0205719\n",
      "[50]\ttrain's avg_binary_log_loss: 0.0204564\tvalid's avg_binary_log_loss: 0.0204507\n",
      "[60]\ttrain's avg_binary_log_loss: 0.0203491\tvalid's avg_binary_log_loss: 0.0203518\n",
      "[70]\ttrain's avg_binary_log_loss: 0.0202786\tvalid's avg_binary_log_loss: 0.0202781\n",
      "[80]\ttrain's avg_binary_log_loss: 0.0202223\tvalid's avg_binary_log_loss: 0.0202154\n",
      "[90]\ttrain's avg_binary_log_loss: 0.0201574\tvalid's avg_binary_log_loss: 0.0201505\n",
      "[100]\ttrain's avg_binary_log_loss: 0.0201157\tvalid's avg_binary_log_loss: 0.0201043\n",
      "[110]\ttrain's avg_binary_log_loss: 0.0200738\tvalid's avg_binary_log_loss: 0.020057\n",
      "[120]\ttrain's avg_binary_log_loss: 0.020045\tvalid's avg_binary_log_loss: 0.0200384\n",
      "[130]\ttrain's avg_binary_log_loss: 0.0200239\tvalid's avg_binary_log_loss: 0.0200187\n",
      "[140]\ttrain's avg_binary_log_loss: 0.0199922\tvalid's avg_binary_log_loss: 0.0199885\n",
      "[150]\ttrain's avg_binary_log_loss: 0.0199735\tvalid's avg_binary_log_loss: 0.0199738\n",
      "[160]\ttrain's avg_binary_log_loss: 0.0199611\tvalid's avg_binary_log_loss: 0.019961\n",
      "[170]\ttrain's avg_binary_log_loss: 0.0199459\tvalid's avg_binary_log_loss: 0.0199447\n",
      "[180]\ttrain's avg_binary_log_loss: 0.0199281\tvalid's avg_binary_log_loss: 0.0199274\n",
      "[190]\ttrain's avg_binary_log_loss: 0.0199047\tvalid's avg_binary_log_loss: 0.0199121\n",
      "[200]\ttrain's avg_binary_log_loss: 0.0181228\tvalid's avg_binary_log_loss: 0.0182145\n",
      "[210]\ttrain's avg_binary_log_loss: 0.0181111\tvalid's avg_binary_log_loss: 0.0182067\n",
      "[220]\ttrain's avg_binary_log_loss: 0.0181041\tvalid's avg_binary_log_loss: 0.0182024\n",
      "[230]\ttrain's avg_binary_log_loss: 0.0180988\tvalid's avg_binary_log_loss: 0.018199\n",
      "[240]\ttrain's avg_binary_log_loss: 0.0168249\tvalid's avg_binary_log_loss: 0.016953\n",
      "[250]\ttrain's avg_binary_log_loss: 0.0168157\tvalid's avg_binary_log_loss: 0.0169515\n",
      "[260]\ttrain's avg_binary_log_loss: 0.0168069\tvalid's avg_binary_log_loss: 0.0169394\n",
      "[270]\ttrain's avg_binary_log_loss: 0.0167985\tvalid's avg_binary_log_loss: 0.0169396\n",
      "[280]\ttrain's avg_binary_log_loss: 0.0167914\tvalid's avg_binary_log_loss: 0.0169402\n",
      "[290]\ttrain's avg_binary_log_loss: 0.0167855\tvalid's avg_binary_log_loss: 0.0169371\n",
      "[300]\ttrain's avg_binary_log_loss: 0.0165629\tvalid's avg_binary_log_loss: 0.0167202\n",
      "[310]\ttrain's avg_binary_log_loss: 0.0165586\tvalid's avg_binary_log_loss: 0.016723\n",
      "[320]\ttrain's avg_binary_log_loss: 0.0165556\tvalid's avg_binary_log_loss: 0.0167225\n",
      "[330]\ttrain's avg_binary_log_loss: 0.0165516\tvalid's avg_binary_log_loss: 0.0167239\n",
      "[340]\ttrain's avg_binary_log_loss: 0.0163893\tvalid's avg_binary_log_loss: 0.0165647\n",
      "[350]\ttrain's avg_binary_log_loss: 0.0163851\tvalid's avg_binary_log_loss: 0.0165634\n",
      "[360]\ttrain's avg_binary_log_loss: 0.0162105\tvalid's avg_binary_log_loss: 0.0163975\n",
      "[370]\ttrain's avg_binary_log_loss: 0.016207\tvalid's avg_binary_log_loss: 0.0163973\n",
      "[380]\ttrain's avg_binary_log_loss: 0.0162037\tvalid's avg_binary_log_loss: 0.016396\n",
      "[390]\ttrain's avg_binary_log_loss: 0.0160815\tvalid's avg_binary_log_loss: 0.0162713\n",
      "[400]\ttrain's avg_binary_log_loss: 0.0160713\tvalid's avg_binary_log_loss: 0.0162602\n",
      "[410]\ttrain's avg_binary_log_loss: 0.0160687\tvalid's avg_binary_log_loss: 0.0162609\n",
      "[420]\ttrain's avg_binary_log_loss: 0.0159632\tvalid's avg_binary_log_loss: 0.0161583\n",
      "[430]\ttrain's avg_binary_log_loss: 0.0159601\tvalid's avg_binary_log_loss: 0.0161621\n",
      "[440]\ttrain's avg_binary_log_loss: 0.0159575\tvalid's avg_binary_log_loss: 0.0161623\n",
      "[450]\ttrain's avg_binary_log_loss: 0.0159541\tvalid's avg_binary_log_loss: 0.016162\n",
      "[460]\ttrain's avg_binary_log_loss: 0.0159518\tvalid's avg_binary_log_loss: 0.0161626\n",
      "[470]\ttrain's avg_binary_log_loss: 0.0158598\tvalid's avg_binary_log_loss: 0.0160903\n",
      "[480]\ttrain's avg_binary_log_loss: 0.0158362\tvalid's avg_binary_log_loss: 0.0160568\n",
      "[490]\ttrain's avg_binary_log_loss: 0.0157521\tvalid's avg_binary_log_loss: 0.0159746\n",
      "[500]\ttrain's avg_binary_log_loss: 0.0157507\tvalid's avg_binary_log_loss: 0.0159721\n",
      "Model saved to ../out/models/AdvBoost_census_B150_T499_L16_S050.model\n",
      "   learning rate  num leaves  num trees  avg_binary_log_loss\n",
      "0            0.5        16.0      499.0             0.015972\n",
      "Pre-processing original files...\n",
      "Loading: ../data/census/train_B300.csv.bz2\n",
      "Loading: ../data/census/valid_B300.csv.bz2\n",
      "Loading: ../data/census/test_B300.csv.bz2\n",
      "Train/Valid/Test sizes: (2474827, 15) (269833, 15) (1376164, 15)\n",
      "Train/Valid/Test split: 0.60 0.07 0.33\n",
      "CatFX: ['workclass', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'native_country']\n",
      "Train/Valid/Test sizes: (2472494, 15) (824164, 15) (824166, 15)\n",
      "Train/Valid/Test split: 0.60 0.20 0.20\n",
      "Saving processed files *.cat.bz2\n",
      "CatFX: ['workclass' 'marital_status' 'occupation' 'relationship' 'race' 'sex'\n",
      " 'native_country']\n",
      "[10]\ttrain's avg_binary_log_loss: 0.0144292\tvalid's avg_binary_log_loss: 0.0144752\n",
      "[20]\ttrain's avg_binary_log_loss: 0.0123794\tvalid's avg_binary_log_loss: 0.0124193\n",
      "[30]\ttrain's avg_binary_log_loss: 0.0122201\tvalid's avg_binary_log_loss: 0.0122493\n",
      "[40]\ttrain's avg_binary_log_loss: 0.0121438\tvalid's avg_binary_log_loss: 0.0121739\n",
      "[50]\ttrain's avg_binary_log_loss: 0.0120699\tvalid's avg_binary_log_loss: 0.012098\n",
      "[60]\ttrain's avg_binary_log_loss: 0.0120179\tvalid's avg_binary_log_loss: 0.0120501\n",
      "[70]\ttrain's avg_binary_log_loss: 0.0119785\tvalid's avg_binary_log_loss: 0.0120111\n",
      "[80]\ttrain's avg_binary_log_loss: 0.0119484\tvalid's avg_binary_log_loss: 0.0119798\n",
      "[90]\ttrain's avg_binary_log_loss: 0.0119253\tvalid's avg_binary_log_loss: 0.0119543\n",
      "[100]\ttrain's avg_binary_log_loss: 0.0119073\tvalid's avg_binary_log_loss: 0.0119346\n",
      "[110]\ttrain's avg_binary_log_loss: 0.0118933\tvalid's avg_binary_log_loss: 0.0119199\n",
      "[120]\ttrain's avg_binary_log_loss: 0.0118825\tvalid's avg_binary_log_loss: 0.0119084\n",
      "[130]\ttrain's avg_binary_log_loss: 0.0118737\tvalid's avg_binary_log_loss: 0.0119004\n",
      "[140]\ttrain's avg_binary_log_loss: 0.0118663\tvalid's avg_binary_log_loss: 0.0118909\n",
      "[150]\ttrain's avg_binary_log_loss: 0.0118602\tvalid's avg_binary_log_loss: 0.0118838\n",
      "[160]\ttrain's avg_binary_log_loss: 0.0118552\tvalid's avg_binary_log_loss: 0.0118792\n",
      "[170]\ttrain's avg_binary_log_loss: 0.0118508\tvalid's avg_binary_log_loss: 0.0118739\n",
      "[180]\ttrain's avg_binary_log_loss: 0.0118471\tvalid's avg_binary_log_loss: 0.0118714\n",
      "[190]\ttrain's avg_binary_log_loss: 0.011844\tvalid's avg_binary_log_loss: 0.0118675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200]\ttrain's avg_binary_log_loss: 0.0118413\tvalid's avg_binary_log_loss: 0.0118646\n",
      "[210]\ttrain's avg_binary_log_loss: 0.011839\tvalid's avg_binary_log_loss: 0.0118617\n",
      "[220]\ttrain's avg_binary_log_loss: 0.011837\tvalid's avg_binary_log_loss: 0.0118598\n",
      "[230]\ttrain's avg_binary_log_loss: 0.0118353\tvalid's avg_binary_log_loss: 0.0118583\n",
      "[240]\ttrain's avg_binary_log_loss: 0.0118337\tvalid's avg_binary_log_loss: 0.0118566\n",
      "[250]\ttrain's avg_binary_log_loss: 0.0118322\tvalid's avg_binary_log_loss: 0.0118554\n",
      "[260]\ttrain's avg_binary_log_loss: 0.0118306\tvalid's avg_binary_log_loss: 0.011854\n",
      "[270]\ttrain's avg_binary_log_loss: 0.0118289\tvalid's avg_binary_log_loss: 0.0118524\n",
      "[280]\ttrain's avg_binary_log_loss: 0.0118272\tvalid's avg_binary_log_loss: 0.0118507\n",
      "[290]\ttrain's avg_binary_log_loss: 0.0118255\tvalid's avg_binary_log_loss: 0.0118491\n",
      "[300]\ttrain's avg_binary_log_loss: 0.0118239\tvalid's avg_binary_log_loss: 0.0118474\n",
      "[310]\ttrain's avg_binary_log_loss: 0.0118222\tvalid's avg_binary_log_loss: 0.0118458\n",
      "[320]\ttrain's avg_binary_log_loss: 0.0118205\tvalid's avg_binary_log_loss: 0.0118442\n",
      "[330]\ttrain's avg_binary_log_loss: 0.0118188\tvalid's avg_binary_log_loss: 0.0118425\n",
      "[340]\ttrain's avg_binary_log_loss: 0.0118172\tvalid's avg_binary_log_loss: 0.0118409\n",
      "[350]\ttrain's avg_binary_log_loss: 0.0118155\tvalid's avg_binary_log_loss: 0.0118393\n",
      "[360]\ttrain's avg_binary_log_loss: 0.0118138\tvalid's avg_binary_log_loss: 0.0118376\n",
      "[370]\ttrain's avg_binary_log_loss: 0.0118122\tvalid's avg_binary_log_loss: 0.011836\n",
      "[380]\ttrain's avg_binary_log_loss: 0.0118105\tvalid's avg_binary_log_loss: 0.0118344\n",
      "[390]\ttrain's avg_binary_log_loss: 0.0118088\tvalid's avg_binary_log_loss: 0.0118328\n",
      "[400]\ttrain's avg_binary_log_loss: 0.0118072\tvalid's avg_binary_log_loss: 0.0118312\n",
      "[410]\ttrain's avg_binary_log_loss: 0.0118055\tvalid's avg_binary_log_loss: 0.0118295\n",
      "[420]\ttrain's avg_binary_log_loss: 0.0118039\tvalid's avg_binary_log_loss: 0.0118279\n",
      "[430]\ttrain's avg_binary_log_loss: 0.0118022\tvalid's avg_binary_log_loss: 0.0118263\n",
      "[440]\ttrain's avg_binary_log_loss: 0.0118006\tvalid's avg_binary_log_loss: 0.0118247\n",
      "[450]\ttrain's avg_binary_log_loss: 0.0117989\tvalid's avg_binary_log_loss: 0.0118231\n",
      "[460]\ttrain's avg_binary_log_loss: 0.0117973\tvalid's avg_binary_log_loss: 0.0118215\n",
      "[470]\ttrain's avg_binary_log_loss: 0.0117956\tvalid's avg_binary_log_loss: 0.0118199\n",
      "[480]\ttrain's avg_binary_log_loss: 0.011794\tvalid's avg_binary_log_loss: 0.0118183\n",
      "[490]\ttrain's avg_binary_log_loss: 0.0117924\tvalid's avg_binary_log_loss: 0.0118167\n",
      "[500]\ttrain's avg_binary_log_loss: 0.0117907\tvalid's avg_binary_log_loss: 0.0118151\n",
      "Model saved to ../out/models/AdvBoost_census_B300_T500_L16_S050.model\n",
      "   learning rate  num leaves  num trees  avg_binary_log_loss\n",
      "0            0.5        16.0      500.0             0.011815\n"
     ]
    }
   ],
   "source": [
    "for B in [5, 15, 150, 300]:\n",
    "\n",
    "    experiments = train_adversarial_boosting ( \"../data/census/train_B{:d}.csv.bz2\".format(B),\n",
    "                                               \"../data/census/valid_B{:d}.csv.bz2\".format(B),\n",
    "                                               \"../data/census/test_B{:d}.csv.bz2\".format(B),\n",
    "                                               \"../out/models/AdvBoost_census_B{:d}\".format(B))  \n",
    "\n",
    "    experiments.to_csv('AdvBoosting_Census_B{:d}.csv'.format(B))\n",
    "\n",
    "    print (experiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 2.3M\r\n",
      "-rw-rw-r-- 1 lucchese lucchese 155K Feb 19 11:39 AdvBoost_census_B300_T500_L16_S050.model\r\n",
      "-rw-rw-r-- 1 lucchese lucchese 212K Feb 19 11:36 AdvBoost_census_B150_T499_L16_S050.model\r\n",
      "-rw-rw-r-- 1 lucchese lucchese 407K Feb 19 11:34 AdvBoost_census_B15_T500_L16_S050.model\r\n",
      "-rw-rw-r-- 1 lucchese lucchese 490K Feb 19 11:33 AdvBoost_census_B5_T365_L16_S050.model\r\n",
      "-rw-rw-r-- 1 lucchese lucchese  60K Feb 19 11:24 lgbm_census_T22_L32_S050.model\r\n",
      "-rw-rw-r-- 1 lucchese lucchese 115K Feb 19 11:24 lgbm_census_T81_L16_S050.model\r\n",
      "-rw-rw-r-- 1 lucchese lucchese 535K Feb 19 11:24 lgbm_census_T197_L32_S010.model\r\n",
      "-rw-rw-r-- 1 lucchese lucchese 278K Feb 19 11:24 lgbm_census_T196_L16_S010.model\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lht ../out/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ls -lht ../data/census/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ls -lht ../out/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug running time\n",
    "#%load_ext line_profiler\n",
    "# %lprun -f AdvBoosting  AdvBoosting(train, trees=3)\n",
    "# %lprun -f AdvBoosting_gen_data  AdvBoosting(train, trees=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm ../out/models/*.lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
