{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def infer_categorical_features(dataset):\n",
    "#     categorical_features = set([])\n",
    "#     for column in dataset.columns:\n",
    "#         if dataset[column].dtype == 'object':\n",
    "#             categorical_features.add(column)\n",
    "#     return categorical_features\n",
    "            \n",
    "def label_encode(dataset, categorical_features):\n",
    "    dataset_le = dataset.copy()\n",
    "    for column in dataset_le.columns:\n",
    "        if column in categorical_features:\n",
    "            dataset_le[column] = dataset_le[column].astype('category')\n",
    "            dataset_le[column] = dataset_le[column].cat.codes.astype(np.int32)\n",
    "    return dataset_le\n",
    "\n",
    "def load_atk_train_valid_test( atk_train_file, atk_valid_file, atk_test_file, \n",
    "                               train_split=0.6, valid_split=0.2,\n",
    "                               force=False):\n",
    "    \n",
    "    \n",
    "    if  ( force or \n",
    "          not os.path.exists(atk_train_file+\".cat.bz2\") or\n",
    "          not os.path.exists(atk_valid_file+\".cat.bz2\") or\n",
    "          not os.path.exists(atk_test_file+\".cat.bz2\") or \n",
    "          not os.path.exists(atk_train_file+\".cat.json\") ):\n",
    "    \n",
    "        print (\"Pre-processing original files...\")\n",
    "\n",
    "        print (\"Loading:\", atk_train_file)\n",
    "        print (\"Loading:\", atk_valid_file)\n",
    "        print (\"Loading:\", atk_test_file)\n",
    "\n",
    "        train = pd.read_csv(atk_train_file)\n",
    "        valid = pd.read_csv(atk_valid_file)\n",
    "        test  = pd.read_csv(atk_test_file)\n",
    "        \n",
    "        print (\"Train/Valid/Test sizes:\", train.shape, valid.shape, test.shape)\n",
    "        print (\"Train/Valid/Test split: {:.2f} {:.2f} {:.2f}\"\n",
    "                   .format( train.shape[0]/(train.shape[0]+valid.shape[0]+test.shape[0]),\n",
    "                            valid.shape[0]/(train.shape[0]+valid.shape[0]+test.shape[0]),\n",
    "                            test.shape[0] /(train.shape[0]+valid.shape[0]+test.shape[0]) ) )\n",
    "\n",
    "        # concat to process correctly label encoding\n",
    "        full = pd.concat( [train, valid, test] )\n",
    "\n",
    "        # get index of categorical features (-1 because of instance_id)\n",
    "        cat_fx = full.columns.values[ np.where(full.dtypes=='object')[0] ]\n",
    "        cat_fx = list(cat_fx)    \n",
    "        full = label_encode(full, cat_fx)\n",
    "        with open(atk_train_file+\".cat.json\", 'w') as fp:\n",
    "            json.dump(cat_fx, fp)\n",
    "        print (\"CatFX:\", cat_fx)\n",
    "\n",
    "        # split-back into train valid test\n",
    "        train_size = int( full.shape[0]*train_split )\n",
    "        valid_size = int( full.shape[0]*valid_split )\n",
    "        train_cat = full.iloc[0:train_size,:]\n",
    "        valid_cat = full.iloc[train_size:train_size+valid_size,:]\n",
    "        test_cat  = full.iloc[train_size+valid_size:,:]    \n",
    "\n",
    "        print (\"Train/Valid/Test sizes:\", train_cat.shape, valid_cat.shape, test_cat.shape)\n",
    "        print (\"Train/Valid/Test split: {:.2f} {:.2f} {:.2f}\"\n",
    "                   .format( train_cat.shape[0]/(train_cat.shape[0]+valid_cat.shape[0]+test_cat.shape[0]),\n",
    "                            valid_cat.shape[0]/(train_cat.shape[0]+valid_cat.shape[0]+test_cat.shape[0]),\n",
    "                            test_cat.shape[0] /(train_cat.shape[0]+valid_cat.shape[0]+test_cat.shape[0]) ) )\n",
    "\n",
    "        # save to file\n",
    "        print (\"Saving processed files *.cat.bz2\")\n",
    "        train_cat.to_csv(atk_train_file+\".cat.bz2\", compression=\"bz2\", index=False)\n",
    "        valid_cat.to_csv(atk_valid_file+\".cat.bz2\", compression=\"bz2\", index=False)\n",
    "        test_cat.to_csv (atk_test_file+\".cat.bz2\",  compression=\"bz2\", index=False)\n",
    "        \n",
    "    else:\n",
    "        print (\"Loading pre-processed files...\")\n",
    "\n",
    "        train_cat = pd.read_csv(atk_train_file+\".cat.bz2\")\n",
    "        valid_cat = pd.read_csv(atk_valid_file+\".cat.bz2\")\n",
    "        test_cat  = pd.read_csv(atk_test_file+\".cat.bz2\")\n",
    "        \n",
    "        with open(atk_train_file+\".cat.json\", 'r') as fp:\n",
    "            cat_fx = json.load(fp)\n",
    "    \n",
    "    # return data\n",
    "    return train_cat, valid_cat, test_cat, cat_fx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adversarial Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AdvBoosting_gen_data(model, data, groups):\n",
    "    ''' \n",
    "    model  : is the LightGBM Model\n",
    "    data   : data matrix with all valid attacks (last column is label)\n",
    "    groups : grouping of same attacked instance \n",
    "    returns the new data matrix and new groups\n",
    "    \n",
    "    WARNING: currently works only for binary classification\n",
    "    '''\n",
    "    # score the datataset\n",
    "    labels = data[:,-1]\n",
    "    \n",
    "    predictions = model.predict(data[:,:-1]) # exclude labels\n",
    "    # binarize\n",
    "    predictions = (predictions>0).astype(np.float)\n",
    "    predictions = 2*predictions - 1\n",
    "    \n",
    "    # check mispredictions\n",
    "    matchings = labels * predictions\n",
    "    \n",
    "    # select original data + attacked instances\n",
    "    new_selected = [] # id of selected instances\n",
    "    new_groups   = []\n",
    "    \n",
    "    offset = 0\n",
    "    for g in groups:\n",
    "        if g==0:\n",
    "            print (\"Error !!!!\")\n",
    "        elif g==1:\n",
    "            # there are no attacks, just add original\n",
    "            new_selected += [offset]\n",
    "            new_groups   += [1]\n",
    "        else:\n",
    "            # get a slice of the matching scores\n",
    "            g_matchings = matchings[offset:offset+g]\n",
    "\n",
    "            # most misclassified (smallest margin)\n",
    "            # skip original\n",
    "            adv_instance = np.argmin(g_matchings[1:])+1\n",
    "\n",
    "            # add original and adversarial\n",
    "            new_selected += [offset, adv_instance]\n",
    "            new_groups   += [2]\n",
    "        \n",
    "        offset += g\n",
    "    \n",
    "    new_dataset = data[new_selected,:]\n",
    "    \n",
    "    return new_dataset, new_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our custom metrics\n",
    "def binary_log_loss(pred, true_label):\n",
    "\n",
    "    return np.log(1.0 + np.exp(-pred * true_label))\n",
    "\n",
    "# self-defined eval metric\n",
    "# f(preds: array, train_data: Dataset) -> name: string, value: array, is_higher_better: bool\n",
    "def avg_log_loss(preds, train_data):\n",
    "    \n",
    "    labels = train_data.get_label()\n",
    "    losses = np.log(1.0 + np.exp(-preds*labels))\n",
    "    avg_loss = np.mean(losses)\n",
    "    \n",
    "    return 'avg_binary_log_loss', avg_loss, False\n",
    "\n",
    "\n",
    "def optimize_log_loss(preds, train_data):\n",
    "    labels = train_data.get_label()\n",
    "    exp_pl = np.exp(preds * labels)\n",
    "    # http://www.wolframalpha.com/input/?i=differentiate+log(1+%2B+exp(-kx)+)\n",
    "    grads = -labels / (1.0 +  exp_pl)  \n",
    "    # http://www.wolframalpha.com/input/?i=d%5E2%2Fdx%5E2+log(1+%2B+exp(-kx)+)\n",
    "    hess = labels**2 * exp_pl / (1.0 + exp_pl)**2 \n",
    "\n",
    "    # this is to optimize average logloss\n",
    "    norm = 1.0/len(preds)\n",
    "    grads *= norm\n",
    "    hess *= norm\n",
    "    \n",
    "    return grads, hess\n",
    "\n",
    "\n",
    "# self-defined eval metric\n",
    "# f(preds: array, train_data: Dataset) -> name: string, value: array, is_higher_better: bool\n",
    "def avg_log_loss_uma(preds, train_data):\n",
    "    labels = train_data.get_label()\n",
    "    attack_lens = train_data.get_group()\n",
    "    \n",
    "    offset = 0\n",
    "    max_logloss = []\n",
    "    avg_max_logloss = 0.0\n",
    "    \n",
    "    if attack_lens is not None:\n",
    "    \n",
    "        for atk in attack_lens:\n",
    "            losses = [binary_log_loss(h,t) for h,t in zip(preds[offset:offset+atk], labels[offset:offset+atk])]\n",
    "            max_logloss.append(max(losses))\n",
    "\n",
    "            offset += atk\n",
    "        \n",
    "        avg_max_logloss = np.mean(max_logloss)  \n",
    "\n",
    "    return 'avg_binary_log_loss_under_max_attack', avg_max_logloss, False\n",
    "\n",
    "def avg_non_interferent_log_loss(preds, train_data, alpha=1.0):\n",
    "    \n",
    "    # binary logloss under maximal attack\n",
    "    _, loss_uma, _    = avg_log_loss_uma(preds, train_data)\n",
    "    \n",
    "    # binary logloss (plain)\n",
    "    _, loss_plain, _  = avg_log_loss(preds, train_data)\n",
    "    \n",
    "    # combine the above two losses together\n",
    "    weighted_loss = alpha*loss_uma + (1.0-alpha)*loss_plain\n",
    "\n",
    "    return 'avg_non_interferent_log_loss [alpha={}]'.format(alpha), weighted_loss, False\n",
    "\n",
    "def optimize_log_loss_uma(preds, train_data):\n",
    "    labels = train_data.get_label()\n",
    "    attack_lens = train_data.get_group()\n",
    "    \n",
    "    grads = np.zeros_like(labels, dtype=np.float64)\n",
    "    hess = np.zeros_like(grads)\n",
    "    \n",
    "    if attack_lens is not None:\n",
    "\n",
    "        norm = 1.0 / float(len(attack_lens))\n",
    "\n",
    "        offset = 0\n",
    "        for atk in attack_lens:\n",
    "            exp_pl = np.exp(- preds[offset:offset+atk] * labels[offset:offset+atk])\n",
    "\n",
    "            inv_sum = 1.0 / np.sum(1.0 + exp_pl)\n",
    "\n",
    "            x_grad = inv_sum * exp_pl\n",
    "\n",
    "            grads[offset:offset+atk] = norm * x_grad * (- labels[offset:offset+atk])\n",
    "            hess[offset:offset+atk]  = norm * x_grad * (1.0 - x_grad)\n",
    "\n",
    "            offset += atk    \n",
    "    \n",
    "    return grads, hess\n",
    "\n",
    "def optimize_non_interferent_log_loss(preds, train_data, alpha=1.0):\n",
    "    # binary logloss under maximal attack\n",
    "    grads_uma, hess_uma = optimize_log_loss_uma(preds, train_data)\n",
    "    \n",
    "    # binary logloss (plain)\n",
    "    grads_plain, hess_plain = optimize_log_loss(preds, train_data)\n",
    "    \n",
    "    # combine the above two losses together\n",
    "    grads = alpha*grads_uma + (1.0-alpha)*grads_plain\n",
    "    hess  = alpha*hess_uma  + (1.0-alpha)*hess_plain\n",
    "    \n",
    "    return grads, hess\n",
    "\n",
    "\n",
    "\n",
    "def AdvBoosting_extend_model(data, cat_fx, input_model=None, num_trees=1, params=None):\n",
    "    ''' \n",
    "    model  : is the LightGBM Model\n",
    "    data   : data matrix with all valid attacks (last column is label)\n",
    "    returns the new model (is model modified inplace?)\n",
    "    '''\n",
    "    \n",
    "    if cat_fx is None or len(cat_fx)==0:\n",
    "        cat_fx = \"auto\"\n",
    "\n",
    "    lgbm_train = lightgbm.Dataset(data=data[:,:-1], \n",
    "                                  label=data[:,-1],\n",
    "                                  categorical_feature = cat_fx)\n",
    "    \n",
    "    lgbm_info = {}\n",
    "    lgbm_model = lightgbm.train(params, lgbm_train, \n",
    "                                num_boost_round = num_trees, \n",
    "                                init_model = input_model,\n",
    "                                fobj = optimize_log_loss, \n",
    "                                feval = avg_log_loss,\n",
    "                                evals_result = lgbm_info,\n",
    "                                valid_sets   = [lgbm_train], \n",
    "                                valid_names  = ['adv-train'],\n",
    "                                verbose_eval=10)\n",
    "\n",
    "    return lgbm_model, lgbm_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AdvBoosting( atk_train, trees, \n",
    "                 cat_fx,\n",
    "                 params,\n",
    "                 output_model_file,\n",
    "                 partial_save=10, \n",
    "                 adv_rounds=1):\n",
    "    ''' \n",
    "    atk_data: full dataset including all valid attacks\n",
    "    atk_groups: lenght of each attack set\n",
    "    trees: total number of trees to be produced\n",
    "    adv_rounds: adversarial instance injecting frequency\n",
    "    '''\n",
    "    # temp lgbm file\n",
    "    temp = output_model_file+\".tmp\"\n",
    "    \n",
    "    # get groups and remove instance ids\n",
    "    atk_groups = atk_train['instance_id'].value_counts().sort_index().values\n",
    "    atk_train.drop('instance_id', axis=1, inplace=True)\n",
    "    \n",
    "    # get index of categorical features \n",
    "    cat_fx = np.where(atk_train.columns.isin(cat_fx) )[0]\n",
    "    cat_fx = list([int(x) for x in cat_fx])  \n",
    "    # print (\"CatFX:\", atk_train.columns.values[cat_fx])\n",
    "\n",
    "    # prepare data (avoiding pandas)\n",
    "    atk_data   = atk_train.values\n",
    "\n",
    "    # train first trees\n",
    "    original_ids = np.cumsum(atk_groups[:-1])\n",
    "    original_ids = np.insert(original_ids, 0, 0)\n",
    "    \n",
    "    model, model_info = AdvBoosting_extend_model( atk_data[original_ids, :], \n",
    "                                                  cat_fx=cat_fx,\n",
    "                                                  input_model=None, \n",
    "                                                  num_trees=adv_rounds, \n",
    "                                                  params=params )\n",
    "    \n",
    "    \n",
    "    # train remaining trees\n",
    "    for t in range(adv_rounds+1, trees+1, adv_rounds):\n",
    "        # attack dataset\n",
    "        adv_data, adv_offsets = AdvBoosting_gen_data( model, atk_data, atk_groups )\n",
    "        \n",
    "        # train additional trees\n",
    "        model.save_model(temp)\n",
    "        model, model_info = AdvBoosting_extend_model( adv_data, \n",
    "                                                      cat_fx=cat_fx,\n",
    "                                                      input_model=temp, \n",
    "                                                      num_trees=adv_rounds, \n",
    "                                                      params=params)\n",
    "        # save partial model\n",
    "        if t%partial_save==0 and t!=trees:\n",
    "            partial_filename = \"{}.T{:03d}.lgbm\".format(output_model_file, t)\n",
    "            model.save_model( filename=partial_filename )\n",
    "            \n",
    "    model.save_model(filename=output_model_file)\n",
    "    \n",
    "    return model, model_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scripting wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training( atk_train_file, atk_valid_file, atk_test_file,\n",
    "                  output_model_file,\n",
    "                  num_trees,\n",
    "                  learning_rate,\n",
    "                  num_leaves,\n",
    "                  partial_save,\n",
    "                  adversarial_rounds):\n",
    "\n",
    "    # load train/valid/test\n",
    "    train, valid, test, cat_fx = load_atk_train_valid_test(atk_train_file, atk_valid_file, atk_test_file)\n",
    "    \n",
    "    # train params\n",
    "    lgbm_params = { 'learning_rate': float(learning_rate), \n",
    "                    'num_leaves': int(num_leaves)} \n",
    "\n",
    "    # start training\n",
    "    return AdvBoosting(  train, \n",
    "                         trees=num_trees, \n",
    "                         cat_fx = cat_fx,\n",
    "                         output_model_file=output_model_file, \n",
    "                         adv_rounds=adversarial_rounds, \n",
    "                         partial_save=partial_save, \n",
    "                         params=lgbm_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-processed files...\n",
      "CatFX: ['workclass' 'marital_status' 'occupation' 'relationship' 'race' 'sex'\n",
      " 'native_country']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucchese/.local/lib/python3.6/site-packages/lightgbm/basic.py:1036: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "/home/lucchese/.local/lib/python3.6/site-packages/lightgbm/basic.py:681: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\ttrain's avg_binary_log_loss: 0.442694\tvalid's avg_binary_log_loss: 0.443485\n",
      "[20]\ttrain's avg_binary_log_loss: 0.383888\tvalid's avg_binary_log_loss: 0.384591\n",
      "[30]\ttrain's avg_binary_log_loss: 0.364054\tvalid's avg_binary_log_loss: 0.365226\n",
      "[40]\ttrain's avg_binary_log_loss: 0.354723\tvalid's avg_binary_log_loss: 0.35629\n",
      "[50]\ttrain's avg_binary_log_loss: 0.332285\tvalid's avg_binary_log_loss: 0.336143\n",
      "{'train': defaultdict(<class 'list'>, {'avg_binary_log_loss': [0.6442831012057314, 0.6043478691937245, 0.5712039382536149, 0.5429154323835782, 0.5194995815960616, 0.49926751235772854, 0.4818859720707152, 0.4670586329204811, 0.45398464711676156, 0.44269372914143423, 0.4326994349622958, 0.4242787202485628, 0.4165668471107094, 0.40984903164484426, 0.40423865203413867, 0.3990045596889301, 0.39457656403367825, 0.39066311150609556, 0.3869512093246306, 0.38388825406014965, 0.38107445998894607, 0.37843677935834447, 0.3762391990774234, 0.3738217495960891, 0.37205918526098186, 0.3704253868418409, 0.36852629087430694, 0.36722138451870956, 0.3656307836492787, 0.36405409641826575, 0.3630698929748407, 0.36215678469657736, 0.3609331091118873, 0.3601127694126037, 0.35902265550374574, 0.35803226322701986, 0.35715887419957604, 0.35634814304513746, 0.35544907064617226, 0.35472308994273255, 0.34927747751890464, 0.3486645204168614, 0.3479909256933628, 0.3474293438762867, 0.34683136355157, 0.3463007643485171, 0.3457492209503944, 0.3383542204301219, 0.3379785179882868, 0.3322853159842678]}), 'valid': defaultdict(<class 'list'>, {'avg_binary_log_loss': [0.6445198073535529, 0.6047581776325023, 0.5716938484385594, 0.5437304395542207, 0.5202685408076835, 0.5001201020091954, 0.4827288549871254, 0.46777654477883335, 0.4546920824433188, 0.4434846553086925, 0.43316809788960414, 0.4247565272334261, 0.41693705242303747, 0.41021383573698456, 0.4045646205259857, 0.3993641554520049, 0.39506332357425966, 0.3912510683556621, 0.3875413350297143, 0.3845908590073142, 0.3817911768319243, 0.3790833799922164, 0.37693595885908, 0.3745982088388579, 0.37291582283658437, 0.3712987432065432, 0.3695010885902893, 0.36832971473389386, 0.3668567866512054, 0.36522620744104856, 0.36435487498374763, 0.3635169532747717, 0.36226585389112775, 0.3615144501096788, 0.3604313881809392, 0.3594929934323055, 0.35859601999664537, 0.3578838819468726, 0.3569593141653201, 0.35628957724957766, 0.3515319553846273, 0.3510372229826349, 0.3504565366636007, 0.3500488513585168, 0.3495227362075274, 0.34906691117258887, 0.3485971612252921, 0.34180347776673664, 0.34154256950973155, 0.3361433462780212]})}\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "def train_gradient_boosting_baseline( train_file, valid_file, test_file,\n",
    "                                      output_model_file ):\n",
    "    \n",
    "    # load train/valid/test\n",
    "    train, valid, test, cat_fx = load_atk_train_valid_test(train_file, valid_file, test_file)\n",
    "\n",
    "    # get index of categorical features \n",
    "    cat_fx = np.where(train.columns.isin(cat_fx) )[0]\n",
    "    cat_fx = list([int(x) for x in cat_fx])  \n",
    "    print (\"CatFX:\", train.columns.values[cat_fx])\n",
    "    \n",
    "    # datasets\n",
    "    lgbm_train = lightgbm.Dataset(data=train.values[:,:-1], \n",
    "                                  label=train.values[:,-1],\n",
    "                                  categorical_feature = cat_fx)\n",
    "    \n",
    "    lgbm_valid = lightgbm.Dataset(data=valid.values[:,:-1], \n",
    "                                  label=valid.values[:,-1],\n",
    "                                  categorical_feature = cat_fx)\n",
    "\n",
    "    # train params\n",
    "    lgbm_params = { 'learning_rate': 0.1, \n",
    "                    'num_leaves': 16} \n",
    "    lgbm_info = {}\n",
    "    lgbm_model = lightgbm.train(lgbm_params, lgbm_train, \n",
    "                                num_boost_round = 50,\n",
    "                                fobj            = optimize_log_loss, \n",
    "                                feval           = avg_log_loss,\n",
    "                                evals_result    = lgbm_info,\n",
    "                                valid_sets      = [lgbm_train, lgbm_valid], \n",
    "                                valid_names     = ['train', 'valid'],\n",
    "                                verbose_eval    = 10)\n",
    "\n",
    "    print (lgbm_info)\n",
    "    print (len ( lgbm_info['valid']['avg_binary_log_loss']) )\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "train_gradient_boosting_baseline ( \"../data/census/train_ori.csv.bz2\",\n",
    "                                   \"../data/census/valid_ori.csv.bz2\",\n",
    "                                   \"../data/census/test_ori.csv.bz2\",\n",
    "                                   \"../out/models/GBDT_census.lgbm\" )  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adversatial Boosing Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-processed files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lucchese/.local/lib/python3.6/site-packages/lightgbm/basic.py:1036: UserWarning: Using categorical_feature in Dataset.\n",
      "  warnings.warn('Using categorical_feature in Dataset.')\n",
      "/home/lucchese/.local/lib/python3.6/site-packages/lightgbm/basic.py:681: UserWarning: categorical_feature in param dict is overridden.\n",
      "  warnings.warn('categorical_feature in param dict is overridden.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\tadv-train's avg_binary_log_loss: 0.3392\n"
     ]
    }
   ],
   "source": [
    "run_training( \"../data/census/train_B5.csv.bz2\", \n",
    "              \"../data/census/valid_B5.csv.bz2\",\n",
    "              \"../data/census/test_B5.csv.bz2\",\n",
    "              \"../out/models/adv_boosting.lgbm\",\n",
    "              num_trees=10,\n",
    "              num_leaves=16,\n",
    "              learning_rate=0.1,\n",
    "              partial_save=10,\n",
    "              adversarial_rounds=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 20M\r\n",
      "-rw-rw-r-- 1 lucchese lucchese 305K Feb 18 18:46 test_B5.csv.bz2.cat.bz2\r\n",
      "-rw-rw-r-- 1 lucchese lucchese 307K Feb 18 18:46 valid_B5.csv.bz2.cat.bz2\r\n",
      "-rw-rw-r-- 1 lucchese lucchese 912K Feb 18 18:46 train_B5.csv.bz2.cat.bz2\r\n",
      "-rw-rw-r-- 1 lucchese lucchese   94 Feb 18 18:46 train_B5.csv.bz2.cat.json\r\n",
      "-rw-rw-r-- 1 lucchese lucchese 503K Feb 18 18:41 test_B5.csv.bz2\r\n",
      "-rw-rw-r-- 1 lucchese lucchese 100K Feb 18 18:41 valid_B5.csv.bz2\r\n",
      "-rw-rw-r-- 1 lucchese lucchese 902K Feb 18 18:41 train_B5.csv.bz2\r\n",
      "-rw-rw-r-- 1 lucchese lucchese 103K Feb 18 18:13 test_ori.csv.bz2\r\n",
      "-rw-rw-r-- 1 lucchese lucchese  23K Feb 18 18:13 valid_ori.csv.bz2\r\n",
      "-rw-rw-r-- 1 lucchese lucchese 183K Feb 18 18:13 train_ori.csv.bz2\r\n",
      "-rw-rw-r-- 1 lucchese lucchese 902K Feb 18 14:11 test_B15.csv.bz2\r\n",
      "-rw-rw-r-- 1 lucchese lucchese 179K Feb 18 14:11 valid_B15.csv.bz2\r\n",
      "-rw-rw-r-- 1 lucchese lucchese 1.6M Feb 18 14:11 train_B15.csv.bz2\r\n",
      "-rw-rw-r-- 1 lucchese lucchese 1.1M Feb 18 09:52 test_strong_att.csv.bz2.cat.bz2\r\n",
      "-rw-rw-r-- 1 lucchese lucchese 1.2M Feb 18 09:52 valid_strong_att.csv.bz2.cat.bz2\r\n",
      "-rw-rw-r-- 1 lucchese lucchese 3.4M Feb 18 09:51 train_strong_att.csv.bz2.cat.bz2\r\n",
      "-rw-rw-r-- 1 lucchese lucchese  15K Feb 16 09:29 adv_boosting.lgbm\r\n",
      "-rw-rw-r-- 1 lucchese lucchese 376K Feb 12 08:45 valid_strong_att.csv.bz2\r\n",
      "-rw-rw-r-- 1 lucchese lucchese  53K Feb 12 08:45 valid_weak_att.csv.bz2\r\n",
      "-rw-rw-r-- 1 lucchese lucchese  55K Feb 12 08:45 valid_weak_att_2.csv.bz2\r\n",
      "-rw-rw-r-- 1 lucchese lucchese  55K Feb 12 08:45 valid_weak_att_3.csv.bz2\r\n",
      "-rw-rw-r-- 1 lucchese lucchese 487K Feb 12 08:45 train_weak_att_3.csv.bz2\r\n",
      "-rw-rw-r-- 1 lucchese lucchese 485K Feb 12 08:45 train_weak_att_2.csv.bz2\r\n",
      "-rw-rw-r-- 1 lucchese lucchese 469K Feb 12 08:45 train_weak_att.csv.bz2\r\n",
      "-rw-rw-r-- 1 lucchese lucchese 3.4M Feb 12 08:45 train_strong_att.csv.bz2\r\n",
      "-rw-rw-r-- 1 lucchese lucchese 227K Feb 12 08:45 train.csv.bz2\r\n",
      "-rw-rw-r-- 1 lucchese lucchese 271K Feb 12 08:45 test_weak_att_2.csv.bz2\r\n",
      "-rw-rw-r-- 1 lucchese lucchese 273K Feb 12 08:45 test_weak_att_3.csv.bz2\r\n",
      "-rw-rw-r-- 1 lucchese lucchese 1.9M Feb 12 08:45 test_strong_att.csv.bz2\r\n",
      "-rw-rw-r-- 1 lucchese lucchese 262K Feb 12 08:45 test_weak_att.csv.bz2\r\n",
      "-rw-rw-r-- 1 lucchese lucchese 1.8K Feb 12 08:45 attacks.txt\r\n",
      "-rw-rw-r-- 1 lucchese lucchese 114K Feb 12 08:45 test.csv.bz2\r\n"
     ]
    }
   ],
   "source": [
    "!ls -lht ../data/census/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pre-processed files...\n",
      "CatFX: ['workclass' 'marital_status' 'occupation' 'relationship' 'race' 'sex'\n",
      " 'native_country']\n",
      "[10]\tadv-train's avg_binary_log_loss: 0.442759\n"
     ]
    }
   ],
   "source": [
    "# run_training( \"../data/census/train_B5.csv.bz2\", \n",
    "#               \"../data/census/valid_B5.csv.bz2\",\n",
    "#               \"../data/census/test_B5.csv.bz2\",\n",
    "#               \"../out/models/debug.lgbm\",\n",
    "#               num_trees=10,\n",
    "#               learning_rate=0.1,\n",
    "#               partial_save=1000,\n",
    "#               adversarial_rounds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ls -lht ../out/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug running time\n",
    "#%load_ext line_profiler\n",
    "# %lprun -f AdvBoosting  AdvBoosting(train, trees=3)\n",
    "# %lprun -f AdvBoosting_gen_data  AdvBoosting(train, trees=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm ../out/models/*.lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
