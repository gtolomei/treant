{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Formulation using LOG-LOSS\n",
    "\n",
    " For a single sample with *true label* $y_t \\in \\{0,1\\}$ and the *estimated probability* $y_p$ that $y_t = 1$, the *log loss* is:\n",
    "\n",
    "$$- (y_t \\ln(y_p) + (1 - y_t) \\ln(1 - y_p))$$\n",
    "\n",
    " - If $y_t=1$: we sum to the loss $-\\ln(y_p)$ (a very large penalty if $y_p$ is very wrong).\n",
    " - If $y_t=0$: we sum to the loss $-\\ln(1-y_p)$ (a very large penalty if $y_p$ is very wrong).\n",
    "\n",
    "\n",
    "*Log loss* is undefined for $y_p=0$ or $y_p=1$, since $ln(0) = -\\infty$, so in the formula the probability $y_p$ must be clipped to $[eps, 1-eps]$ for $eps$ very small like $eps=1e-15$. Equivalently,   $y_p = \\max(eps, \\min(1 - eps, y_p))$, .\n",
    "\n",
    "If we have to split a node associated with dataset $D$, we have:\n",
    "\n",
    "$$\n",
    "L (t_{\\lambda\\rightarrow\\sigma},{\\cal D}) = \n",
    "-\\frac{1}{|L|}\\sum_{(\\vec{x},y) \\in L} (y \\ln(y_l) + (1 - y) \\ln(1 - y_l)) \n",
    "-\\frac{1}{|R|}\\sum_{(\\vec{x},y) \\in L} (y \\ln(y_r) + (1 - y) \\ln(1 - y_r))\n",
    "$$\n",
    "\n",
    "where $D = L \\cup R$.\n",
    "\n",
    "Let $L^1$ and $L^0$ be the two disjoint subsets of $L$ such that $L= L^0 \\cup L^1$, and $\\forall (\\vec{x},y) \\in L^1,\\ y=1$ and $\\forall (\\vec{x},y) \\in L^0,\\ y=0$.\n",
    "The same holds for $R = R^0 \\cup R^1$, the two disjoint subsets of $R$.\n",
    "\n",
    "The log-loss function becomes:\n",
    "\n",
    "$$\n",
    "\\begin{array}{lcl}\n",
    "L (t_{\\lambda\\rightarrow\\sigma},{\\cal D}) & = &\n",
    "-\\frac{1}{|L^0 \\cup L^1|}(\\sum_{(\\vec{x},1) \\in L^1}  \\ln(y_l)  \n",
    "+\\sum_{(\\vec{x},0) \\in L^0} \\ln(1 - y_l)) -\\\\\n",
    " &  &\n",
    "-\\frac{1}{|R^0 \\cup R^1|}(\\sum_{(\\vec{x},1) \\in R^1} \\ln(y_r)  \n",
    "+\\sum_{(\\vec{x},0) \\in R^0} \\ln(1 - y_r))\\\\\n",
    "& = & -\\frac{1}{|L^0 \\cup L^1|} (|L^1|\\; \\ln(y_l) + \n",
    "|L^0|\\; \\ln(1 - y_l)) -\\\\\n",
    "&  & -\\frac{1}{|R^0 \\cup R^1|} (|R^1|\\; \\ln(y_r) + \n",
    "|R^0|\\; \\ln(1 - y_r))\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "*Minimizing* the loss version with the **SUM** produces the same result as the version with the **MEAN**:\n",
    "\n",
    "$$\n",
    "L (t_{\\lambda\\rightarrow\\sigma},{\\cal D})  =  \n",
    "-|L^1|\\; \\ln(y_l) - |L^0|\\; \\ln(1 - y_l)) - |R^1|\\; \\ln(y_r) -\n",
    "|R^0|\\; \\ln(1 - y_r)\n",
    "$$\n",
    "\n",
    "For example, if $|L^1|=200$ and $|L^0|=800$, and $|R^1|=40$ and $|R^0| =60$, $y_l = 0.2$ and  $y_r = 0.4$.\n",
    "\n",
    "In general, left and right leaves can be solved independently.<br>\n",
    "Let $a = |L^1|$ and $b = |L^0|$, then \n",
    "$$loss = - a \\ln(y) - b \\ln(1-y)$$\n",
    "$$loss' = - a \\frac{1}{y} + b \\frac{1}{1-y}$$\n",
    "$$loss'' = a \\frac{1}{y^2} + b \\frac{1}{(1-y)^2}$$\n",
    "\n",
    "$\\forall y \\in (0,1),\\ a,b>0\\ :\\ loss'' > 0$. Thus, $loss$ is **convex**.\n",
    "\n",
    "To find the minimum, \n",
    "\n",
    "$$loss' = - a \\frac{1}{y} + b \\frac{1}{1-y} = 0$$\n",
    "$$- a (1-y) + b y = 0$$\n",
    "$$- a + a y + b y = 0$$\n",
    "$$y = \\frac{a}{a + b}$$\n",
    "\n",
    "So $y$ is the *mean* of all the values 0/1 in $L = L^1 \\cup L^0$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Indeed, in the loss formula we have to use $\\max(eps, \\min(1 - eps, y_r))$ and  $\\max(eps, \\min(1 - eps, y_l))$ in the logarithm. However, since in this case $y_r \\neq 0\\ or\\ 1$ and $y_l \\neq 0\\ or\\ 1$, the max-min is not needed. \n",
    "\n",
    "If we have also $U$:\n",
    "\n",
    "$$\\begin{array}{lcl} \n",
    "D & = & L \\cup R \\cup U =\\\\\n",
    "  & = & (L^0 \\cup L^1) \\cup (R^0 \\cup R^1) \n",
    "\\cup (U^0 \\cup U^1)\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "\n",
    "Finally, the new loss under attack.\n",
    "\n",
    "$$\n",
    "\\begin{array}{lcl}\n",
    "L (t_{\\lambda\\rightarrow\\sigma},{\\cal D}) &  = & \n",
    "-|L^1|\\; \\ln(y_l) - |L^0|\\; \\ln(1 - y_l)) - |R^1|\\; \\ln(y_r) -\n",
    "|R^0|\\; \\ln(1 - y_r) \\; + \\\\\n",
    "&& - \\sum_{(\\vec{x},0) \\in U^0} \\max(\\ln(1 - y_l), \\ln(1 - y_r)) \\; +\\\\\n",
    "&& - \\sum_{(\\vec{x},1) \\in U^1} \\max(\\ln(y_l), \\ln(y_r))\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "For example, if $|L^1|=200$ and $|L^0|=800$, and $|R^1|=40$ and $|R^0| =60$, and $|U^1|=3$ and $|U^0|=1$.\n",
    "\n",
    "Then:\n",
    "\n",
    "$$\n",
    "\\begin{array}{lcl}\n",
    "L (t_{\\lambda\\rightarrow\\sigma},{\\cal D}) &  = &\n",
    "-200\\; \\ln(y_l) - 800\\; \\ln(1 - y_l)) - 40\\; \\ln(y_r) -\n",
    "60\\; \\ln(1 - y_r) \\; + \\\\\n",
    "&& - \\max(\\ln(1 - y_l), \\ln(1 - y_r)) \\; +\\\\\n",
    "&& - 3\\; \\max(\\ln(y_l), \\ln(y_r))\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "where $(0.1998, 0.417476) = \\arg\\min_{(y_l,y_r)} L (t_{\\lambda\\rightarrow\\sigma},{\\cal D})$.\n",
    "\n",
    "\n",
    "Is this problem of min/max equivalent to the following?\n",
    "\n",
    " - let $\\mathcal{U} = \\{(U_l,U_r)\\ \\ |\\ \\ U_l \\cup U_r = U\\  \\land \\ U_l \\cap U_r = \\emptyset\\}$, where $|\\mathcal{U}| = 2^{|U|}$.\n",
    " - $\\forall (U_{l},U_{r})_i \\in \\mathcal{U}\\ :\\  \\mathbf{(y_l^i,y_r^i)} = \\mbox{argmin}_{(y_l,y_r)}\\ L (t_{\\lambda\\rightarrow\\sigma},{\\cal D})$, where ${\\cal D} = L \\cup R \\cup U_{l} \\cup U_{r}$\n",
    " - Maximize over all the partitions, each entailing a different pair of guesses $\\mathbf{(y_l^i,y_r^i)}$:\n",
    " \n",
    " $$\\mathbf{(U_l,U_r)} = \\mbox{argmax}_{(U_l,U_r)_i \\in \\mathcal{U}}\\ \\ \\ \n",
    "-|L^1|\\; \\ln(y_l^i) - |L^0|\\; \\ln(1 - y_l^i)) - |R^1|\\; \\ln(y_r^i) -\n",
    "|R^0|\\; \\ln(1 - y_r^i) -|U_l^1|\\; \\ln(y_l^i) - |U_l^0|\\; \\ln(1 - y_l^i)) - |U_r^1|\\; \\ln(y_r^i) -\n",
    "|U_r^0|\\; \\ln(1 - y_r^i) $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WolframAlpha\n",
    "\n",
    "Suppose that \n",
    " * $L$ is composed of 20% of 1's (200), and 80% of 0's (800).\n",
    " * $R$ is composed of 40% of 1's (40), and 60% of 0's (60).\n",
    "\n",
    "In WolframAlpha:\n",
    "\n",
    "```\n",
    "FindArgMin[{-200  ln[l] - 800  ln[1-l]- 40  ln[r] - 60  ln[1-r], l>0, l<1,r>0, r<1}, {l,r}] \n",
    "```\n",
    "\n",
    "and finally $\\{l,r\\} = \\{0.2, 0.4\\}$.\n",
    "\n",
    "To Plot:\n",
    "\n",
    "\n",
    "```\n",
    "plot -200  ln[l] - 800  ln[1-l]- 40  ln[r] - 60  ln[1-r], l=0..1, r=0..1\n",
    "```\n",
    "\n",
    "In WolframAlpha with the loss under attack:\n",
    "\n",
    "```\n",
    "FindArgMin[{-200  ln[l] - 800  ln[1-l]- 40  ln[r] - 60  ln[1-r]  − max(ln(1−l),ln(1−r)) -3 max(ln(l),ln(r)), l>0, l<1,r>0, r<1}, {l,r}] \n",
    "```\n",
    "\n",
    "and finally $\\{l,r\\} = \\{0.1998, 0.417476\\}$ rather than: $\\{0.2, 0.4\\}$.\n",
    "\n",
    "To Plot the new function:\n",
    "\n",
    "```\n",
    "plot -200  ln[l] - 800  ln[1-l]- 40  ln[r] - 60  ln[1-r] -  max(ln(1−l),ln(1−r)) -3 max(ln(l),ln(r)), l=0..1, r=0..1\n",
    "```\n",
    "\n",
    "\n",
    "## Plotting the function\n",
    "\n",
    "Discontinuities are not visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUMlog-loss: 3.819085009768883\n",
      "MEANlog-loss: 0.3182570841474069\n",
      "scikit logloss: 0.3182570841474069\n",
      "log-loss_UA: 108.53642527545964\n"
     ]
    }
   ],
   "source": [
    "# loss for true label = 1\n",
    "def log_loss1(predicted, eps=1e-15):\n",
    "    p = np.clip(predicted, eps, 1 - eps) # clipping\n",
    "    return -np.log(p)\n",
    "\n",
    "# loss for true label = 0\n",
    "def log_loss0(predicted, eps=1e-15):\n",
    "    p = np.clip(predicted, eps, 1 - eps) # clipping\n",
    "    return -np.log(1 - p)\n",
    "\n",
    "L=np.array([0,0,0,1,1,0])  # true labels\n",
    "#L=np.array([1,1,1,1,1,1])  # ERROR IN SCIKIR-LEARN\n",
    "R=np.array([1,1,1,1,1,1])  # true labels\n",
    "\n",
    "\n",
    "Yl = np.sum(L)/len(L) # guess left\n",
    "Yr = np.sum(R)/len(R) # guess right\n",
    "\n",
    "\n",
    "\n",
    "#  Loss for L and R\n",
    "def LogLoss(L,R, left, right):\n",
    "    return  np.sum(L) * log_loss1(left) + \\\n",
    "            (len(L) - np.sum(L)) * log_loss0(left) + \\\n",
    "            np.sum(R) * log_loss1(right) + \\\n",
    "            (len(R) - np.sum(R)) * log_loss0(right)\n",
    "\n",
    "#  MEAN Loss for L and R\n",
    "def MeanLogLoss(L,R, left, right):\n",
    "    return  (np.sum(L) * log_loss1(left) + \\\n",
    "            (len(L) - np.sum(L)) * log_loss0(left) + \\\n",
    "            np.sum(R) * log_loss1(right) + \\\n",
    "            (len(R) - np.sum(R)) * log_loss0(right)) / (len(L)+len(R))\n",
    "    \n",
    "#  Loss for U\n",
    "def LogLoss_U(U, left, right):\n",
    "    return  np.sum(U) * max(log_loss1(left), log_loss1(right)) + \\\n",
    "            (len(U) - np.sum(U)) * max(log_loss0(left), log_loss0(right))\n",
    "            \n",
    "\n",
    "print(\"SUMlog-loss:\", LogLoss(L,R,Yl,Yr))\n",
    "print(\"MEANlog-loss:\", MeanLogLoss(L,R,Yl,Yr))\n",
    "\n",
    "\n",
    "# Checking scikit. There is a bug when all the true labels are uniform!!\n",
    "# Try with L=np.array([1,1,1,1,1,1])\n",
    "from sklearn.metrics import log_loss\n",
    "Pl = np.full(len(L), Yl)\n",
    "Pr = np.full(len(R), Yr)\n",
    "print(\"scikit logloss:\", \n",
    "      log_loss(np.concatenate((L, R), axis=None), np.concatenate((Pl, Pr), axis=None)))\n",
    "\n",
    "\n",
    "U = np.array([0,0,0,1])\n",
    "#U = np.array([])\n",
    "\n",
    "#  Loss Under Attack\n",
    "def Loss_A(L,R,U,left,right):\n",
    "    return LogLoss(L,R,left,right) + LogLoss_U(U, left, right)\n",
    "\n",
    "def __logloss_under_max_attack(L, R, U, left, right):\n",
    "        \"\"\"\n",
    "        Compute LOG-LOSS Under Max Attack.\n",
    "        \"\"\"\n",
    "        return np.sum(L) * log_loss1(left) + \\\n",
    "            (len(L) - np.sum(L)) * log_loss0(left) + \\\n",
    "            np.sum(R) * log_loss1(right) + \\\n",
    "            (len(R) - np.sum(R)) * log_loss0(right) + \\\n",
    "            np.sum(U) * max(log_loss1(left), log_loss1(right)) + \\\n",
    "            (len(U) - np.sum(U)) * max(log_loss0(left), log_loss0(right))\n",
    "\n",
    "print(\"log-loss_UA:\", Loss_A(L,R,U,Yl,Yr))\n",
    "\n",
    "\n",
    "\n",
    "def plot_cpqf(dataset, L_ids,R_ids,U_ids, loss_f, solution=None, grain=20):\n",
    "    L = dataset[L_ids]\n",
    "    R = dataset[R_ids]\n",
    "    if (len(U_ids) == 0):\n",
    "        U = np.array([])\n",
    "    else:\n",
    "        U = dataset[U_ids]\n",
    "    \n",
    "    lower = np.min( [np.min(L), np.min(R), np.min(U)] )\n",
    "    upper = np.max( [np.max(L), np.max(R), np.max(U)] )\n",
    "    \n",
    "    step = (upper-lower)/10\n",
    "    X, Y = np.mgrid[lower:upper+step:step, lower:upper+step:step]\n",
    "    \n",
    "    # plot decision tree boundary\n",
    "    Z = np.array( [loss_f(L,R,U, x,y) for x,y in np.c_[X.ravel(), Y.ravel()] ] ) \n",
    "    Z = Z.reshape(X.shape)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.contourf(X, Y, Z, alpha=.5) #, vmin=min(Y), vmax=max(Y))\n",
    "    \n",
    "    if solution:\n",
    "        plt.plot(solution[0], solution[1], 'ro')\n",
    "    \n",
    "    plt.xlabel(\"Left\")\n",
    "    plt.ylabel(\"Right\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "#from scipy.optimize import Bounds\n",
    "\n",
    "def find_best_leaves(dataset, L_ids, R_ids, U_ids, loss_f, mybounds, C=()):\n",
    "    L = dataset[L_ids]\n",
    "    R = dataset[R_ids]\n",
    "    if (len(U_ids) == 0):\n",
    "        U = np.array([])\n",
    "    else:\n",
    "        U = dataset[U_ids]\n",
    "    \n",
    "    # seed\n",
    "    x0 = np.array( [np.mean(L), np.mean(R)] )\n",
    "    print(\"SEED:\", x0)\n",
    "    \n",
    "    # loss function to be minimized\n",
    "    fun = lambda x: loss_f(L,R,U, x[0], x[1])\n",
    "        \n",
    "    # constrained optimization \n",
    "    res = minimize(fun, x0, method='SLSQP', bounds=mybounds, constraints=C)\n",
    "    \n",
    "    # check result\n",
    "    if not res.success:\n",
    "        print (\"Solver Error: \", res.message)\n",
    "        return None\n",
    "    \n",
    "    return res.x[0], res.x[1], res.fun"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example with U == $\\emptyset$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset = np.array([1,1,1,1,1,0,  0,0,0,   1,0,0,0,0,0,0,0,0,0,0])\n",
    "L=np.array([0,1,2,3,4,5])\n",
    "U=np.array([])\n",
    "R=np.array([6,7,8,9,10,11,12,13,14,15,16,17,18,19])\n",
    "\n",
    "constraints = ()\n",
    "mybounds = ((0.0, 1.0), (0.0, 1.0)) # the bounds must be fixed\n",
    "best_leaves = find_best_leaves(Dataset, L,R,U, Loss_A, mybounds, constraints)\n",
    "print (\"Solution\", best_leaves)\n",
    "\n",
    "# compute the mean as U is empty\n",
    "Dl = Dataset[L]\n",
    "Dr = Dataset[R]\n",
    "l_val = np.average(Dl)\n",
    "r_val = np.average(Dr)\n",
    "print (\"The solution is equivalent to the mean of L and R:\", l_val, r_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example with U !=  $\\emptyset$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000 450000 450000 100000\n",
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 9.78 µs\n",
      "SEED: [0.49942444 0.49981333]\n",
      "Solution (0.49958899018539965, 0.4995889975433525, 693146.8434511271)\n",
      "CHECK LOSS (Binary Splitting Under Attack): 693146.8434514595\n",
      "CHECK MEAN ->  left:  0.5494207212180856 rigth: 0.4497708311669613\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Dataset = np.array([1,1,1,1,1,0,  1,0,0,0,0,0,0,0,0,0,1,    0,0,1])\n",
    "#L=np.array([0,1,2,3,4,5])\n",
    "#R=np.array([6,7,8,9,10,11,12,13,14,15,16,17,18,19])\n",
    "#U=np.array([17,18,19])\n",
    "\n",
    "dim_D = 1000000\n",
    "Dataset = np.random.rand(dim_D)\n",
    "Dataset = Dataset.round()\n",
    "Dataset = Dataset.astype(int)\n",
    "#print(Dataset)\n",
    "\n",
    "dim_U = int(.1 * dim_D)\n",
    "dim_L = int((dim_D - dim_U)/2)\n",
    "dim_R = dim_D - dim_L - dim_U\n",
    "print(dim_D, dim_L, dim_R, dim_U)\n",
    "\n",
    "L = np.arange(dim_L)\n",
    "R = np.arange(dim_L, dim_L+dim_R)\n",
    "U = np.arange(dim_L+dim_R, dim_D)\n",
    "\n",
    "\n",
    "constraints = ()\n",
    "mybounds = ((0.0, 1.0), (0.0, 1.0)) # the bounds must be fixed\n",
    "\n",
    "%time\n",
    "best_leaves = find_best_leaves(Dataset, L,R,U, Loss_A, mybounds, constraints)\n",
    "if best_leaves:\n",
    "    print (\"Solution\", best_leaves)\n",
    "\n",
    "    # check our method\n",
    "    Ld = Dataset[L]\n",
    "    Rd = Dataset[R]\n",
    "    Ud = Dataset[U]\n",
    "\n",
    "    Ul = 0\n",
    "    Ur = 0\n",
    "    sz_Ul = 0\n",
    "    sz_Ur = 0\n",
    "    loss_val = LogLoss(Ld, Rd, best_leaves[0], best_leaves[1])\n",
    "    for v in Ud:\n",
    "        if v == 1:\n",
    "            if log_loss1(best_leaves[1]) > log_loss1(best_leaves[0]):\n",
    "                Ur += 1\n",
    "                sz_Ur += 1\n",
    "                loss_val += log_loss1(best_leaves[1])\n",
    "            else:\n",
    "                Ul += 1\n",
    "                sz_Ul += 1\n",
    "                loss_val += log_loss1(best_leaves[0])\n",
    "        else:\n",
    "            if log_loss0(best_leaves[1]) > log_loss0(best_leaves[0]):\n",
    "                sz_Ur += 1\n",
    "                loss_val += log_loss0(best_leaves[1])\n",
    "            else:\n",
    "                sz_Ul += 1\n",
    "                loss_val += log_loss0(best_leaves[0])\n",
    "                \n",
    "\n",
    "    print(\"CHECK LOSS (Binary Splitting Under Attack):\", loss_val)\n",
    "    print(\"CHECK MEAN ->  left: \", (np.sum(Ld)+Ul) / (len(Ld)+sz_Ul),  \"rigth:\", (np.sum(Rd)+Ur) / (len(Rd)+sz_Ur))\n",
    "\n",
    "    plot_cpqf(Dataset, L,R,U, Loss_A, [best_leaves[0], best_leaves[1]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example with sklearn log-loss (bug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BUG in log_loss\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "T = np.array([1,1,1,0,0,1,1])  # true labels\n",
    "Pred = np.array([.7,.8,1.0,.2,.3,.9,.9])  # predicted prob of being 1/true  \n",
    "print(\"scikit logloss:\",   log_loss(T, Pred))\n",
    "\n",
    "#data set omogeneo\n",
    "T = np.array([1,1,1,1,1,1,1])\n",
    "print(\"scikit logloss:\",   log_loss(T, Pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example with U != $\\emptyset$ and constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset = np.array([1,1,1,1,1,0,  0,0,0,   1,0,0,0,0,0,0,0,0,0,0])\n",
    "L=np.array([0,1,2,3,4,5])\n",
    "U=np.array([6,7,8])\n",
    "R=np.array([9,10,11,12,13,14,15,16,17,18,19])\n",
    "\n",
    "mybounds = ((0.0, 1.0), (0.0, 1.0)) # the bounds must be fixed\n",
    "\n",
    "# constrains are in the form\n",
    "#  cons = ({'type': 'ineq', 'fun': lambda x:  x[0] - 2 * x[1] + 2},\n",
    "# meaning that constraint function has to be non negative  (>= 0)\n",
    "constraints = ( {'type': 'ineq',        # min error in left leaf:   loss(Dataset[8]) >= .5, where Dataset[8]=0 or 1, \n",
    "                                        # becomes:    -.5 + loss(Dataset[8]) >= 0\n",
    "                 'fun': lambda leaves:  np.array([-.5 + Dataset[8] * log_loss1(leaves[0]) + (1-Dataset[8]) * log_loss0(leaves[0])])},\n",
    "               {'type': 'ineq',         # max error in right leaf:  loss(Dataset[8]) <= .5, where Dataset[8]=0 or 1,\n",
    "                                        # becomes:    .5 - loss(Dataset[8]) >= 0\n",
    "                 'fun': lambda leaves:  np.array([.5 - Dataset[8] * log_loss1(leaves[1]) - (1-Dataset[8]) * log_loss0(leaves[1])])} )\n",
    "\n",
    "# https://docs.scipy.org/doc/scipy-0.18.1/reference/tutorial/optimize.html\n",
    "# in the \"Constrained minimization of multivariate scalar functions (minimize)\" they use np.array([...]) in the constraint\n",
    "best_leaves = find_best_leaves(Dataset, L,R,U, Loss_A, mybounds, constraints)\n",
    "print (\"Solution\", best_leaves)\n",
    "if best_leaves:\n",
    "    plot_cpqf(Dataset, L,R,U, Loss_A, [best_leaves[0], best_leaves[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\arg\\min_{y_l, y_r}\\  f(L,y_l) + f(R,r) + f(U_l,y_l) + f(U_r,y_r)$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "187px",
    "left": "744px",
    "right": "20px",
    "top": "64px",
    "width": "520px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
