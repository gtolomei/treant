{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from parallel_robust_forest import load_attack_rules\n",
    "from sklearn.model_selection import train_test_split\n",
    "from joblib import Parallel,delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "def load_dataset(path, \n",
    "                 dataset_filename, \n",
    "                 sep=\",\", \n",
    "                 header=None,\n",
    "                 nrows=None,\n",
    "                 names=None, \n",
    "                 index_col=False, \n",
    "                 na_values='?'):\n",
    "    \n",
    "    return pd.read_csv(os.path.join(path,dataset_filename), \n",
    "                       sep=sep, \n",
    "                       header=header,\n",
    "                       nrows=nrows,\n",
    "                       names=names, \n",
    "                       index_col=index_col, \n",
    "                       na_values='?')\n",
    "\n",
    "def binarize_data(data, label, threshold):\n",
    "    data[label] = np.where(data[label] >= threshold, 1, -1)\n",
    "\n",
    "    \n",
    "def discretize(datasets, n_bins, skip_columns=[]):\n",
    "    full = pd.concat(datasets)\n",
    "    for col in full.columns.values:\n",
    "        if not col in skip_columns:\n",
    "            # process full\n",
    "            new_col, bins = pd.qcut(full[col], n_bins, duplicates='drop', retbins=True, labels=False)\n",
    "            # apply other\n",
    "            for d in datasets:\n",
    "                new_col = pd.cut(d[col], bins, labels=False, include_lowest=True)  \n",
    "                d[col] = bins[new_col]\n",
    "def serialize_dataset(p_data, \n",
    "                      path, \n",
    "                      dataset_filename, \n",
    "                      suffix, \n",
    "                      sep=\",\", \n",
    "                      compression=\"bz2\", \n",
    "                      index=False):\n",
    "    \n",
    "    p_data.to_csv(path+\"/\"+dataset_filename.split(\".\")[0]+suffix, \n",
    "                  sep=sep, \n",
    "                  compression=compression, \n",
    "                  index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb_instance(x, rules, budget, max_budget_per_feature, thresholds):\n",
    "    \"\"\"\n",
    "    Returns the set of possible perturbations of a given instance.\n",
    "\n",
    "    This function takes as input an instance and returns a set of perturbations of that instance, \n",
    "    using the specified amount of budget and considering the cost of perturbing each individual feature.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : pandas.Series\n",
    "        The original instance\n",
    "    rules : list\n",
    "        The list of modification rules\n",
    "    budget : float\n",
    "        The attacker's budget\n",
    "    max_budget_per_feature : dict\n",
    "        The maximum allowed amount of budget units that can be spend on each feature\n",
    "    thresholds : dict\n",
    "        feature -> list of relevant thresholds\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        The set of perturbations (including the original instance, placed at the very beginning)\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize the queue (FIFO) with both the original instance, \n",
    "    # the initial budget, and an empty dictionary of budget units spent so far\n",
    "    queue = [(x, budget, {})]\n",
    "    # visited perturbations\n",
    "    seen = { tuple(x): [budget, {}] }\n",
    "    # initialize the set of perturbations of this instance with the empty list\n",
    "    # perturbations = []\n",
    "    \n",
    "    # loop until the queue is not empty\n",
    "    while len(queue)>0:\n",
    "        item = queue.pop() # dequeue the first inserted element\n",
    "        x = item[0] # get the instance\n",
    "        b = item[1] # get the residual budget\n",
    "        budget_units_spent = item[2] # get the dictionary containing the amount of budget spent on each feature, so far\n",
    "        \n",
    "        # loop through all the features subject to the set of attack rules\n",
    "        for r in rules:\n",
    "            f = x.index.values[r.get_target_feature()]\n",
    "            # check budget\n",
    "            if not( r.get_cost() <= b and \n",
    "                   budget_units_spent.get(f, 0) + r.get_cost() <= max_budget_per_feature[f] ):\n",
    "                continue\n",
    "            # check validity\n",
    "            if not r.is_applicable(x):\n",
    "                continue\n",
    "            \n",
    "            # apply rule to a copy\n",
    "            x_atks = []\n",
    "            if not r.is_num():\n",
    "                xx = r.apply(x)\n",
    "                x_atks += [xx]\n",
    "            else:\n",
    "                x_tmp = r.apply(x)\n",
    "                # Evaluate crossing of multiple thresholds\n",
    "                low,high=sorted([x[f], x_tmp[f]])\n",
    "                z = set(thresholds[f][np.logical_and(thresholds[f]>=low, thresholds[f]<=high)])\n",
    "                z |= set([low,high])\n",
    "                # Evaluate extremes of validity interval\n",
    "                extremes = r.get_pre_interval()\n",
    "                z |= set([t for t in extremes if low < t < high])\n",
    "                \n",
    "                for zi in z:\n",
    "                    xx = x.copy()\n",
    "                    xx[f] = zi\n",
    "                    x_atks += [xx]\n",
    "\n",
    "            # process all atks\n",
    "            for xx in x_atks:\n",
    "                # skip if already seen and with a larger residual budget\n",
    "                xx_t = tuple(xx)\n",
    "                res_b = b - r.get_cost()\n",
    "                seen_budgets = seen.get(xx_t)\n",
    "                if seen_budgets is not None and seen_budgets[0] >= res_b:\n",
    "                    continue\n",
    "\n",
    "                # update budgets spent\n",
    "                updated_budget_units_spent = budget_units_spent.copy()\n",
    "                updated_budget_units_spent[f] = updated_budget_units_spent.get(f,0) + r.get_cost()\n",
    "                # add to frontier and to past seen elements\n",
    "                seen[xx_t] = [res_b, updated_budget_units_spent]\n",
    "                queue.append([xx, res_b, updated_budget_units_spent])\n",
    "    \n",
    "    perturbations_df = pd.DataFrame.from_records(list(seen.keys()), columns=x.index.values)\n",
    "    perturbations_df = perturbations_df.drop_duplicates()\n",
    "    return perturbations_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb_thread(x, rules, budget, max_budget_per_feature, thresholds,skip_class,instance_id =-1):\n",
    "    \n",
    "    if instance_id%500==0:\n",
    "        print(\"***** Perturbing instance [ID = #{}]... *****\".format(instance_id))  \n",
    "        \n",
    "    if skip_class is not None and instance[-1]==skip_class:\n",
    "        # keep the original instance only\n",
    "        perturbations = pd.DataFrame([x])\n",
    "    else:\n",
    "            \n",
    "        perturbations = perturb_instance(x=x, rules=rules, budget=budget, \n",
    "                                             max_budget_per_feature=max_budget_per_feature,\n",
    "                                             thresholds=thresholds) \n",
    "            \n",
    "    perturbations.insert(loc=0,  column=\"instance_id\", \n",
    "                            value=[instance_id for i in range(perturbations.shape[0])], \n",
    "                            allow_duplicates=True)\n",
    "    return perturbations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perturb_dataset(data, budget, max_budget_per_feature, rules, skip_class=None):\n",
    "    \"\"\"\n",
    "    Returns the dataset extended with all instance perturbations.\n",
    "\n",
    "    This function takes as input a dataset and returns another dataset which is obtained from the original\n",
    "    by adding all the possible perturbations an attacker with budget B can apply to every instance.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pandas.DataFrame\n",
    "        The original dataset\n",
    "    rules : list\n",
    "        The list of modification rules\n",
    "    budget : float\n",
    "        The attacker's budget\n",
    "    max_budget_per_feature : dict\n",
    "        The maximum allowed amount of budget units that can be spend on each feature\n",
    "    costs : dict\n",
    "        A mapping between each feature and its cost of perturbation\n",
    "    skip_class : int\n",
    "        if class (i.e. last columns) equals skip_class, then instance is skipped\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pandas.DataFrame\n",
    "        The perturbed dataset\n",
    "\n",
    "    \"\"\"\n",
    "    if data is None or data.empty:\n",
    "        return # if not, just return None\n",
    "    \n",
    "    # compute valid thresholds\n",
    "    thresholds = {c:np.unique(data[c]) for c in data.columns}\n",
    "\n",
    "    # prepare the perturbed dataset to be returned, initially empty with an extra \"instance_id\" column\n",
    "    cols = [\"instance_id\"] + data.columns.tolist()\n",
    "    perturbed_data = pd.DataFrame(columns=cols)\n",
    "    \n",
    "    perturbed_data = Parallel(n_jobs=24)(delayed(perturb_thread)\n",
    "                                             (x=instance, rules=rules, budget=budget,\n",
    "                                                max_budget_per_feature=max_budget_per_feature,\n",
    "                                                thresholds=thresholds, skip_class=skip_class,\n",
    "                                                instance_id=instance_id+1) # start from id 1\n",
    "                                        for instance_id,(index, instance) in enumerate(data.iterrows()))\n",
    "    \n",
    "    perturbed_data = pd.concat(perturbed_data)\n",
    "    \n",
    "    # eventually, return the perturbed dataset\n",
    "    print(\"***** Return the final perturbed dataset *****\")\n",
    "    \n",
    "    return perturbed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WINE Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME=\"wine\"\n",
    "\n",
    "raw_colnames = ['fixed_acidity', 'volatile_acidity', 'citric_acid', 'residual_sugar', 'chlorides', 'free_sulfur_dioxide',\n",
    "            'total_sulfur_dioxide', 'density', 'pH', 'sulphites', 'alcohol', 'is_white', 'quality']\n",
    "\n",
    "DATASET_PATH=\"../data/{}\".format(DATASET_NAME)\n",
    "RAW_DATASET_PATH=DATASET_PATH + \"/raw\"\n",
    "ATK_DATASET_PATH=DATASET_PATH + \"/attacks\"\n",
    "\n",
    "ATKS_FILE = os.path.join(ATK_DATASET_PATH, \"attacks.json\")\n",
    "\n",
    "TRAINING_SET=\"train.csv.bz2\"\n",
    "VALIDATION_SET=\"valid.csv.bz2\"\n",
    "TEST_SET=\"test.csv.bz2\"\n",
    "N_TRAIN_INSTANCES=None # replace this with None to load the whole training set\n",
    "N_TEST_INSTANCES=None # replace this with None to load the whole test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "train = load_dataset(RAW_DATASET_PATH, TRAINING_SET, names=raw_colnames, header=0, nrows=N_TRAIN_INSTANCES)\n",
    "valid = load_dataset(RAW_DATASET_PATH, VALIDATION_SET, names=raw_colnames, header=0)\n",
    "test  = load_dataset(RAW_DATASET_PATH, TEST_SET, names=raw_colnames, header=0, nrows=N_TEST_INSTANCES)\n",
    "\n",
    "# binarize\n",
    "binarize_data(train, \"quality\", 6)\n",
    "binarize_data(valid, \"quality\", 6)\n",
    "binarize_data(test, \"quality\", 6)\n",
    "\n",
    "#discretize\n",
    "discretize([train, valid, test], 128, [\"quality\"])\n",
    "\n",
    "print(\"Shape of training set: {}\".format(train.shape))\n",
    "print(\"Shape of validation set: {}\".format(valid.shape))\n",
    "print(\"Shape of test set: {}\".format(test.shape))\n",
    "\n",
    "serialize_dataset(train, DATASET_PATH, TRAINING_SET, \".csv.bz2\")\n",
    "serialize_dataset(valid, DATASET_PATH, VALIDATION_SET, \".csv.bz2\")\n",
    "serialize_dataset(test, DATASET_PATH, TEST_SET, \".csv.bz2\")\n",
    "\n",
    "#train.head()\n",
    "# valid.head()\n",
    "# test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WITH ATTACKS\n",
    "\n",
    "######################\n",
    "# Attacker Definition\n",
    "######################\n",
    "\n",
    "max_budget_per_feature = {\n",
    "    'alcohol'          : 1000,\n",
    "    'residual_sugar'   : 1000,\n",
    "    'volatile_acidity' : 1000,\n",
    "    'free_sulfur_dioxide' :1000\n",
    "}\n",
    "\n",
    "attacker_rules = load_attack_rules(ATKS_FILE, list(train.columns.values) )\n",
    "\n",
    "B = [60] #[20,40,60,80]\n",
    "\n",
    "\n",
    "for budget in B:\n",
    "    print (\"Processing Budget: \", budget)\n",
    "    train_att = perturb_dataset(train, budget, max_budget_per_feature, attacker_rules, skip_class=None)\n",
    "    valid_att = perturb_dataset(valid, budget, max_budget_per_feature, attacker_rules, skip_class=None)\n",
    "    test_att  = perturb_dataset(test, budget, max_budget_per_feature, attacker_rules, skip_class=None)\n",
    "    \n",
    "    serialize_dataset(train_att, ATK_DATASET_PATH, TRAINING_SET, \"_B{}\".format(budget)+\".atks.bz2\")\n",
    "    serialize_dataset(valid_att, ATK_DATASET_PATH, VALIDATION_SET, \"_B{}\".format(budget)+\".atks.bz2\")\n",
    "    serialize_dataset(test_att, ATK_DATASET_PATH, TEST_SET, \"_B{}\".format(budget)+\".atks.bz2\")\n",
    "    \n",
    "    print(\"Shape of attacked training set: {}\".format(train_att.shape))\n",
    "    print(\"Shape of attacked validation set: {}\".format(valid_att.shape))\n",
    "    print(\"Shape of attacked test set: {}\".format(test_att.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CENSUS Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME=\"census\"\n",
    "\n",
    "raw_colnames = ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status',\n",
    "                'occupation', 'relationship', 'race', 'sex', 'capital_gain', 'capital_loss', \n",
    "                'hours_per_week', 'native_country', 'income_greater_than_50k']\n",
    "\n",
    "DATASET_PATH=\"../data/{}\".format(DATASET_NAME)\n",
    "RAW_DATASET_PATH=DATASET_PATH + \"/raw\"\n",
    "ATK_DATASET_PATH=DATASET_PATH + \"/attacks\"\n",
    "\n",
    "ATKS_FILE = os.path.join(ATK_DATASET_PATH, \"attacks.json\")\n",
    "\n",
    "TRAINING_SET=\"train.csv.bz2\"\n",
    "VALIDATION_SET=\"valid.csv.bz2\"\n",
    "TEST_SET=\"test.csv.bz2\"\n",
    "\n",
    "N_TRAIN_INSTANCES=None # replace this with None to load the whole training set\n",
    "N_TEST_INSTANCES=None # replace this with None to load the whole test set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WITHOUT ATTACKS\n",
    "\n",
    "# load\n",
    "train = load_dataset(RAW_DATASET_PATH, TRAINING_SET, names=raw_colnames, header=0, nrows=N_TRAIN_INSTANCES)\n",
    "test = load_dataset(RAW_DATASET_PATH, TEST_SET, names=raw_colnames, header=0, nrows=N_TEST_INSTANCES)\n",
    "\n",
    "# remove education string\n",
    "train = train.drop(['education'], axis=1)\n",
    "test  = test.drop(['education'], axis=1)\n",
    "# drop NA\n",
    "train = train[~train.isnull().any(axis=1)]\n",
    "test  = test[~test.isnull().any(axis=1)]\n",
    "\n",
    "# create the missing validation set\n",
    "train, valid = train_test_split( train, \n",
    "                                 test_size=0.1, \n",
    "                                 random_state=42, \n",
    "                                 stratify=train[\"income_greater_than_50k\"])\n",
    "\n",
    "print(\"Shape of training set: {}\".format(train.shape))\n",
    "print(\"Shape of validation set: {}\".format(valid.shape))\n",
    "print(\"Shape of test set: {}\".format(test.shape))\n",
    "\n",
    "serialize_dataset(train, DATASET_PATH, TRAINING_SET, \".csv.bz2\")\n",
    "serialize_dataset(valid, DATASET_PATH, VALIDATION_SET, \".csv.bz2\")\n",
    "serialize_dataset(test, DATASET_PATH, TEST_SET, \".csv.bz2\")\n",
    "\n",
    "\n",
    "# train.head()\n",
    "# valid.head()\n",
    "# test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## WITH ATTACKS\n",
    "\n",
    "######################\n",
    "# Attacker Definition\n",
    "######################\n",
    "max_budget_per_feature = {\n",
    "    'workclass'     : 1000,\n",
    "    'marital_status': 1000,\n",
    "    'occupation'    : 1000,\n",
    "    'education_num' : 1000,\n",
    "    'hours_per_week': 1000,\n",
    "    'capital_gain'  : 1000\n",
    "}\n",
    "\n",
    "B = [30,60] # 90\n",
    "\n",
    "attacker_rules = load_attack_rules(ATKS_FILE, list(train.columns.values) )\n",
    "\n",
    "for budget in B:\n",
    "    print (\"Processing Budget: \", budget)\n",
    "    train_att = perturb_dataset(train, budget, max_budget_per_feature, attacker_rules, skip_class=None)\n",
    "    valid_att = perturb_dataset(valid, budget, max_budget_per_feature, attacker_rules, skip_class=None)\n",
    "    test_att  = perturb_dataset(test, budget, max_budget_per_feature, attacker_rules, skip_class=None)\n",
    "    \n",
    "    serialize_dataset(train_att, ATK_DATASET_PATH, TRAINING_SET, \"_B{}\".format(budget)+\".atks.bz2\")\n",
    "    serialize_dataset(valid_att, ATK_DATASET_PATH, VALIDATION_SET, \"_B{}\".format(budget)+\".atks.bz2\")\n",
    "    serialize_dataset(test_att, ATK_DATASET_PATH, TEST_SET, \"_B{}\".format(budget)+\".atks.bz2\")\n",
    "    \n",
    "    print(\"Shape of attacked training set: {}\".format(train_att.shape))\n",
    "    print(\"Shape of attacked validation set: {}\".format(valid_att.shape))\n",
    "    print(\"Shape of attacked test set: {}\".format(test_att.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPAM Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME=\"spam\"\n",
    "\n",
    "colnames = ['word_freq_make', 'word_freq_address', 'word_freq_all', 'word_freq_3d',\n",
    "       'word_freq_our', 'word_freq_over', 'word_freq_remove',\n",
    "       'word_freq_internet', 'word_freq_order', 'word_freq_mail',\n",
    "       'word_freq_receive', 'word_freq_will', 'word_freq_people',\n",
    "       'word_freq_report', 'word_freq_addresses', 'word_freq_free',\n",
    "       'word_freq_business', 'word_freq_email', 'word_freq_you',\n",
    "       'word_freq_credit', 'word_freq_your', 'word_freq_font', 'word_freq_000',\n",
    "       'word_freq_money', 'word_freq_hp', 'word_freq_hpl', 'word_freq_george',\n",
    "       'word_freq_650', 'word_freq_lab', 'word_freq_labs', 'word_freq_telnet',\n",
    "       'word_freq_857', 'word_freq_data', 'word_freq_415', 'word_freq_85',\n",
    "       'word_freq_technology', 'word_freq_1999', 'word_freq_parts',\n",
    "       'word_freq_pm', 'word_freq_direct', 'word_freq_cs', 'word_freq_meeting',\n",
    "       'word_freq_original', 'word_freq_project', 'word_freq_re',\n",
    "       'word_freq_edu', 'word_freq_table', 'word_freq_conference',\n",
    "       'char_freq_;', 'char_freq_(', 'char_freq_[', 'char_freq_!',\n",
    "       'char_freq_$', 'char_freq_#', 'capital_run_length_average',\n",
    "       'capital_run_length_longest', 'capital_run_length_total', 'spam']\n",
    "\n",
    "DATASET_PATH=\"../data/{}\".format(DATASET_NAME)\n",
    "RAW_DATASET_PATH=DATASET_PATH + \"/raw\"\n",
    "ATK_DATASET_PATH=DATASET_PATH + \"/attacks\"\n",
    "\n",
    "ATKS_FILE = os.path.join(ATK_DATASET_PATH, \"attacks.json\")\n",
    "\n",
    "TRAINING_SET=\"train.csv.bz2\"\n",
    "VALIDATION_SET=\"valid.csv.bz2\"\n",
    "TEST_SET=\"test.csv.bz2\"\n",
    "N_TRAIN_INSTANCES=None # replace this with None to load the whole training set\n",
    "N_TEST_INSTANCES=None # replace this with None to load the whole test set\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(os.path.join(RAW_DATASET_PATH, \"spam.csv\"))\n",
    "train['class'] = train['class'].map({1: 1, 0: -1})\n",
    "\n",
    "train = train.sample(frac=1).reset_index(drop=True) #to reshuffle\n",
    "\n",
    "print(\"Number of instances = {}\\nNumber of features = {}\".format(train.shape[0], train.shape[1] - 1))\n",
    "print(\"Class distribution:\\n{}\".format(train['class'].value_counts()))\n",
    "print(\"Class distribution:\\n{}\".format(train['class'].value_counts()/len(train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(train)*.6)\n",
    "valid_size = int(len(train)*.2)\n",
    "\n",
    "valid = train[train_size:train_size+valid_size]\n",
    "test  = train[train_size+valid_size:]\n",
    "train = train[:train_size]\n",
    "\n",
    "\n",
    "print(\"Shape of training set: {}\".format(train.shape))\n",
    "print(\"Shape of validation set: {}\".format(valid.shape))\n",
    "print(\"Shape of test set: {}\".format(test.shape))\n",
    "print(\"Class distribution:\\n{}\".format(train['class'].value_counts()/len(train)))\n",
    "\n",
    "print(\"Class distribution:\\n{}\".format(valid['class'].value_counts()/len(valid)))\n",
    "\n",
    "print(\"Class distribution:\\n{}\".format(test['class'].value_counts()/len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "#train = load_dataset(RAW_DATASET_PATH, TRAINING_SET, names=colnames, header=0, nrows=N_TRAIN_INSTANCES)\n",
    "#valid = load_dataset(RAW_DATASET_PATH, VALIDATION_SET, names=colnames, header=0)\n",
    "#test = load_dataset(RAW_DATASET_PATH, TEST_SET, names=colnames, header=0, nrows=N_TEST_INSTANCES)\n",
    "\n",
    "# binarize\n",
    "binarize_data(train, \"class\", 0.5)\n",
    "binarize_data(valid, \"class\", 0.5)\n",
    "binarize_data(test, \"class\", 0.5)\n",
    "\n",
    "#discretize\n",
    "discretize([train, valid, test], 64, [\"class\"])\n",
    "\n",
    "print(\"Shape of training set: {}\".format(train.shape))\n",
    "print(\"Shape of validation set: {}\".format(valid.shape))\n",
    "print(\"Shape of test set: {}\".format(test.shape))\n",
    "\n",
    "serialize_dataset(train, DATASET_PATH, TRAINING_SET, \".csv.bz2\")\n",
    "serialize_dataset(valid, DATASET_PATH, VALIDATION_SET, \".csv.bz2\")\n",
    "serialize_dataset(test, DATASET_PATH, TEST_SET, \".csv.bz2\")\n",
    "\n",
    "# train.head()\n",
    "# valid.head()\n",
    "# test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "max_budget_per_feature = {\n",
    "    'char_freq_!'     : 1000,\n",
    "    'word_freq_remove': 1000,\n",
    "    'char_freq_$'     : 1000,\n",
    "    'capital_run_length_average' : 1000,\n",
    "    'capital_run_length_total': 1000,\n",
    "    'word_freq_hp'  : 1000\n",
    "}\n",
    "\n",
    "B = [50, 60 ]\n",
    "attacker_rules = load_attack_rules(ATKS_FILE, list(train.columns.values) )\n",
    "for budget in B:\n",
    "    print (\"Processing Budget: \", budget)\n",
    "    train_att = perturb_dataset(train, budget, max_budget_per_feature, attacker_rules, skip_class=None)\n",
    "    valid_att = perturb_dataset(valid, budget, max_budget_per_feature, attacker_rules, skip_class=None)\n",
    "    test_att  = perturb_dataset(test, budget, max_budget_per_feature, attacker_rules, skip_class=None)\n",
    "    \n",
    "    serialize_dataset(train_att, ATK_DATASET_PATH, TRAINING_SET, \"_B{}\".format(budget)+\".atks.bz2\")\n",
    "    serialize_dataset(valid_att, ATK_DATASET_PATH, VALIDATION_SET, \"_B{}\".format(budget)+\".atks.bz2\")\n",
    "    serialize_dataset(test_att, ATK_DATASET_PATH, TEST_SET, \"_B{}\".format(budget)+\".atks.bz2\")\n",
    "    \n",
    "    print(\"Shape of attacked training set: {}\".format(train_att.shape))\n",
    "    print(\"Shape of attacked validation set: {}\".format(valid_att.shape))\n",
    "    print(\"Shape of attacked test set: {}\".format(test_att.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CREDIT Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME=\"credit\"\n",
    "\n",
    "DATASET_PATH=\"../data/{}\".format(DATASET_NAME)\n",
    "RAW_DATASET_PATH=DATASET_PATH + \"/raw\"\n",
    "ATK_DATASET_PATH=DATASET_PATH + \"/attacks\"\n",
    "\n",
    "ATKS_FILE = os.path.join(ATK_DATASET_PATH, \"attacks.json\")\n",
    "\n",
    "TRAINING_SET=\"train.csv.bz2\"\n",
    "VALIDATION_SET=\"valid.csv.bz2\"\n",
    "TEST_SET=\"test.csv.bz2\"\n",
    "N_TRAIN_INSTANCES=None # replace this with None to load the whole training set\n",
    "N_TEST_INSTANCES=None # replace this with None to load the whole test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(os.path.join(RAW_DATASET_PATH, \"credit.csv\"))\n",
    "train.drop(columns=\"ID\", inplace=True)\n",
    "\n",
    "binarize_data(train, \"default.payment.next.month\", .5)\n",
    "\n",
    "discretize([train], 128, [\"default.payment.next.month\"])\n",
    "\n",
    "print(\"Number of instances = {}\\nNumber of features = {}\".format(train.shape[0], train.shape[1] - 1))\n",
    "print(\"Class distribution:\\n{}\".format(train['default.payment.next.month'].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a (stratified) sample of N = 6,000 instances (i.e., 20%) out of the whole dataset\n",
    "#y = train[\"default.payment.next.month\"].to_frame()\n",
    "#X = train\n",
    "#_, X_test, _, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(train)*.6)\n",
    "valid_size = int(len(train)*.2)\n",
    "\n",
    "valid = train[train_size:train_size+valid_size]\n",
    "test  = train[train_size+valid_size:]\n",
    "train =  train[:train_size]\n",
    "\n",
    "print(\"Shape of training set: {}\".format(train.shape))\n",
    "print(\"Shape of validation set: {}\".format(valid.shape))\n",
    "print(\"Shape of test set: {}\".format(test.shape))\n",
    "\n",
    "serialize_dataset(train, DATASET_PATH, TRAINING_SET, \".csv.bz2\")\n",
    "serialize_dataset(valid, DATASET_PATH, VALIDATION_SET, \".csv.bz2\")\n",
    "serialize_dataset(test, DATASET_PATH, TEST_SET, \".csv.bz2\")\n",
    "\n",
    "# train.head()\n",
    "# valid.head()\n",
    "# test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WITH ATTACKS\n",
    "\n",
    "######################\n",
    "# Attacker Definition\n",
    "######################\n",
    "max_budget_per_feature = {\n",
    "    'PAY_0'     : 1000,\n",
    "    'BILL_AMT1': 1000,\n",
    "    'PAY_2'    : 1000,\n",
    "    'LIMIT_BAL' : 1000\n",
    "}\n",
    "\n",
    "B = [10,30,40,60] # 90\n",
    "\n",
    "attacker_rules = load_attack_rules(ATKS_FILE, list(train.columns.values) )\n",
    "\n",
    "for budget in B:\n",
    "    print (\"Processing Budget: \", budget)\n",
    "    train_att = perturb_dataset(train, budget, max_budget_per_feature, attacker_rules, skip_class=None)\n",
    "    valid_att = perturb_dataset(valid, budget, max_budget_per_feature, attacker_rules, skip_class=None)\n",
    "    test_att  = perturb_dataset(test, budget, max_budget_per_feature, attacker_rules, skip_class=None)\n",
    "    \n",
    "    serialize_dataset(train_att, ATK_DATASET_PATH, TRAINING_SET, \"_B{}\".format(budget)+\".atks.bz2\")\n",
    "    serialize_dataset(valid_att, ATK_DATASET_PATH, VALIDATION_SET, \"_B{}\".format(budget)+\".atks.bz2\")\n",
    "    serialize_dataset(test_att, ATK_DATASET_PATH, TEST_SET, \"_B{}\".format(budget)+\".atks.bz2\")\n",
    "    \n",
    "    print(\"Shape of attacked training set: {}\".format(train_att.shape))\n",
    "    print(\"Shape of attacked validation set: {}\".format(valid_att.shape))\n",
    "    print(\"Shape of attacked test set: {}\".format(test_att.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WEBSITES Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME=\"websites\"\n",
    "\n",
    "DATASET_PATH=\"../data/{}\".format(DATASET_NAME)\n",
    "RAW_DATASET_PATH=DATASET_PATH + \"/raw\"\n",
    "ATK_DATASET_PATH=DATASET_PATH + \"/attacks\"\n",
    "\n",
    "ATKS_FILE = os.path.join(ATK_DATASET_PATH, \"attacks.json\")\n",
    "\n",
    "TRAINING_SET=\"train.csv.bz2\"\n",
    "VALIDATION_SET=\"valid.csv.bz2\"\n",
    "TEST_SET=\"test.csv.bz2\"\n",
    "N_TRAIN_INSTANCES=None # replace this with None to load the whole training set\n",
    "N_TEST_INSTANCES=None # replace this with None to load the whole test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(os.path.join(RAW_DATASET_PATH, \"dataset.csv\"))\n",
    "\n",
    "train.drop(columns=['URL'], inplace=True)\n",
    "train[\"CONTENT_LENGTH\"] = train[\"CONTENT_LENGTH\"].fillna(0)\n",
    "train.drop(columns=['WHOIS_REGDATE','WHOIS_UPDATED_DATE'], inplace=True)\n",
    "train.dropna(inplace=True)\n",
    "\n",
    "print(\"Number of instances = {}\\nNumber of features = {}\".format(train.shape[0], train.shape[1] - 1))\n",
    "print(\"Class distribution:\\n{}\".format(train['Type'].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train['SERVER'].apply(lambda x: \"codfw.wmnet\" if \"codfw.wmnet\" in str(x) else str(x).split(\"/\")[0] ).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a (stratified) sample of N = 350 instances (i.e., 20%) out of the whole dataset\n",
    "#y = train[\"Type\"].to_frame()\n",
    "#X = train\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=0.2, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(train)*.6)\n",
    "valid_size = int(len(train)*.2)\n",
    "\n",
    "valid = train[train_size:train_size+valid_size]\n",
    "test  = train[train_size+valid_size:]\n",
    "train = train[:train_size]\n",
    "\n",
    "print(\"Shape of training set: {}\".format(train.shape))\n",
    "print(\"Shape of validation set: {}\".format(valid.shape))\n",
    "print(\"Shape of test set: {}\".format(test.shape))\n",
    "\n",
    "# binarize\n",
    "binarize_data(train, \"Type\", 0.5)\n",
    "binarize_data(valid, \"Type\", 0.5)\n",
    "binarize_data(test, \"Type\", 0.5)\n",
    "\n",
    "#discretize\n",
    "discretize([train, valid, test], 128, [\"Type\", \"CHARSET\", \"SERVER\", \"WHOIS_COUNTRY\", \"WHOIS_STATEPRO\"])\n",
    "\n",
    "serialize_dataset(train, DATASET_PATH, TRAINING_SET, \".csv.bz2\")\n",
    "serialize_dataset(valid, DATASET_PATH, VALIDATION_SET, \".csv.bz2\")\n",
    "serialize_dataset(test, DATASET_PATH, TEST_SET, \".csv.bz2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WITH ATTACKS\n",
    "\n",
    "######################\n",
    "# Attacker Definition\n",
    "######################\n",
    "max_budget_per_feature = {\n",
    "    'NUMBER_SPECIAL_CHARACTERS': 1000,\n",
    "    'URL_LENGTH': 1000,\n",
    "    'CONTENT_LENGTH': 1000,\n",
    "    'REMOTE_APP_BYTES' : 1000,\n",
    "    'APP_BYTES' :1000\n",
    "}\n",
    "\n",
    "B = [10,30] # 90\n",
    "\n",
    "attacker_rules = load_attack_rules(ATKS_FILE, list(train.columns.values) )\n",
    "\n",
    "for budget in B:\n",
    "    print (\"Processing Budget: \", budget)\n",
    "    train_att = perturb_dataset(train, budget, max_budget_per_feature, attacker_rules, skip_class=None)\n",
    "    valid_att = perturb_dataset(valid, budget, max_budget_per_feature, attacker_rules, skip_class=None)\n",
    "    test_att  = perturb_dataset(test, budget, max_budget_per_feature, attacker_rules, skip_class=None)\n",
    "    \n",
    "    serialize_dataset(train_att, ATK_DATASET_PATH, TRAINING_SET, \"_B{}\".format(budget)+\".atks.bz2\")\n",
    "    serialize_dataset(valid_att, ATK_DATASET_PATH, VALIDATION_SET, \"_B{}\".format(budget)+\".atks.bz2\")\n",
    "    serialize_dataset(test_att, ATK_DATASET_PATH, TEST_SET, \"_B{}\".format(budget)+\".atks.bz2\")\n",
    "    \n",
    "    print(\"Shape of attacked training set: {}\".format(train_att.shape))\n",
    "    print(\"Shape of attacked validation set: {}\".format(valid_att.shape))\n",
    "    print(\"Shape of attacked test set: {}\".format(test_att.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Financial Distress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME=\"financial\"\n",
    "\n",
    "DATASET_PATH=\"../data/{}\".format(DATASET_NAME)\n",
    "RAW_DATASET_PATH=DATASET_PATH + \"/raw\"\n",
    "ATK_DATASET_PATH=DATASET_PATH + \"/attacks\"\n",
    "\n",
    "ATKS_FILE = os.path.join(ATK_DATASET_PATH, \"attacks.json\")\n",
    "\n",
    "TRAINING_SET=\"train.csv.bz2\"\n",
    "VALIDATION_SET=\"valid.csv.bz2\"\n",
    "TEST_SET=\"test.csv.bz2\"\n",
    "N_TRAIN_INSTANCES=None # replace this with None to load the whole training set\n",
    "N_TEST_INSTANCES=None # replace this with None to load the whole test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(os.path.join(RAW_DATASET_PATH, \"dataset.csv\"))\n",
    "\n",
    "binarize_data(train, \"Financial Distress\", 0)\n",
    "\n",
    "# drop company and time\n",
    "train.drop(columns=['Company', 'Time'], inplace=True)\n",
    "\n",
    "# 80 is categorical\n",
    "train['x80'] = train['x80'].astype(\"category\")\n",
    "\n",
    "discretize([train], 256, [\"Financial Distress\", 'x80'])\n",
    "\n",
    "# put label in last column\n",
    "train = pd.concat([train.iloc[:,1:], train.iloc[:,0]], axis=1)\n",
    "\n",
    "print(\"Number of instances = {}\\nNumber of features = {}\".format(train.shape[0], train.shape[1] - 1))\n",
    "print(\"Class distribution:\\n{}\".format(train['Financial Distress'].value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified sampling\n",
    "positive = train[train['Financial Distress']>0]\n",
    "negative = train[train['Financial Distress']<0]\n",
    "\n",
    "train_pos_size = int(len(positive)*.6)\n",
    "valid_pos_size = int(len(positive)*.2)\n",
    "train_neg_size = int(len(negative)*.6)\n",
    "valid_neg_size = int(len(negative)*.2)\n",
    "\n",
    "valid = pd.concat([positive[train_pos_size:train_pos_size+valid_pos_size], \n",
    "                   negative[train_neg_size:train_neg_size+valid_neg_size]]) \n",
    "test  = pd.concat([positive[train_pos_size+valid_pos_size:], \n",
    "                   negative[train_neg_size+valid_neg_size:]]) \n",
    "train = pd.concat([positive[:train_pos_size], \n",
    "                   negative[:train_neg_size]]) \n",
    "\n",
    "print(\"Shape of training set: {}\".format(train.shape))\n",
    "print(\"Shape of validation set: {}\".format(valid.shape))\n",
    "print(\"Shape of test set: {}\".format(test.shape))\n",
    "\n",
    "serialize_dataset(train, DATASET_PATH, TRAINING_SET, \".csv.bz2\")\n",
    "serialize_dataset(valid, DATASET_PATH, VALIDATION_SET, \".csv.bz2\")\n",
    "serialize_dataset(test, DATASET_PATH, TEST_SET, \".csv.bz2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe().iloc[:,38:55]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature importance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm\n",
    "def print_fx_imp(model, colnames):\n",
    "    fx_uses = model.feature_importance(importance_type='split')\n",
    "    fx_gain = model.feature_importance(importance_type='gain')\n",
    "\n",
    "    for i,f in enumerate(np.argsort(fx_gain)[::-1]):\n",
    "        print (\"{:2d} {:20s} {:.3f} {:4d}\".format(i, colnames[f], fx_gain[f], fx_uses[f]))\n",
    "\n",
    "print(\"-- GDBT --\")    \n",
    "#gbdt = lightgbm.Booster(model_file=\"../out/models/census/std-gbdt_census_T100_S0050_L24_R100.model\")\n",
    "gbdt = lightgbm.Booster(model_file=\"../out/models/financial/std-gbdt_financial_T100_S0010_L256_R100.model\")\n",
    "print(gbdt.num_trees())\n",
    "print_fx_imp(gbdt, train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WITH ATTACKS\n",
    "\n",
    "######################\n",
    "# Attacker Definition\n",
    "######################\n",
    "max_budget_per_feature = {\n",
    "    'x36': 1000,\n",
    "    'x81': 1000,\n",
    "    'x47': 1000,\n",
    "    'x37': 1000,\n",
    "    'x40':1000,\n",
    "    'x3':1000,\n",
    "    'x16':1000,\n",
    "    'x52':1000\n",
    "    \n",
    "}\n",
    "\n",
    "B = [10] # 90\n",
    "\n",
    "attacker_rules = load_attack_rules(ATKS_FILE, list(train.columns.values) )\n",
    "\n",
    "for budget in B:\n",
    "    print (\"Processing Budget: \", budget)\n",
    "    train_att = perturb_dataset(train, budget, max_budget_per_feature, attacker_rules, skip_class=None)\n",
    "    valid_att = perturb_dataset(valid, budget, max_budget_per_feature, attacker_rules, skip_class=None)\n",
    "    test_att  = perturb_dataset(test, budget, max_budget_per_feature, attacker_rules, skip_class=None)\n",
    "    \n",
    "    serialize_dataset(train_att, ATK_DATASET_PATH, TRAINING_SET, \"_B{}\".format(budget)+\".atks.bz2\")\n",
    "    serialize_dataset(valid_att, ATK_DATASET_PATH, VALIDATION_SET, \"_B{}\".format(budget)+\".atks.bz2\")\n",
    "    serialize_dataset(test_att, ATK_DATASET_PATH, TEST_SET, \"_B{}\".format(budget)+\".atks.bz2\")\n",
    "    \n",
    "    print(\"Shape of attacked training set: {}\".format(train_att.shape))\n",
    "    print(\"Shape of attacked validation set: {}\".format(valid_att.shape))\n",
    "    print(\"Shape of attacked test set: {}\".format(test_att.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# malware -APIcall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_NAME=\"malware\"\n",
    "\n",
    "DATASET_PATH=\"../data/{}\".format(DATASET_NAME)\n",
    "RAW_DATASET_PATH=DATASET_PATH + \"/raw\"\n",
    "ATK_DATASET_PATH=DATASET_PATH + \"/attacks\"\n",
    "\n",
    "ATKS_FILE = os.path.join(ATK_DATASET_PATH, \"attacks.json\")\n",
    "\n",
    "TRAINING_SET=\"train.csv.bz2\"\n",
    "VALIDATION_SET=\"valid.csv.bz2\"\n",
    "TEST_SET=\"test.csv.bz2\"\n",
    "N_TRAIN_INSTANCES=None # replace this with None to load the whole training set\n",
    "N_TEST_INSTANCES=None # replace this with None to load the whole test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of instances = 47580\n",
      "Number of features = 1000\n",
      "Class distribution:\n",
      " 1    45651\n",
      "-1     1929\n",
      "Name: malware, dtype: int64\n",
      "Class distribution:\n",
      " 1    0.959458\n",
      "-1    0.040542\n",
      "Name: malware, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(os.path.join(RAW_DATASET_PATH, \"malware.csv\"))\n",
    "\n",
    "train.drop(columns=['hash'], inplace=True)\n",
    "\n",
    "train['malware'] = train['malware'].map({1: 1, 0: -1})\n",
    "\n",
    "    \n",
    "train = train.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "#to_remove = np.random.choice(train[train['malware']==1].index,size=15000,replace=False)\n",
    "#train = train.drop(to_remove)\n",
    "#train = train.sample(frac=1).reset_index(drop=True) #to reshuffle\n",
    "print(\"Number of instances = {}\\nNumber of features = {}\".format(train.shape[0], train.shape[1] - 1))\n",
    "print(\"Class distribution:\\n{}\".format(train['malware'].value_counts()))\n",
    "print(\"Class distribution:\\n{}\".format(train['malware'].value_counts()/len(train)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train.columns.get_loc(\"exit\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training set: (9516, 1001)\n",
      "Class distribution:\n",
      " 1    0.96322\n",
      "-1    0.03678\n",
      "Name: malware, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "train_size = int(len(train)*.6)\n",
    "valid_size = int(len(train)*.2)\n",
    "\n",
    "valid = train[train_size:train_size+valid_size]\n",
    "test  = train[train_size+valid_size:]\n",
    "train = train[:train_size]\n",
    "\n",
    "print(\"Shape of training set: {}\".format(test.shape))\n",
    "#print(\"Class distribution:\\n{}\".format(test['malware'].value_counts()/len(valid)))\n",
    "\n",
    "print(\"Class distribution:\\n{}\".format(test['malware'].value_counts()/len(test)))\n",
    "\n",
    "#serialize_dataset(train, DATASET_PATH, TRAINING_SET, \".csv.bz2\")\n",
    "#serialize_dataset(valid, DATASET_PATH, VALIDATION_SET, \".csv.bz2\")\n",
    "\n",
    "\n",
    "# binarize\n",
    "\n",
    "#binarize_data(test, \"quality\", 6)\n",
    "\n",
    "#discretize\n",
    "#discretize([train, valid, test], 128, [\"quality\"])\n",
    "\n",
    "serialize_dataset(test, DATASET_PATH, TEST_SET, \".csv.bz2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training set: (32028, 1001)\n",
      "Class distribution:\n",
      " 1    0.588548\n",
      "-1    0.411452\n",
      "Name: malware, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "add = train[train['malware']==-1]\n",
    "for i in range (10):\n",
    "    train =train.append(add)\n",
    "train = train.sample(frac=1).reset_index(drop=True) #to reshuffle\n",
    "to_remove = np.random.choice(train[train['malware']==1].index,size=8500,replace=False)\n",
    "train = train.drop(to_remove)\n",
    "train = train.sample(frac=1).reset_index(drop=True) #to reshuffle\n",
    "\n",
    "print(\"Shape of training set: {}\".format(train.shape))\n",
    "#print(\"Shape of validation set: {}\".format(valid.shape))\n",
    "#print(\"Shape of test set: {}\".format(test.shape))\n",
    "\n",
    "print(\"Class distribution:\\n{}\".format(train['malware'].value_counts()/len(train)))\n",
    "\n",
    "#binarize_data(valid, \"quality\", 6)\n",
    "#discretize([train], 128, [\"malware\"])\n",
    "\n",
    "serialize_dataset(train, DATASET_PATH, TRAINING_SET, \".csv.bz2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training set: (10626, 1001)\n",
      "Class distribution:\n",
      " 1    0.60559\n",
      "-1    0.39441\n",
      "Name: malware, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "add = valid[valid['malware']==-1]\n",
    "for i in range (10):\n",
    "    valid =valid.append(add)\n",
    "train = train.sample(frac=1).reset_index(drop=True) #to reshuffle\n",
    "to_remove = np.random.choice(valid[valid['malware']==1].index,size=2700,replace=False)\n",
    "valid = valid.drop(to_remove)\n",
    "valid = valid.sample(frac=1).reset_index(drop=True) #to reshuffle\n",
    "\n",
    "print(\"Shape of training set: {}\".format(valid.shape))\n",
    "#print(\"Shape of validation set: {}\".format(valid.shape))\n",
    "#print(\"Shape of test set: {}\".format(test.shape))\n",
    "print(\"Class distribution:\\n{}\".format(valid['malware'].value_counts()/len(valid)))\n",
    "\n",
    "\n",
    "#binarize_data(train, \"quality\", 6)\n",
    "#discretize([train, valid, test], 128, [\"quality\"])\n",
    "\n",
    "\n",
    "serialize_dataset(valid, DATASET_PATH, VALIDATION_SET, \".csv.bz2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm\n",
    "def print_fx_imp(model, colnames):\n",
    "    fx_uses = model.feature_importance(importance_type='split')\n",
    "    fx_gain = model.feature_importance(importance_type='gain')\n",
    "\n",
    "    for i,f in enumerate(np.argsort(fx_gain)[::-1]):\n",
    "        print (\"{:2d} {:20s} {:.3f} {:4d}\".format(i, colnames[f], fx_gain[f], fx_uses[f]))\n",
    "\n",
    "print(\"-- GDBT --\")    \n",
    "#gbdt = lightgbm.Booster(model_file=\"../out/models/census/std-gbdt_census_T100_S0050_L24_R100.model\")\n",
    "#gbdt = lightgbm.Booster(model_file=\"../out/models/malware/rf-gbdt_malware_T100_S0050_L256_R48.model\")\n",
    "#print(gbdt.num_trees())\n",
    "#print_fx_imp(gbdt, train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WITH ATTACKS\n",
    "\n",
    "######################\n",
    "# Attacker Definition\n",
    "######################\n",
    "\n",
    "max_budget_per_feature = {\n",
    "    '_cexit'     : 1000,\n",
    "    'SearchPathW': 1000,\n",
    "    'exit'       :1000\n",
    "    #'_controlfp' :1000,\n",
    "    #'VirtualAlloc':1000,\n",
    "    #'Polyline'    :1000\n",
    "    \n",
    "}\n",
    "\n",
    "attacker_rules = load_attack_rules(ATKS_FILE, list(train.columns.values) )\n",
    "\n",
    "B = [20,40,60] \n",
    "\n",
    "for budget in B:\n",
    "    print (\"Processing Budget: \", budget)\n",
    "    \n",
    "    train_att = perturb_dataset(train, budget, max_budget_per_feature, attacker_rules, skip_class=None)\n",
    "    valid_att = perturb_dataset(valid, budget, max_budget_per_feature, attacker_rules, skip_class=None)\n",
    "    test_att  = perturb_dataset(test, budget, max_budget_per_feature, attacker_rules, skip_class=None)\n",
    "    \n",
    "    \n",
    "    serialize_dataset(train_att, ATK_DATASET_PATH, TRAINING_SET, \"_B{}\".format(budget)+\".atks.bz2\")\n",
    "    serialize_dataset(valid_att, ATK_DATASET_PATH, VALIDATION_SET, \"_B{}\".format(budget)+\".atks.bz2\")\n",
    "    serialize_dataset(test_att, ATK_DATASET_PATH, TEST_SET, \"_B{}\".format(budget)+\".atks.bz2\")\n",
    "    \n",
    "    print(\"Shape of attacked training set: {}\".format(train_att.shape))\n",
    "    print(\"Shape of attacked validation set: {}\".format(valid_att.shape))\n",
    "    print(\"Shape of attacked test set: {}\".format(test_att.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bbc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
